# üîÑ Tema 7: Doble-Texting ‚Äì Manejo de Mensajes Simult√°neos en LangGraph  

## üåü Introducci√≥n  

Cuando un usuario interact√∫a con un chatbot o un sistema basado en LangGraph, puede ocurrir que **env√≠e m√∫ltiples mensajes antes de recibir una respuesta**.  
Este fen√≥meno se conoce como **doble-texting** y puede generar **inconsistencias en el flujo conversacional** si no se maneja correctamente.  

üìå **Ejemplo de problema con doble-texting:**  
1Ô∏è‚É£ Un usuario env√≠a el mensaje: *"¬øCu√°nto cuesta el producto?"*  
2Ô∏è‚É£ Antes de que el chatbot responda, env√≠a otro mensaje: *"Olv√≠dalo, mejor dime sobre el env√≠o."*  
3Ô∏è‚É£ Si el grafo no maneja el doble-texting, **puede responder primero a la pregunta del precio**, ignorando el segundo mensaje o proces√°ndolo de forma desordenada.  

üîπ Para evitar estos problemas, LangGraph ofrece **cuatro estrategias** para manejar el doble-texting de forma efectiva:  
‚úÖ **Reject (Rechazar)** ‚Äì Ignorar el segundo mensaje.  
‚úÖ **Enqueue (Encolar)** ‚Äì Poner el mensaje en espera hasta que termine la respuesta actual.  
‚úÖ **Interrupt (Interrumpir)** ‚Äì Detener la ejecuci√≥n y procesar el mensaje nuevo.  
‚úÖ **Rollback (Revertir)** ‚Äì Deshacer la ejecuci√≥n y comenzar desde el nuevo mensaje.  

Cada una de estas estrategias se adapta a diferentes necesidades seg√∫n el contexto del grafo.  

üîó **Referencia oficial**: [Doble-Texting en LangGraph](https://langchain-ai.github.io/langgraph/concepts/double_texting/)  

---

## ‚ùå 1. Reject ‚Äì Rechazar Mensajes Adicionales  

Con esta estrategia, **LangGraph ignora cualquier nuevo mensaje** si ya hay una ejecuci√≥n en curso.  
Es √∫til cuando queremos **procesar solo una solicitud a la vez**, evitando interrupciones o cambios en el flujo.  

???+ warning "Atenci√≥n"  
    Esta estrategia puede generar una mala experiencia de usuario si el segundo mensaje es m√°s relevante que el primero.  

```python
from langgraph_sdk import get_client
url_for_cli_deployment = "http://localhost:8123"
client = get_client(url=url_for_cli_deployment)

import httpx
from langchain_core.messages import HumanMessage

# Creamos un thread
thread = await client.threads.create()

# Creamos las operaciones
input_1 = "Realizame esta suma 4+2."
input_2 = "perdon, era 4+3."
config = {"configurable": {"user_id": "Test-Double-Texting"}}
graph_name = "calculator" 

# Ejecutamos el primero Run
run = await client.runs.create(
    thread["thread_id"],
    graph_name,
    input={"messages": [HumanMessage(content=input_1)]}, 
    config=config,
)
try:
    # Simulamos que ejecutamos otro Run de forma seguida.
    await client.runs.create(
        thread["thread_id"],
        graph_name,
        input={"messages": [HumanMessage(content=input_2)]}, 
        config=config,
        multitask_strategy="reject",
    )
except httpx.HTTPStatusError as e:
    print("A√∫n hay una ejecuci√≥n en marcha:", e)
```

```python title="Resultado"
A√∫n hay una ejecuci√≥n en marcha: Client error '409 Conflict' for url 'http://localhost:8123/threads/xxxxxxxxxxxxxxxxxx/runs'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409
```

Como vemos en el c√≥digo, al ejecutar el segundo *run*, obtenemos un error que indica que ya hay un proceso en ejecuci√≥n.  
Esto significa que **el segundo mensaje no se procesar√°**, evitando conflictos en el flujo del grafo.  

???+ quote "Recomendaci√≥n"

    Si se usa esta estrategia, es importante **informar al usuario** que debe esperar antes de enviar otro mensaje.  


---

## üì• 2. Enqueue ‚Äì Encolar Mensajes en Espera  

En lugar de ignorar el nuevo mensaje, **LangGraph lo almacena en una cola** y lo procesa cuando termine la ejecuci√≥n actual.  
Esta estrategia permite **mantener el orden de los mensajes sin perder informaci√≥n**.  

???+ note "Nota"  
    Con esta estrategia, el chatbot responder√° **cada mensaje en el orden en que se recibi√≥**, garantizando coherencia en la conversaci√≥n.  

```python hl_lines="24"
from langchain_core.messages import HumanMessage, convert_to_messages

# Creamos un thread
thread = await client.threads.create()

# Creamos las operaciones
input_1 = "Realizame esta suma 4+2."
input_2 = "perdon, era 4+3."
config = {"configurable": {"user_id": "Test-Double-Texting"}}
graph_name = "calculator" 

first_run = await client.runs.create(
    thread["thread_id"],
    graph_name,
    input={"messages": [HumanMessage(content=input_1)]}, 
    config=config,
)

second_run = await client.runs.create(
    thread["thread_id"],
    graph_name,
    input={"messages": [HumanMessage(content=input_2)]}, 
    config=config,
    multitask_strategy="enqueue",
)

# Esperamos hasta que el segundo termine
await client.runs.join(thread["thread_id"], second_run["run_id"])

# Obtenemos el resultado del grafo
state = await client.threads.get_state(thread["thread_id"])
for m in convert_to_messages(state["values"]["messages"]):
    m.pretty_print()
```

```python title="Resultado"
================================ Human Message =================================

Realizame esta suma 4+2.
================================== Ai Message ==================================
Tool Calls:
  sumar (call_ptkHcyJZIEtBxedW0map6xxO)
 Call ID: call_ptkHcyJZIEtBxedW0map6xxO
  Args:
    a: 4
    b: 2
================================= Tool Message =================================
Name: sumar

6
================================== Ai Message ==================================

La suma de 4 + 2 es 6.
================================ Human Message =================================

perdon, era 4+3.
================================== Ai Message ==================================
Tool Calls:
  sumar (call_T4rPpzzVKsQPFz2Q7b4JRdtW)
 Call ID: call_T4rPpzzVKsQPFz2Q7b4JRdtW
  Args:
    a: 4
    b: 3
================================= Tool Message =================================
Name: sumar

7
================================== Ai Message ==================================

La suma de 4 + 3 es 7.
```

Aqu√≠ vemos que, aunque se env√≠an dos mensajes consecutivos, LangGraph **almacena el segundo mensaje en la cola**  
y lo procesa **una vez que finaliza el primero**.  

???+ quote "Ventajas" 

    - **Garantiza respuestas en orden.**  
    - **Evita errores de ejecuci√≥n simult√°nea.**  
    - **Mantiene el flujo de la conversaci√≥n sin bloqueos.** 

---

## ‚èπÔ∏è 3. Interrupt ‚Äì Interrumpir la Ejecuci√≥n en Curso  

Esta estrategia **detiene la ejecuci√≥n actual y prioriza el nuevo mensaje**, asegurando que siempre se responda la √∫ltima entrada del usuario.  

???+ example "Ejemplo"  
    Si un usuario pregunta por un producto y luego dice *"Olv√≠dalo, mejor dime sobre el env√≠o"*, el chatbot **interrumpir√° la respuesta** y procesar√° la √∫ltima solicitud.  

```python hl_lines="24"
from langchain_core.messages import HumanMessage, convert_to_messages

# Creamos un thread
thread = await client.threads.create()

# Creamos las operaciones
input_1 = "Realizame esta suma 4+2."
input_2 = "perdon, era 4+3."
config = {"configurable": {"user_id": "Test-Double-Texting"}}
graph_name = "calculator" 

interrupted_run = await client.runs.create(
    thread["thread_id"],
    graph_name,
    input={"messages": [HumanMessage(content=input_1)]}, 
    config=config,
)

second_run = await client.runs.create(
    thread["thread_id"],
    graph_name,
    input={"messages": [HumanMessage(content=input_2)]}, 
    config=config,
    multitask_strategy="interrupt",
)

# Esperamos hasta que el segundo termine
await client.runs.join(thread["thread_id"], second_run["run_id"])

# Obtenemos el resultado del grafo
state = await client.threads.get_state(thread["thread_id"])
for m in convert_to_messages(state["values"]["messages"]):
    m.pretty_print()
```

```python title="Resultado"
================================ Human Message =================================

Realizame esta suma 4+2.
================================ Human Message =================================

perdon, era 4+3.
================================== Ai Message ==================================
Tool Calls:
  sumar (call_y7Q6Og96OLpoVXZ4Xt92yE1B)
 Call ID: call_y7Q6Og96OLpoVXZ4Xt92yE1B
  Args:
    a: 4
    b: 3
================================= Tool Message =================================
Name: sumar

7
================================== Ai Message ==================================

La suma de 4 + 3 es igual a 7.
```

Como podemos ver, **la ejecuci√≥n anterior se interrumpe** y el segundo mensaje se procesa inmediatamente.  
Sin embargo, **cualquier informaci√≥n generada hasta ese punto sigue almacenada**.  

???+ warning "Cuidado"  
    Esta estrategia no borra el estado del grafo antes de interrumpir la ejecuci√≥n. Si la ejecuci√≥n anterior hab√≠a guardado datos parciales, estos seguir√°n disponibles.  

Podemos comprobar que el *run* ha sido interrumpido ejecutando lo siguiente: 

```python
# Confirmamos que el primer run se ha interrumpido y que el segundo ha terminado.
print((await client.runs.get(thread["thread_id"], interrupted_run["run_id"]))["status"])
print((await client.runs.get(thread["thread_id"], second_run["run_id"]))["status"])
```

```python title="Resultado"
interrupted
success
```

---

## üîÑ 4. Rollback ‚Äì Revertir la Ejecuci√≥n y Procesar el Nuevo Mensaje  

Con esta estrategia, **LangGraph deshace la ejecuci√≥n actual y vuelve al estado anterior** antes de procesar el nuevo mensaje.  
Es √∫til cuando queremos garantizar que cada respuesta tenga en cuenta la √∫ltima entrada del usuario sin interrupciones abruptas.  

???+ quote "Ejemplo"  
    Si un usuario dice *"Quiero pedir una pizza"*, pero segundos despu√©s dice *"Mejor una hamburguesa"*, la solicitud anterior se **revierte** y se procesa solo la √∫ltima.  

```python hl_lines="24"
from langchain_core.messages import HumanMessage, convert_to_messages

# Creamos un thread
thread = await client.threads.create()

# Creamos las operaciones
input_1 = "Realizame esta suma 4+2."
input_2 = "perdon, era 4+3."
config = {"configurable": {"user_id": "Test-Double-Texting"}}
graph_name = "calculator" 

rolled_back_run = await client.runs.create(
    thread["thread_id"],
    graph_name,
    input={"messages": [HumanMessage(content=input_1)]}, 
    config=config,
)

second_run = await client.runs.create(
    thread["thread_id"],
    graph_name,
    input={"messages": [HumanMessage(content=input_2)]}, 
    config=config,
    multitask_strategy="rollback",
)

# Esperamos hasta que el segundo termine
await client.runs.join(thread["thread_id"], second_run["run_id"])

# Obtenemos el resultado del grafo
state = await client.threads.get_state(thread["thread_id"])
for m in convert_to_messages(state["values"]["messages"]):
    m.pretty_print()
``` 

```python title="Resultado"
================================ Human Message =================================

perdon, era 4+3.
================================== Ai Message ==================================
Tool Calls:
  sumar (call_O8oEtXiQ34q6K4MjhFsAkuP7)
 Call ID: call_O8oEtXiQ34q6K4MjhFsAkuP7
  Args:
    a: 4
    b: 3
================================= Tool Message =================================
Name: sumar

7
================================== Ai Message ==================================

La soluci√≥n de la ecuaci√≥n \( 4 + 3 \) es \( 7 \).
```

Aqu√≠, observamos que **el primer mensaje se borra completamente del estado del grafo** y se procesa solo el √∫ltimo.

???+ Warning "Consideraciones"

    ‚úÖ Esta estrategia garantiza **precisi√≥n en la respuesta**, pero...  
    ‚ùå Puede **eliminar informaci√≥n √∫til** que se haya procesado en el primer mensaje.

    Se recomienda usar Rollback solo cuando se tiene **un control claro** de los datos almacenados en el grafo.

---

## ‚ú® Conclusi√≥n  

El doble-texting es un desaf√≠o com√∫n en sistemas conversacionales, pero con LangGraph podemos **gestionar m√∫ltiples mensajes de manera estructurada**.  

‚úÖ **Podemos ignorar mensajes adicionales (Reject).**  
‚úÖ **Podemos encolarlos para responderlos en orden (Enqueue)(Recomendado).**  
‚úÖ **Podemos interrumpir la ejecuci√≥n para priorizar el √∫ltimo mensaje (Interrupt).**  
‚úÖ **Podemos revertir la ejecuci√≥n y comenzar desde el nuevo mensaje (Rollback).**  

üöÄ **Con estas estrategias, podemos mejorar la fluidez y coherencia en las conversaciones con LangGraph.**  

---

## üßë‚Äçüè´ **¬øQu√© Hemos Aprendido?**  

- Qu√© es el **doble-texting** y por qu√© puede generar problemas en un grafo.  
- Cu√°les son las **cuatro estrategias** para manejar m√∫ltiples mensajes.  
- Cu√°ndo usar **Reject, Enqueue, Interrupt y Rollback** seg√∫n el caso de uso.  

---

## üîé Recursos:

- :simple-googlecolab: Ver notebook en [Google Colab](https://colab.research.google.com/drive/1j3I46DOc_YUFsJlA8Rchh-mSB0U0vo8M?usp=sharing)
- :books: Concept: [Double Texting](https://langchain-ai.github.io/langgraph/concepts/double_texting/#double-texting)
- :books: How-to-guide: [Reject](https://langchain-ai.github.io/langgraph/cloud/how-tos/reject_concurrent/#reject)
- :books: How-to-guide: [Enqueue](https://langchain-ai.github.io/langgraph/cloud/how-tos/enqueue_concurrent/#enqueue)
- :books: How-to-guide: [Interrupt](https://langchain-ai.github.io/langgraph/cloud/how-tos/interrupt_concurrent/#interrupt)
- :books: How-to-guide: [Rollback](https://langchain-ai.github.io/langgraph/cloud/how-tos/rollback_concurrent/#rollback)

---

## üåê **¬øQu√© es lo Siguiente?**  

En el pr√≥ximo tema, exploraremos **c√≥mo exponer nuestro grafo como una API REST o GraphQL**, permitiendo integrarlo en cualquier aplicaci√≥n externa.  
