# ğŸ–¥ï¸ Tema 5: Uso de la SDK de LangGraph  

## ğŸŒŸ IntroducciÃ³n  

En el tema anterior, aprendimos a usar **LangGraph CLI** para iniciar y administrar nuestros grafos de manera local o en contenedores Docker.  
Ahora, daremos el siguiente paso: **usar la SDK de LangGraph** para interactuar con nuestro grafo de manera programÃ¡tica.  

ğŸ“Œ **Â¿Por quÃ© usar la SDK de LangGraph?**  
- Permite **conectarse a grafos remotos** y ejecutarlos desde cÃ³digo.  
- Facilita la **gestiÃ³n de ejecuciones (runs) y sesiones (threads)**.  
- Proporciona herramientas para **recuperar y almacenar datos** dentro de LangGraph.  
- Nos da acceso a caracterÃ­sticas avanzadas como **streaming, Human-in-the-loop y persistencia de datos**.  

ğŸ”— **Referencia oficial**: [DocumentaciÃ³n de la SDK](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/)  

```python
pip install langgraph-sdk
```

---

## ğŸ› ï¸ **1. Remote Graph â€“ Ejecutar un Grafo de Forma Remota**  

LangGraph nos permite ejecutar nuestros grafos en un **servidor remoto**, en lugar de hacerlo localmente.  
Para lograrlo, podemos optar por dos mÃ©todos:  
1ï¸âƒ£ **Usar la SDK con `get_client()`**  
2ï¸âƒ£ **Utilizar la clase `RemoteGraph`** para interactuar con el grafo remoto directamente.  

ğŸ“Œ **Ejemplo de conexiÃ³n a un grafo remoto:**  


```python title="OpciÃ³n 1: client"
from langgraph_sdk import get_client

# Connect via SDK
url_for_cli_deployment = "http://localhost:8123"
client = get_client(url=url_for_cli_deployment)
```


```python title="OpciÃ³n 2: RemoteGraph"
from langgraph.client import RemoteGraph

# Especificamos que grafo queremos usar
graph_name = <NOMBRE_DEL_GRAFO>

# Conectamos con el grafo remoto en LangGraph Cloud o servidor propio
graph = RemoteGraph(graph_name, url="http://localhost:8080")

# Definimos la entrada para el grafo
input_data = {"messages": ["Hola, Â¿cÃ³mo estÃ¡s?"]}

# Ejecutamos el grafo y obtenemos la respuesta
response = graph.invoke(input_data)

# Mostramos la respuesta generada
print(response)
```

ğŸ”¹ En este ejemplo:  
1. Nos conectamos a un **servidor remoto** que ejecuta nuestro grafo.  
2. Enviamos una **entrada** y obtenemos una **respuesta**.  
3. Podemos utilizarlo de la misma manera que un `StateGraph` local.  

---

## ğŸ”„ **2. Runs â€“ GestiÃ³n de Ejecuciones en LangGraph**  

Un **run** es una instancia de ejecuciÃ³n de un grafo.  
Cada vez que ejecutamos un grafo, se crea un nuevo **run**, que podemos gestionar mediante la SDK.  

Un **background run** se ejecuta en segundo plano, permitiendo consultar su estado mÃ¡s adelante. existen dos tipos:  
- Fire and forget - Ejecutamos un run background y no esperamos a que acabe.  
- Esperamos la respuesta (blocking or polling) - Ejecutamos un run y esperamos a que responda.  

ğŸ“Œ **Ejemplo: Crear y recuperar un background run en LangGraph**  

```python
from langgraph_sdk import get_client
from langchain_core.messages import HumanMessage

# Connect via SDK
url_for_cli_deployment = "http://localhost:8123"
client = get_client(url=url_for_cli_deployment)

# Creamos un nuevo thread
thread = await client.threads.create()

# Iniciamos un nuevo run dentro del thread
user_input = "Realizame esta operaciÃ³n 5*2."
config = {"configurable": {"user_id": "Test"}}
graph_name = "calculator" 
run = await client.runs.create(thread["thread_id"], graph_name, input={"messages": [HumanMessage(content=user_input)]}, config=config)

# Mostramos el estado de ejecuciones en el thread
print(await client.runs.get(thread["thread_id"], run["run_id"]))
```

```json title="Resultado"
{
   "run_id":"1efedf2f-724e-60a1-92b6-19db6d246e4a",
   "thread_id":"8cb32afa-c3a9-4640-9e07-fedce27cc892",
   "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5",
   "created_at":"2025-02-18T12:22:05.633972+00:00",
   "updated_at":"2025-02-18T12:22:05.633972+00:00",
   "metadata":{
      "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5"
   },
   "status":"running",
   "kwargs":{
      "input":{
         "messages":[
            {
               "id":"None",
               "name":"None",
               "type":"human",
               "content":"Realizame esta operaciÃ³n 5*2.",
               "example":false,
               "additional_kwargs":{
                  
               },
               "response_metadata":{
                  
               }
            }
         ]
      },
      "config":{
         "metadata":{
            "created_by":"system",
            "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5"
         },
         "configurable":{
            "run_id":"1efedf2f-724e-60a1-92b6-19db6d246e4a",
            "user_id":"Test",
            "graph_id":"calculator",
            "thread_id":"8cb32afa-c3a9-4640-9e07-fedce27cc892",
            "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5",
            "langgraph_auth_user":"None",
            "langgraph_auth_user_id":"",
            "langgraph_auth_permissions":[
               
            ]
         }
      },
      "command":"None",
      "webhook":"None",
      "subgraphs":false,
      "temporary":false,
      "stream_mode":[
         "values"
      ],
      "feedback_keys":"None",
      "interrupt_after":"None",
      "interrupt_before":"None"
   },
   "multitask_strategy":"reject"
}
```

???+ Note "Nota"

    Si el estado del run aparece como `pending` o `running`, significa que el grafo aÃºn estÃ¡ en ejecuciÃ³n.  

ğŸ“Œ **Si queremos esperar el resultado del run antes de continuar:**  

En caso de quisieramos esperar a que el run termine (blocking or polling) para mostrar el resultado podemos usar `client.runs.join`.

```python
# Esperamos a que termine y mostramos el output
await client.runs.join(thread["thread_id"], run["run_id"])
print(await client.runs.get(thread["thread_id"], run["run_id"]))
```

```json
{
   "run_id":"1efedf2f-724e-60a1-92b6-19db6d246e4a",
   "thread_id":"8cb32afa-c3a9-4640-9e07-fedce27cc892",
   "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5",
   "created_at":"2025-02-18T12:22:05.633972+00:00",
   "updated_at":"2025-02-18T12:22:05.633972+00:00",
   "metadata":{
      "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5"
   },
   "status":"success",
   "kwargs":{
      "input":{
         "messages":[
            {
               "id":"None",
               "name":"None",
               "type":"human",
               "content":"Realizame esta operaciÃ³n 5*2.",
               "example":false,
               "additional_kwargs":{
                  
               },
               "response_metadata":{
                  
               }
            }
         ]
      },
      "config":{
         "metadata":{
            "created_by":"system",
            "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5"
         },
         "configurable":{
            "run_id":"1efedf2f-724e-60a1-92b6-19db6d246e4a",
            "user_id":"Test",
            "graph_id":"calculator",
            "thread_id":"8cb32afa-c3a9-4640-9e07-fedce27cc892",
            "assistant_id":"0d1ff216-6054-5926-941a-33f750dff0a5",
            "langgraph_auth_user":"None",
            "langgraph_auth_user_id":"",
            "langgraph_auth_permissions":[
               
            ]
         }
      },
      "command":"None",
      "webhook":"None",
      "subgraphs":false,
      "temporary":false,
      "stream_mode":[
         "values"
      ],
      "feedback_keys":"None",
      "interrupt_after":"None",
      "interrupt_before":"None"
   },
   "multitask_strategy":"reject"
}
```

---

## ğŸ”€ **3. Streaming Runs â€“ Recibir Resultados en Tiempo Real**  

En algunos casos, queremos **recibir respuestas parciales** mientras el grafo se ejecuta.  
Para ello, LangGraph SDK nos permite hacer **streaming de runs**.  

ğŸ“Œ **Ejemplo de streaming de un run:**  

```python
# Creamos una ejecuciÃ³n en streaming
user_input = "Realizame esta operaciÃ³n ((4+2)*2)/2."
async for chunk in client.runs.stream(thread["thread_id"], 
                                      graph_name, 
                                      input={"messages": [HumanMessage(content=user_input)]},
                                      config=config,
                                      stream_mode="messages-tuple"):

    # Filtramos que el evento sea "messages" y que el tipo de dato sera "AIMessageChunk" que es la respuesta de la ia
    if chunk.event == "messages":
        print("".join(
            data_item['content'] 
            for data_item in chunk.data 
            if data_item.get("type") == "AIMessageChunk" and 'content' in data_item
            ), end="", flush=True)
```

```python title="Resultado"
La operaciÃ³n \(((4+2) \times 2) / 2\) es igual a \(6\).
```

ğŸ”¹ **Ventajas del streaming:**  
âœ… Nos permite **visualizar resultados en tiempo real**.  
âœ… Es Ãºtil en **asistentes conversacionales** que generan respuesta progresivamente.  
âœ… Reduce la latencia en respuestas largas.  

???+ Note Nota

    Para obtener los token en stream debemos de usar **`stream_mode="messages-tuple"`** 

---

## ğŸ§µ **4. Threads â€“ OrganizaciÃ³n de Conversaciones y Runs**  

Un **Thread** en LangGraph agrupa mÃºltiples **runs** dentro de una misma conversaciÃ³n o contexto.  

ğŸ“Œ **Ejemplo: Crear y administrar un Thread en LangGraph**  

```python
from langchain_core.messages import convert_to_messages

thread_state = await client.threads.get_state(thread['thread_id'])
for m in convert_to_messages(thread_state['values']['messages']):
    m.pretty_print()
```

```python title="Resultado"
================================ Human Message =================================

Realizame esta operaciÃ³n ((4+2)*2)/2.
================================== Ai Message ==================================
Tool Calls:
  sumar (call_p75b81d9KwOL75B1MLBCqfDF)
 Call ID: call_p75b81d9KwOL75B1MLBCqfDF
  Args:
    a: 4
    b: 2
================================= Tool Message =================================
Name: sumar

6
================================== Ai Message ==================================
Tool Calls:
  multiplicar (call_iiFyJDKknNcSp7RmRL38Elld)
 Call ID: call_iiFyJDKknNcSp7RmRL38Elld
  Args:
    a: 6
    b: 2
================================= Tool Message =================================
Name: multiplicar

12
================================== Ai Message ==================================
Tool Calls:
  dividir (call_UzMKbkQoMkx3j7m5IjDvEewc)
 Call ID: call_UzMKbkQoMkx3j7m5IjDvEewc
  Args:
    a: 12
    b: 2
================================= Tool Message =================================
Name: dividir

6.0
================================== Ai Message ==================================

La operaciÃ³n \(((4+2) \times 2) / 2\) es igual a \(6\).
```

ğŸ“Œ **Ejemplo: Listar Threads en LangGraph**  

```python
# Listamos todos los threads disponibles
threads = await client.threads.search()

for t in threads:
    print(t)
```

ğŸ“Œ **Ejemplo: ELiminar Threads en LangGraph**  

```python
await client.threads.delete(
    thread_id="<THREAD_ID>"
)
```

ğŸ”¹ **Diferencia clave entre threads y runs:**  
- Un **thread** agrupa mÃºltiples ejecuciones relacionadas.  
- Un **run** es una Ãºnica ejecuciÃ³n dentro de un thread.  

ğŸ”¹ **Casos de uso:**  
âœ… **Chatbots** que necesitan agrupar interacciones de un usuario en una misma sesiÃ³n.  
âœ… **Flujos conversacionales complejos** donde varios runs deben estar conectados.  

---

## ğŸ‘©â€ğŸ’» **5. Human in the Loop â€“ IntegraciÃ³n de IntervenciÃ³n Humana**  

Como vimos en el **[Curso 2, Tema 4: Human in the Loop](../curso2/tema4_human_loop.md)**, LangGraph permite pausar el flujo del grafo y solicitar intervenciÃ³n humana.  

ğŸ“Œ **Ejemplo: Usar Human in the Loop con la SDK**  

```python
# Una vez detenido por la iterrupciÃ³n del grafo, volveriamos a ejecutarlo de la siguiente manera 
async for chunk in client.runs.stream(thread["thread_id"], 
                                      graph_name, 
                                      input=Command(resume="SI"),
                                      config=config,                                      
                                      stream_mode="messages-tuple"):

    if chunk.event == "messages":
        print("".join(data_item['content'] for data_item in chunk.data if 'content' in data_item), end="", flush=True)
```

ğŸ”¹ **Casos de uso:**  
âœ… **Validar respuestas antes de ejecutarlas**.  
âœ… **Solicitar confirmaciÃ³n de un usuario** antes de tomar una decisiÃ³n crÃ­tica.  

---

## ğŸ—„ï¸ **6. Manejo del Store con la SDK**  

Hemos explorado **LangGraph Store** en los **Temas 1, 2 y 3** de este curso.  
Ahora veremos cÃ³mo interactuar con estos datos mediante la SDK.  

ğŸ“Œ **Ejemplo: Almacenar datos en el Store**  

```python
from uuid import uuid4

# Almacenamos los datos
await client.store.put_item(
    ("testing", "Test"),
    key=str(uuid4()),
    value={"todo": "Probando aÃ±adir desde la SDK"},
)
```

ğŸ“Œ **Ejemplo: Buscar informaciÃ³n en la base de datos de LangGraph**  

```python
# Buscamos todos los valores en un namespace especÃ­fico
items = await client.store.search_items(
    ("testing", "Test"),
    limit=5,
    offset=0
)

# Mostramos los resultados de la bÃºsqueda
for item in items['items']:
    print(item)
```

???+ Note "Nota"

    Tal y como vimos en los temas sobre `Store`, puedes consultar mÃ¡s mÃ©todos en la documentaciÃ³n oficial:  
    **[StoreClient](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.client.StoreClient)**  


---

## âœ¨ **ConclusiÃ³n**  

La SDK de LangGraph nos permite **conectar, ejecutar y gestionar grafos remotos** de manera eficiente.  

âœ… Podemos ejecutar grafos alojados en **LangGraph Cloud** o **Docker**.  
âœ… Podemos administrar **ejecuciones (runs) y sesiones (threads)**.  
âœ… Podemos utilizar **streaming** para recibir resultados en tiempo real.  
âœ… Podemos integrar **Human in the Loop** y manipular datos almacenados en **LangGraph Store**.  

ğŸš€ **Con estos conocimientos, podemos integrar LangGraph en cualquier aplicaciÃ³n de manera flexible y escalable.**  

---

## ğŸ§‘â€ğŸ« **Â¿QuÃ© Hemos Aprendido?**  

- CÃ³mo conectarnos a grafos remotos usando **RemoteGraph**.  
- CÃ³mo gestionar ejecuciones mediante **background runs**.  
- CÃ³mo recibir datos en tiempo real con **streaming runs**.  
- CÃ³mo organizar interacciones con **threads**.  
- CÃ³mo usar **Human in the Loop** dentro de la SDK.  
- CÃ³mo acceder a **datos almacenados en LangGraph Store**.  

---

## ğŸ” Recursos:

- :simple-googlecolab: Ver notebook en [Google Colab](https://colab.research.google.com/drive/15axod42AkitpQzDsB_IF_i9sHLslLZ7L?usp=sharing)
- :books: Concept: [LangGraph SDK](https://langchain-ai.github.io/langgraph/concepts/sdk/)
- :books: How-to-guide: [Runs](https://langchain-ai.github.io/langgraph/how-tos/#runs)
- :books: How-to-guide: [RemoteGraph](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/)
- :books: How-to-guide: [stream_mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/#stream-graph-in-messages-mode)
- :books: How-to-guide: [Check status of your threads](https://langchain-ai.github.io/langgraph/cloud/how-tos/check_thread_status/)
- :books: API Reference: [Langgraph SDK](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/)
- :books: Class: [StoreClient](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.client.StoreClient)
- :books: Ejemplos: [Background Runs](https://langchain-ai.github.io/langgraph/cloud/how-tos/background_run/)

---

## ğŸŒ **Â¿QuÃ© es lo Siguiente?**  

En el prÃ³ximo tema, exploraremos **Doble-Texting**, una tÃ©cnica para manejar mÃºltiples mensajes simultÃ¡neos en un grafo.  
Analizaremos diferentes estrategias como **reject, enqueue, interrupt y rollback** para gestionar estos eventos correctamente.  