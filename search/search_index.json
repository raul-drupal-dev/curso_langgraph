{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\uddd1\u200d\ud83d\udcbb Curso Completo de LangGraph 0.3.xx","text":""},{"location":"#construccion-de-flujos-avanzados-con-llms","title":"Construcci\u00f3n de Flujos Avanzados con LLMs","text":""},{"location":"#bienvenido-al-curso-de-langgraph-03xx","title":"\ud83d\ude80 \u00a1Bienvenido al Curso de LangGraph 0.3.xx!","text":"<p>Este curso est\u00e1 dise\u00f1ado para ense\u00f1arte desde los conceptos m\u00e1s b\u00e1sicos hasta la creaci\u00f3n de un asistente virtual avanzado usando LangGraph 0.3.xx.  </p> <p>Si buscas dominar el desarrollo de flujos de trabajo con modelos de lenguaje grande (LLMs) y optimizar tus proyectos de IA, est\u00e1s en el lugar adecuado.  </p> <p>LangGraph es una herramienta poderosa para la creaci\u00f3n de pipelines modulares, paralelos y escalables, con capacidades que te permitir\u00e1n llevar tus aplicaciones de IA al siguiente nivel.  </p>"},{"location":"#que-aprenderas-en-este-curso","title":"\ud83d\udccb \u00bfQu\u00e9 Aprender\u00e1s en Este Curso?","text":"<p>A lo largo de este curso, adquirir\u00e1s habilidades pr\u00e1cticas y te\u00f3ricas para:  </p> <ul> <li>\u2705 Construir flujos de trabajo avanzados con LangGraph.  </li> <li>\u2705 Crear pipelines din\u00e1micos y modulares que interact\u00faan con LLMs.  </li> <li>\u2705 Implementar memoria a corto y largo plazo en tus aplicaciones.  </li> <li>\u2705 Desplegar y escalar tus proyectos de IA en entornos reales.  </li> <li>\u2705 Desarrollar un asistente virtual completo con memoria y capacidad de respuesta avanzada.  </li> </ul>"},{"location":"#objetivos-del-curso","title":"\ud83c\udfaf Objetivos del Curso","text":"<p>Al finalizar, podr\u00e1s: - Construir pipelines completos con LangGraph que manejan consultas, flujos paralelos y enrutamiento condicional. - Crear asistentes virtuales con memoria persistente. - Optimizar y monitorizar flujos con herramientas como LangSmith y LangGraph Studio. - Desplegar tus aplicaciones de LangGraph en producci\u00f3n utilizando LangGraph Local Server o CLI.  </p>"},{"location":"#estructura-del-curso","title":"\ud83d\udcc2 Estructura del Curso","text":"<p>El curso se divide en tres m\u00f3dulos principales, con teor\u00eda, ejemplos de c\u00f3digo y ejercicios pr\u00e1cticos:  </p>"},{"location":"#modulo-1-fundamentos-y-componentes-basicos-de-langgraph","title":"M\u00f3dulo 1: Fundamentos y Componentes B\u00e1sicos de LangGraph","text":"<ul> <li>\u00bfQu\u00e9 es LangGraph y para qu\u00e9 sirve? </li> <li>Creaci\u00f3n de nodos (Nodes) y conexiones (Edges). </li> <li>Control del estado y memoria (State Schema y Postgres). </li> <li>Construcci\u00f3n de flujos con chains y routers. </li> <li>Uso de herramientas (Tools)</li> </ul>"},{"location":"#modulo-2-aplicaciones-avanzadas-y-flujos-complejos","title":"M\u00f3dulo 2: Aplicaciones Avanzadas y Flujos Complejos","text":"<ul> <li>Construcci\u00f3n de chatbots resumidores (Summarizing). </li> <li>Implementaci\u00f3n de puntos de interrupci\u00f3n y streaming. </li> <li>Ejecuci\u00f3n paralela de tareas y subgraf\u00edas. </li> <li>T\u00e9cnicas de map-reduce </li> <li>Uso de LangGraph Studio y LangSmith para visualizar y debugear flujos. </li> </ul>"},{"location":"#modulo-3-despliegue-y-proyecto-final","title":"M\u00f3dulo 3: Despliegue y Proyecto Final","text":"<ul> <li>Memoria a largo plazo con LangGraph Store. </li> <li>Memory schema y LangGrpah Store </li> <li>Uso de LangGraph CLI </li> <li>Introducci\u00f3n a LangGraph SDK (Python) </li> <li>Repaso a las t\u00e9cnicas de doble-texting </li> <li>Proyecto final con API REST/GraphQL (Comming Soon)</li> <li>Explorar LangFlow</li> </ul>"},{"location":"#requisitos-previos","title":"\ud83d\udee0\ufe0f Requisitos Previos","text":"<ul> <li>Conocimientos b\u00e1sicos de Python. </li> <li>Familiaridad con modelos de lenguaje grande (LLMs).  </li> <li>Experiencia en LangChain (opcional).  </li> </ul>"},{"location":"#para-quien-esta-dirigido-este-curso","title":"\ud83d\udccc \u00bfPara Qui\u00e9n Est\u00e1 Dirigido Este Curso?","text":"<p>Este curso es ideal para: - \ud83e\uddd1\u200d\ud83d\udcbb Desarrolladores de IA que quieren construir pipelines avanzados. - \ud83d\udcca Ingenieros de datos que buscan optimizar flujos de procesamiento de lenguaje natural. - \ud83d\udcda Investigadores y entusiastas que desean integrar LLMs en proyectos reales.  </p>"},{"location":"#resultado-final-del-curso","title":"\ud83c\udfc6 Resultado Final del Curso","text":"<p>Al completar el curso, habr\u00e1s desarrollado un asistente virtual funcional que: - Gestiona m\u00faltiples consultas con LLMs. - Mantiene memoria y contexto de conversaciones previas. - Ejecuci\u00f3n de tareas paralelas y flujos complejos. - Es capaz de desplegarse y escalarse en entornos de producci\u00f3n. </p>"},{"location":"#tecnologias-utilizadas","title":"\ud83d\udee0\ufe0f Tecnolog\u00edas Utilizadas","text":"<ul> <li>LangGraph 0.3.xx </li> <li>LangChain (opcional) </li> <li>LangSmith (trazado y depuraci\u00f3n) </li> <li>LangGraph Studio (Visualizaci\u00f3n de grafos)</li> <li>OpenAI (u otros LLMs) </li> <li>Postgres (para memoria de corto plazo) </li> </ul> <p>\ud83c\udf1f \u00a1Empieza tu viaje ahora y lleva tus habilidades de IA al siguiente nivel! </p>"},{"location":"curso1/","title":"\ud83d\udcda Curso 1: Fundamentos de LangGraph","text":""},{"location":"curso1/#bienvenida-al-curso-1","title":"\ud83d\udc4b Bienvenida al Curso 1","text":"<p>\u00a1Bienvenidos al primer curso de Fundamentos de LangGraph! \ud83d\ude80 En este curso, nos sumergiremos en los conceptos esenciales para dominar LangGraph, una poderosa herramienta para construir flujos de trabajo complejos con modelos de lenguaje. Desde nodos y conexiones hasta la gesti\u00f3n de memoria y reducci\u00f3n de resultados, cubriremos todos los pilares clave que necesitas para crear aplicaciones con LLMs.</p>"},{"location":"curso1/#que-aprenderemos","title":"\ud83c\udfaf \u00bfQu\u00e9 Aprenderemos?","text":"<ul> <li>Comprender la estructura b\u00e1sica de LangGraph y sus ventajas.  </li> <li>Crear y conectar nodos para formar flujos de trabajo.  </li> <li>Implementar routers para enrutar din\u00e1micamente los datos.  </li> <li>Utilizar reducers para consolidar y filtrar informaci\u00f3n.  </li> <li>Manejar la memoria de estado (state schema) para flujos persistentes.</li> <li>Utilizar Tools para ejecutar acciones externas al grafo.  </li> <li>Optimizar y filtrar mensajes con t\u00e9cnicas avanzadas de trim y filter.  </li> </ul>"},{"location":"curso1/#objetivo-del-curso","title":"\ud83c\udfc6 Objetivo del Curso","text":"<p>Al finalizar este curso, ser\u00e1s capaz de construir flujos de trabajo de LLM desde cero, comprender c\u00f3mo funciona LangGraph a nivel fundamental y aplicar este conocimiento para construir soluciones reales y escalables.  </p>"},{"location":"curso1/#temario","title":"\ud83d\udccb Temario","text":"<ol> <li>Tema 0: Que es LangGraph? </li> <li>Tema 1: Nodos </li> <li>Tema 2: Edges (Conexiones) </li> <li>Tema 3: State Schema </li> <li>Tema 3: Memoria </li> <li>Tema 5: Chains y Flujos de Trabajo </li> <li>Tema 6: Routers </li> <li>Tema 7: Reducers </li> <li>Tema 8: tools </li> <li>Tema 9: Trim y Filtrado de Mensajes </li> </ol>"},{"location":"curso1/#resultado-final-del-curso","title":"\ud83c\udfc1 Resultado Final del Curso","text":"<ul> <li>Habr\u00e1s construido flujos b\u00e1sicos de LLM usando LangGraph.  </li> <li>Tendr\u00e1s una base s\u00f3lida para afrontar proyectos avanzados en los siguientes cursos.  </li> <li>Entender\u00e1s los conceptos b\u00e1sicos de memoria, rutas y nodos aplicados en pipelines de procesamiento.  </li> </ul>"},{"location":"curso1/#tecnologias-utilizadas","title":"\u2699\ufe0f Tecnolog\u00edas Utilizadas","text":"<ul> <li>LangGraph 0.3.x </li> <li>Python 3.10+ </li> <li>FastAPI </li> <li>PostgreSQL para manejo de memoria  </li> <li>Google Colab (opcional para pruebas)  </li> </ul>"},{"location":"curso1/tema0_que_es_langgraph/","title":"\ud83c\udf1f \u00bfQu\u00e9 es LangGraph?","text":""},{"location":"curso1/tema0_que_es_langgraph/#introduccion","title":"\ud83e\udde0 Introducci\u00f3n","text":"<p>Imagina un mundo donde puedes construir flujos conversacionales inteligentes, manejar datos complejos y coordinar m\u00faltiples herramientas con un simple modelo de lenguaje. Eso es lo que LangGraph te permite hacer: transformar ideas en sistemas poderosos basados en inteligencia artificial, de manera modular, escalable y eficiente.  </p>"},{"location":"curso1/tema0_que_es_langgraph/#que-es-langgraph_1","title":"\ud83d\ude80 \u00bfQu\u00e9 es LangGraph?","text":"<p>LangGraph es una biblioteca de Python dise\u00f1ada para trabajar con modelos de lenguaje grandes (LLMs). Su prop\u00f3sito principal es facilitar la construcci\u00f3n de grafos din\u00e1micos donde: - Cada nodo representa una tarea espec\u00edfica (por ejemplo, procesar mensajes o ejecutar una herramienta). - Los edges controlan c\u00f3mo fluye la informaci\u00f3n entre los nodos.  </p> Nota <p>Es como crear un mapa de decisiones o un sistema l\u00f3gico que evoluciona en tiempo real dependiendo de la entrada del usuario.  </p>"},{"location":"curso1/tema0_que_es_langgraph/#para-que-sirve-langgraph","title":"\ud83c\udf0d \u00bfPara Qu\u00e9 Sirve LangGraph?","text":"<p>LangGraph extiende las capacidades de los modelos LLM al proporcionar una estructura para: - Crear asistentes virtuales avanzados que entienden y recuerdan conversaciones. - Dise\u00f1ar sistemas de soporte t\u00e9cnico que redirigen a los usuarios autom\u00e1ticamente seg\u00fan sus preguntas. - Coordinar m\u00faltiples herramientas y APIs para ofrecer respuestas din\u00e1micas y precisas.  </p>"},{"location":"curso1/tema0_que_es_langgraph/#caracteristicas-clave","title":"\ud83d\udd11 Caracter\u00edsticas Clave","text":"<ol> <li>Modularidad: Construye grafos paso a paso, reutilizando nodos y funciones.  </li> <li>Memoria: Integra memoria de corto y largo plazo para mantener el contexto.  </li> <li>Flexibilidad: Crea flujos simples o complejos seg\u00fan las necesidades del proyecto.  </li> <li>Escalabilidad: Perfecto para proyectos peque\u00f1os o sistemas empresariales.  </li> <li>Integraci\u00f3n: Est\u00e1 pensado para usarse con los principales LLM del mercado.</li> </ol>"},{"location":"curso1/tema0_que_es_langgraph/#casos-de-uso-comunes","title":"\u2728 Casos de Uso Comunes","text":"<ul> <li>Chatbots Inteligentes: Responden preguntas frecuentes, realizan c\u00e1lculos o conectan con APIs externas.  </li> <li>Soporte T\u00e9cnico Automatizado: Redirige a departamentos espec\u00edficos como soporte, ventas o consultas generales.  </li> <li>Procesamiento de Datos: Maneja flujos de datos complejos, como res\u00famenes autom\u00e1ticos o an\u00e1lisis en tiempo real.  </li> <li>Aplicaciones Empresariales: Sistemas de workflow que combinan tareas manuales y automatizadas.  </li> </ul>"},{"location":"curso1/tema0_que_es_langgraph/#por-que-langgraph","title":"\ud83d\udca1 \u00bfPor Qu\u00e9 LangGraph?","text":"<p>LangGraph no solo organiza flujos de trabajo, sino que tambi\u00e9n te permite desbloquear el verdadero potencial de los modelos LLM. Con esta herramienta, podr\u00e1s dise\u00f1ar sistemas adaptables y din\u00e1micos que se ajusten a cualquier reto o escenario que enfrentes.  </p> <p>Adem\u00e1s, gracias a langGraph podras utilizar (o intercambiar) los diferentes <code>LLM</code> del mercado sin modificar tu c\u00f3digo principal.</p>"},{"location":"curso1/tema0_que_es_langgraph/#preparate-para-empezar","title":"\ud83d\ude80 \u00a1Prep\u00e1rate para Empezar!","text":"<p>\u00bfEst\u00e1s listo para sumergirte en el mundo de LangGraph? A lo largo de este curso, explorar\u00e1s c\u00f3mo construir grafos desde cero, dominar\u00e1s sus componentes clave y aprender\u00e1s a aplicarlos en el mundo real. \u00a1El viaje apenas comienza!  </p>"},{"location":"curso1/tema1_nodos/","title":"\ud83e\udde9 Tema 1: Nodos en LangGraph","text":""},{"location":"curso1/tema1_nodos/#que-es-un-nodo","title":"\ud83e\udde9 \u00bfQu\u00e9 es un Nodo?","text":"<p>En LangGraph, los nodos representan las operaciones fundamentales que se realizan en un flujo de trabajo. Cada nodo se encarga de procesar datos o estados, y a partir de esto, devolver un nuevo estado o resultado. Piensa en los nodos como piezas individuales de una m\u00e1quina que trabajan juntas para completar una tarea compleja.  </p> <p>Los nodos est\u00e1n conectados mediante aristas (edges) que permiten el flujo de informaci\u00f3n entre ellos, formando un grafo que ejecuta operaciones de manera secuencial o condicional.  </p>"},{"location":"curso1/tema1_nodos/#como-funciona-un-nodo","title":"\ud83d\udee0\ufe0f \u00bfC\u00f3mo Funciona un Nodo?","text":"<ul> <li>Un nodo es una funci\u00f3n que toma un estado de entrada y devuelve un nuevo estado.  </li> <li>Los nodos pueden representar tareas simples (como sumar n\u00fameros) o procesos m\u00e1s complejos (como ejecutar un modelo de IA).  </li> <li>Cada nodo tiene una conexi\u00f3n hacia otros nodos, permitiendo que el flujo contin\u00fae seg\u00fan la l\u00f3gica definida.  </li> </ul>"},{"location":"curso1/tema1_nodos/#ejemplo-construyendo-un-grafo-con-nodos","title":"\ud83d\ude80 Ejemplo: Construyendo un Grafo con Nodos","text":"<p>Para entender mejor c\u00f3mo funcionan los nodos, vamos a construir un grafo simple utilizando LangGraph.  </p> <p>El objetivo ser\u00e1 simular un grafo que cambia de estado y elige diferentes caminos en funci\u00f3n de decisiones aleatorias.  </p>"},{"location":"curso1/tema1_nodos/#1-instalacion-de-langgraph","title":"1. Instalaci\u00f3n de LangGraph","text":"<p>Primero, asegur\u00e9monos de tener instalada la librer\u00eda LangGraph. <pre><code>%pip install --quiet -U langgraph\n</code></pre></p>"},{"location":"curso1/tema1_nodos/#2-definiendo-el-estado-del-grafo","title":"2. Definiendo el Estado del Grafo","text":"<p>El estado del grafo es una estructura que guarda informaci\u00f3n sobre el progreso y los datos en cada paso. Aqu\u00ed definimos una clase <code>State</code> que representa el estado del grafo con una variable <code>graph_state</code> de tipo texto. <pre><code>from typing_extensions import TypedDict\n\nclass State(TypedDict):\n    graph_state: str\n</code></pre></p>"},{"location":"curso1/tema1_nodos/#3-creando-los-nodos","title":"3. Creando los Nodos","text":"<p>Ahora crearemos tres nodos simples: 1. Nodo 1: Modifica el estado inicial agregando \"Me gusta. 2. Nodo 2: A\u00f1ade \"programar!\" al estado. 3. Nodo 3: A\u00f1ade \"salir en bici!\" al estado.  </p> <p>Cada nodo es una funci\u00f3n que recibe el estado actual (<code>state</code>) y devuelve un nuevo estado modificado. <pre><code>def node_1(state):\n    print(\"---Node 1---\")\n    return {\"graph_state\": state['graph_state'] + \" Me gusta\"}\n\ndef node_2(state):\n    print(\"---Node 2---\")\n    return {\"graph_state\": state['graph_state'] + \" programar!\"}\n\ndef node_3(state):\n    print(\"---Node 3---\")\n    return {\"graph_state\": state['graph_state'] + \" salir en bici!\"}\n</code></pre></p>"},{"location":"curso1/tema1_nodos/#4-agregando-logica-de-decision","title":"4. Agregando L\u00f3gica de Decisi\u00f3n","text":"<p>Aqu\u00ed agregamos una funci\u00f3n <code>decide_hooby</code> que decide aleatoriamente si el flujo contin\u00faa hacia el Nodo 2 (programar) o el Nodo 3 (salir en bici). Esto simula una bifurcaci\u00f3n en el camino que depender\u00e1 del resultado de una probabilidad del 50%. <pre><code>import random\nfrom typing import Literal\n\ndef decide_hooby(state) -&gt; Literal[\"node_2\", \"node_3\"]:\n    if random.random() &lt; 0.5:\n        return \"node_2\"\n    return \"node_3\"\n</code></pre></p>"},{"location":"curso1/tema1_nodos/#5-construccion-del-grafo","title":"5. Construcci\u00f3n del Grafo","text":"<p>En este paso, construimos el grafo utilizando <code>StateGraph</code>.   </p> <p><pre><code>from IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\"node_1\", decide_hooby)\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\n</code></pre> Vamos a explicar l\u00ednea por l\u00ednea: </p> <ol> <li> <p><code>StateGraph(State)</code>:  </p> <ul> <li>Creamos una instancia de <code>StateGraph</code> y le pasamos el estado que definimos antes (<code>State</code>).  </li> <li>\u00bfPor qu\u00e9 lo hacemos?: LangGraph necesita saber qu\u00e9 tipo de estado manejar\u00e1 el grafo.   </li> </ul> </li> <li> <p><code>builder.add_node(\"node_1\", node_1)</code>:  </p> <ul> <li>A\u00f1adimos el Nodo 1 al grafo.  </li> <li>El primer par\u00e1metro <code>\"node_1\"</code> es el identificador del nodo, y el segundo (<code>node_1</code>) es la funci\u00f3n que ejecutar\u00e1.  </li> </ul> </li> <li> <p><code>builder.add_node(\"node_2\", node_2)</code>:  </p> <ul> <li>Agregamos el Nodo 2. Funciona de la misma forma que el Nodo 1.  </li> </ul> </li> <li> <p><code>builder.add_node(\"node_3\", node_3)</code>:  </p> <ul> <li>Agregamos el Nodo 3 al grafo.  </li> </ul> </li> <li> <p><code>builder.add_edge</code>:</p> <ul> <li>Agregamos los edges, esto lo veremos m\u00e1s en profundidad en el siguiente tema.</li> </ul> </li> <li> <p><code>Finalmente, compilamos el grafo</code>:  </p> <ul> <li><code>graph = builder.compile()</code>: Ensambla el grafo con todos los nodos y conexiones. </li> </ul> </li> </ol>"},{"location":"curso1/tema1_nodos/#6-visualizacion-del-grafo","title":"6. Visualizaci\u00f3n del Grafo","text":"<p>Para visualizar el grafo, usamos la funci\u00f3n <code>draw_mermaid_png()</code> que genera un diagrama del grafo. Esto nos permite ver gr\u00e1ficamente c\u00f3mo est\u00e1n conectados los nodos. </p> <pre><code>display(Image(graph.get_graph().draw_mermaid_png())) \n</code></pre> <p></p>"},{"location":"curso1/tema1_nodos/#7-invocamos-el-grafo","title":"7. Invocamos el Grafo","text":"<p>Para invocar el grafo, simplemente usamos el siguiente c\u00f3digo: </p> <p><pre><code>graph.invoke({\"graph_state\" : \"Hola, me llamo Raul.\"})\n</code></pre> Resultado 1<pre><code>---Node 1---\n---Node 3---\n{'graph_state': 'Hola, me llamo Raul. Me gusta programar!'}\n</code></pre> Resultado 2<pre><code>---Node 1---\n---Node 3---\n{'graph_state': 'Hola, me llamo Raul. Me gusta salir en bici!'}\n</code></pre></p> <p>Como vemos, nuestro grafo ha creado dos cadenas diferentes basado en la aleatoriedad. Con esto podemos tener una idea b\u00e1sica de como funcionan los nodos y cual es su principal funci\u00f3n.</p>"},{"location":"curso1/tema1_nodos/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Definici\u00f3n: Nodos</li> </ul>"},{"location":"curso1/tema1_nodos/#que-hemos-aprendido","title":"\ud83e\udde9 \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Nodos: Son funciones que procesan datos y modifican el estado de un grafo.  </li> <li>Decisiones Condicionales: Podemos agregar l\u00f3gica de bifurcaci\u00f3n para que el flujo tome diferentes caminos.  </li> <li>StateGraph: Es la base sobre la que construimos el grafo, a\u00f1adiendo nodos y conexiones.  </li> <li>Visualizaci\u00f3n: LangGraph permite visualizar gr\u00e1ficamente el flujo de trabajo.  </li> </ul>"},{"location":"curso1/tema1_nodos/#siguientes-pasos","title":"\ud83c\udf10 Siguientes Pasos","text":"<p>En el siguiente tema aprenderemos sobre Edges (Conexiones), que permiten conectar nodos y controlar el flujo de informaci\u00f3n entre ellos.  </p>"},{"location":"curso1/tema2_edges/","title":"\ud83c\udfaf Tema 2: Edges (Conexiones)","text":""},{"location":"curso1/tema2_edges/#que-son-los-edges","title":"\ud83e\udde9 \u00bfQu\u00e9 son los Edges?","text":"<p>Los edges son las conexiones entre nodos que dirigen el flujo de datos de un nodo a otro. Estas conexiones pueden ser:  </p> <ul> <li>Directas: Conectan un nodo con el siguiente sin condiciones.  </li> <li>Condicionales: Deciden din\u00e1micamente qu\u00e9 nodo visitar a continuaci\u00f3n bas\u00e1ndose en una funci\u00f3n l\u00f3gica.  </li> </ul> <p>En este ejemplo, utilizaremos un flujo de decisi\u00f3n aleatorio para mostrar c\u00f3mo se pueden agregar edges y bifurcaciones.  </p>"},{"location":"curso1/tema2_edges/#ejemplo-practico-construyendo-un-grafo-con-edges","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico: Construyendo un Grafo con Edges","text":"<p>Usando el c\u00f3digo usado anteriormente con los nodos, mostramos el c\u00f3digo de ejemplo que ilustra c\u00f3mo agregar edges entre nodos:  </p> <pre><code>def node_1(state):\n    print(\"---Node 1---\")\n    return {\"graph_state\": state['graph_state'] + \" Me gusta\"}\n\ndef node_2(state):\n    print(\"---Node 2---\")\n    return {\"graph_state\": state['graph_state'] + \" programar!\"}\n\ndef node_3(state):\n    print(\"---Node 3---\")\n    return {\"graph_state\": state['graph_state'] + \" salir en bici!\"}\n</code></pre>"},{"location":"curso1/tema2_edges/#explicacion-paso-a-paso","title":"\ud83d\udd0d Explicaci\u00f3n Paso a Paso","text":""},{"location":"curso1/tema2_edges/#definicion-de-nodos","title":"\ud83d\udccc Definici\u00f3n de Nodos","text":"<pre><code>from langgraph.graph import StateGraph, START, END\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n</code></pre> <ul> <li><code>node_1</code>: A\u00f1ade al estado <code>\"Me gusta\"</code>.  </li> <li><code>node_2</code>: A\u00f1ade al estado <code>\"programar!\"</code>.  </li> <li><code>node_3</code>: A\u00f1ade al estado <code>\"salir en bici!\"</code>.  </li> </ul> <p>En este caso, <code>node_1</code> act\u00faa como punto de partida para las decisiones que se tomar\u00e1n posteriormente.  </p>"},{"location":"curso1/tema2_edges/#decision-con-condiciones","title":"\ud83d\udd04 Decisi\u00f3n con Condiciones","text":"<pre><code>import random\nfrom typing import Literal\n\ndef decide_hooby(state) -&gt; Literal[\"node_2\", \"node_3\"]:\n    if random.random() &lt; 0.5:\n        return \"node_2\"\n    return \"node_3\"\n</code></pre> <p>La funci\u00f3n <code>decide_hooby</code> decide, de forma aleatoria, si el flujo debe continuar hacia <code>node_2</code> o <code>node_3</code>.  </p> <ul> <li>50% de probabilidad de ir a <code>node_2</code> (programar).  </li> <li>50% de probabilidad de ir a <code>node_3</code> (salir en bici).  </li> </ul> <p>Este edge condicional permite que el grafo tenga m\u00faltiples rutas de ejecuci\u00f3n.  </p>"},{"location":"curso1/tema2_edges/#construccion-del-grafo","title":"\ud83c\udfd7\ufe0f Construcci\u00f3n del Grafo","text":"<pre><code>builder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\"node_1\", decide_hooby)\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\n</code></pre> <ul> <li><code>add_edge</code>: Conecta directamente <code>START</code> con <code>node_1</code>.  </li> <li><code>add_conditional_edges</code>: Desde <code>node_1</code>, el flujo se bifurca condicionalmente hacia <code>node_2</code> o <code>node_3</code>.  </li> <li><code>add_edge</code> (Final): Ambos nodos (<code>node_2</code> y <code>node_3</code>) terminan en el nodo final <code>END</code>.  </li> </ul>"},{"location":"curso1/tema2_edges/#visualizando-el-grafo","title":"\ud83d\udcc8 Visualizando el Grafo","text":"<p>Una vez construido, podemos visualizar el grafo:  </p> <pre><code>from IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre>"},{"location":"curso1/tema2_edges/#invocando-el-grafo","title":"\u2699\ufe0f Invocando el Grafo","text":"<p>Para ejecutar el grafo y ver su comportamiento, utilizamos:  </p> <p><pre><code>graph.invoke({\"graph_state\" : \"Hola, me llamo Raul.\"})\n</code></pre> Resultado 1<pre><code>---Node 1---\n---Node 3---\n{'graph_state': 'Hola, me llamo Raul. Me gusta programar!'}\n</code></pre> Resultado 2<pre><code>---Node 1---\n---Node 3---\n{'graph_state': 'Hola, me llamo Raul. Me gusta salir en bici!'}\n</code></pre></p>"},{"location":"curso1/tema2_edges/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Gu\u00eda: Edges</li> </ul>"},{"location":"curso1/tema2_edges/#que-hemos-aprendido","title":"\ud83e\udde9 \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Edges (Conexiones): Son las rutas que conectan nodos en el grafo, permitiendo el flujo de informaci\u00f3n de un nodo a otro.  </li> <li>Edges Condicionales: Permiten bifurcaciones en el flujo, tomando decisiones din\u00e1micas que afectan el camino que sigue el grafo.  </li> <li>Flujo de Trabajo Din\u00e1mico: Al conectar nodos de forma condicional, podemos crear grafos m\u00e1s complejos y adaptativos, generando diferentes resultados seg\u00fan la l\u00f3gica aplicada.  </li> <li>Construcci\u00f3n de Grafos: Hemos aprendido a agregar nodos, edges y compilar el grafo usando <code>StateGraph</code> y <code>add_edge</code>.  </li> </ul>"},{"location":"curso1/tema2_edges/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el siguiente tema, profundizaremos en State Schema y Memoria. Aprenderemos: - C\u00f3mo almacenar y mantener el estado a lo largo del flujo del grafo. - Estrategias para preservar memoria en LangGraph. - Esquemas de estado personalizados para que el grafo retenga y modifique informaci\u00f3n de manera continua.  </p> <p>\u00a1Nos vemos en el pr\u00f3ximo tema! \ud83d\ude80  </p>"},{"location":"curso1/tema3_state_schema/","title":"\ud83e\udde9 Tema 3: State Schema","text":""},{"location":"curso1/tema3_state_schema/#que-es-el-state-schema","title":"\ud83d\ude80 \u00bfQu\u00e9 es el State Schema?","text":"<p>El State Schema en LangGraph define la estructura de datos que viaja a trav\u00e9s de un grafo durante su ejecuci\u00f3n. Es como el \u201cADN\u201d del flujo, que transporta y actualiza informaci\u00f3n a medida que se avanza por diferentes nodos, piensa en el estado como una mochila \ud83e\uddf3 que lleva datos entre los nodos, la cual vamos llenando o vaciando a medida que vamos necesitando.</p>"},{"location":"curso1/tema3_state_schema/#por-que-es-importante","title":"\ud83e\udde0 \u00bfPor qu\u00e9 es importante?","text":"<p>El State Schema permite: - Controlar y validar los datos que circulan por el grafo. - Definir tipos de datos espec\u00edficos (texto, listas, enteros, etc.) para evitar errores. - Modificar y actualizar atributos a lo largo del flujo.  </p> <p>Imagina que est\u00e1s construyendo un asistente virtual: - El State podr\u00eda contener atributos como el nombre del usuario, historial de mensajes y preferencias. - A medida que la conversaci\u00f3n avanza, los nodos modifican o consultan estos atributos.  </p>"},{"location":"curso1/tema3_state_schema/#como-se-define-el-state-schema","title":"\u2699\ufe0f \u00bfC\u00f3mo se Define el State Schema?","text":"<p>Para definir un <code>State</code> personalizado, utilizamos <code>TypedDict</code>. Esto nos permite crear una plantilla de estado con atributos espec\u00edficos y sus respectivos tipos.  </p>"},{"location":"curso1/tema3_state_schema/#ejemplo-creacion-de-un-state-con-varios-atributos","title":"\ud83d\udccb Ejemplo: Creaci\u00f3n de un State con Varios Atributos","text":"<pre><code>from typing_extensions import TypedDict\nfrom typing import Literal\n\nclass State(TypedDict):\n    name: str\n    age: int\n    preferences: Literal[\"Videojuegos\",\"Programaci\u00f3n\"]\n</code></pre>"},{"location":"curso1/tema3_state_schema/#explicacion","title":"\ud83e\udde9 Explicaci\u00f3n:","text":"<ul> <li><code>TypedDict</code> crea una plantilla para el estado.  </li> <li><code>name</code> y <code>age</code> son atributos que definen el nombre y edad de un usuario.  </li> <li><code>preferences</code> es una lista que guarda intereses o preferencias. Usando <code>Literal</code> forzamos a que la aplicaci\u00f3n solamente guarde esos terminos en concreto.  </li> </ul> <p>Con esto, aseguramos que el estado tenga una estructura clara y solo acepte los tipos de datos definidos.  </p>"},{"location":"curso1/tema3_state_schema/#como-funciona-el-state-en-un-grafo","title":"\ud83d\udd04 \u00bfC\u00f3mo Funciona el State en un Grafo?","text":"<p>El <code>State</code> fluye de un nodo a otro, llevando consigo informaci\u00f3n que puede ser modificada, eliminada o ampliada. Cada nodo puede: - Leer atributos del estado. - Modificar datos existentes. - Agregar nuevos atributos si es necesario.  </p> <p>Esto nos permite construir flujos din\u00e1micos y adaptativos, donde el grafo evoluciona seg\u00fan las interacciones del usuario o los c\u00e1lculos realizados.  </p>"},{"location":"curso1/tema3_state_schema/#ejemplo-completo-con-nodos","title":"\ud83d\udee0\ufe0f Ejemplo Completo con Nodos","text":"<p>Vamos a construir un grafo que: 1. Reciba el nombre y edad del usuario. 2. Modifique el estado agregando una preferencia de acuerdo con la edad.  </p>"},{"location":"curso1/tema3_state_schema/#definiendo-nodos-para-modificar-el-estado","title":"\ud83d\udccb Definiendo Nodos para Modificar el Estado","text":"<pre><code>def user_input_node(state: State):\n    print(\"--- Nodo 1: Recibir Usuario ---\")\n    state[\"name\"] = \"Raul\"\n    state[\"preferences\"] = []\n    return state\n\ndef recommendation_node(state: State):\n    print(\"--- Nodo 2: Recomendaci\u00f3n ---\")\n    if state[\"age\"] &lt; 18:\n        state[\"preferences\"].append(\"Videojuegos\")\n    else:\n        state[\"preferences\"].append(\"Programaci\u00f3n\")\n    return state\n</code></pre> <ul> <li>Nodo <code>user_input_node</code>: Toma el nombre, inicializa las preferencias y los a\u00f1ade al estado.  </li> <li>Nodo <code>recommendation_node</code>: Agrega recomendaciones basadas en la edad del usuario.  </li> </ul>"},{"location":"curso1/tema3_state_schema/#construccion-del-grafo","title":"\ud83c\udfd7\ufe0f Construcci\u00f3n del Grafo","text":"<pre><code>from langgraph.graph import StateGraph, START, END\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"user_input\", user_input_node)\nbuilder.add_node(\"recommendation\", recommendation_node)\n\nbuilder.add_edge(START, \"user_input\")\nbuilder.add_edge(\"user_input\", \"recommendation\")\nbuilder.add_edge(\"recommendation\", END)\n\ngraph = builder.compile()\n</code></pre> <p>Explicaci\u00f3n de cada paso: 1. Creamos el grafo con <code>StateGraph</code>. 2. A\u00f1adimos los nodos. 3. Definimos el flujo:    - El grafo comienza en <code>START</code> y pasa por <code>user_input_node</code>.    - Luego se dirige a <code>recommendation_node</code> antes de finalizar (<code>END</code>).  </p>"},{"location":"curso1/tema3_state_schema/#visualizacion-del-grafo","title":"\ud83d\udcc8 Visualizaci\u00f3n del Grafo","text":"<p>Para observar c\u00f3mo se estructura nuestro grafo, generamos una visualizaci\u00f3n:</p> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p>"},{"location":"curso1/tema3_state_schema/#invocando-el-grafo-y-resultados","title":"\ud83d\ude80 Invocando el Grafo y Resultados","text":""},{"location":"curso1/tema3_state_schema/#ejecucion-del-grafo","title":"Ejecuci\u00f3n del Grafo","text":"<pre><code>graph.invoke({\"name\": \"\", \"age\": 40})\n</code></pre> Resultado<pre><code>--- Nodo 1: Recibir Usuario ---\n--- Nodo 2: Recomendaci\u00f3n ---\n{'name': 'Raul', 'age': 40, 'preferences': ['Programaci\u00f3n']}\n</code></pre> <ul> <li>Si el usuario tiene menos de 18 a\u00f1os, el grafo recomienda videojuegos.  </li> <li>Si tiene 18 o m\u00e1s, se recomienda programaci\u00f3n.  </li> </ul>"},{"location":"curso1/tema3_state_schema/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Ver M\u00e1s ejemplos en Google Colab</li> <li> Gu\u00eda: State</li> </ul>"},{"location":"curso1/tema3_state_schema/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>State Schema: Define la estructura de datos que viaja por el grafo.  </li> <li>TypedDict: Permite definir el esquema con tipos espec\u00edficos, asegurando que el estado tenga una estructura clara y validada.  </li> <li>Nodos y Estado: Los nodos pueden modificar y ampliar el estado, creando flujos din\u00e1micos.</li> <li><code>Literal</code>: Podemos filtar la informaci\u00f3n que vamos a manejar.  </li> </ul>"},{"location":"curso1/tema3_state_schema/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, profundizaremos en el uso de Memoria en LangGraph. Aprenderemos c\u00f3mo usar <code>MemorySaver</code> para almacenar datos a lo largo de m\u00faltiples invocaciones, creando flujos de trabajo que recuerdan informaci\u00f3n pasada.  </p>"},{"location":"curso1/tema4_memory/","title":"\ud83e\udde0 Tema 4: Memoria","text":""},{"location":"curso1/tema4_memory/#que-es-el-la-memoria","title":"\ud83d\udd75 \u00bfQu\u00e9 es el la Memoria?","text":"<p>La memoria es la capacidad que tiene el grafo para recordar informaci\u00f3n a lo largo de m\u00faltiples invocaciones. Es especialmente \u00fatil cuando creamos chatbots o flujos conversacionales donde el contexto es crucial.  </p> <ul> <li>Sin memoria: Cada vez que invocamos el grafo, comienza desde cero.  </li> <li>Con memoria: El grafo guarda un historial de mensajes o datos, permitiendo respuestas basadas en todo el contexto anterior.  </li> </ul> <p>\u00bfPor qu\u00e9 es importante? - Persistencia de datos. - Toma de decisiones basadas en el contexto actual. - Mayor precisi\u00f3n en interacciones prolongadas. </p>"},{"location":"curso1/tema4_memory/#como-funciona","title":"\ud83d\udee0\ufe0f \u00bfC\u00f3mo Funciona?","text":"<p>LangGraph ofrece varios tipos de memoria:  </p> <ol> <li> <p>Memoria de Estado (State Memory): </p> <ul> <li>El estado se transfiere de nodo a nodo, reteniendo informaci\u00f3n solo durante una invocaci\u00f3n.  </li> </ul> </li> <li> <p>Memoria de Corto Alcance (MemorySaver): </p> <ul> <li>Guarda datos durante m\u00faltiples invocaciones, pero no persiste despu\u00e9s de cerrar el programa. </li> <li>Almacena la informaci\u00f3n en memoria vol\u00e1til (RAM).  </li> <li>Ideal para flujos temporales que necesitan recordar el contexto durante la sesi\u00f3n actual.    \ud83d\udc49 Documentaci\u00f3n de MemorySaver</li> </ul> </li> <li> <p>Memoria Persistente (Postgres u otros): </p> <ul> <li>Permite almacenar el historial en bases de datos externas (ej: PostgreSQL).  </li> <li>Se ver\u00e1 en temas m\u00e1s avanzados. </li> </ul> </li> <li> <p>Memoria Transversal (Cross-Session Memory): </p> <ul> <li>Conserva datos a lo largo de m\u00faltiples sesiones y usuarios.  </li> <li>Se estudiar\u00e1 en profundidad en futuros m\u00f3dulos. </li> </ul> </li> </ol>"},{"location":"curso1/tema4_memory/#implementacion-de-memorysaver","title":"\ud83e\udde9 Implementaci\u00f3n de MemorySaver","text":"<p>Usando el ejemplo que vimos anteriormente en art\u00edculos pasados, vamos a definir un grafo sencillo para poder ver como actula el MemorySaver:  </p> <ol> <li>CustomState: Definimos un custom state para almacenar los mensajes.</li> <li>Nodo de entrada del usuario: Agrega el mensaje del usuario al estado.  </li> <li>Nodo de respuesta de IA: El modelo de OpenAI (<code>gpt-4o-mini</code>) genera una respuesta basada en el historial.  </li> </ol> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langgraph.graph import MessagesState\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nclass CustomState(MessagesState):\n    messages: list[AIMessage | HumanMessage | SystemMessage] = []\n\n# Nodo de entrada del usuario\ndef user_input_node(state: CustomState):\n    print(\"--- Usuario dice ---\")\n    print(state['messages'][-1].content)\n    return {\"messages\": state['messages']}\n\n# Nodo de IA (respuesta)\ndef ai_response_node(state: CustomState):\n    print(\"--- IA responde ---\")\n    response = llm.invoke(state['messages'])\n    print(response.content)\n    return {\"messages\": state['messages'] + [AIMessage(content=response.content)]}\n</code></pre> <ul> <li><code>user_input_node:</code> Toma el mensaje del usuario y lo guarda en el estado.  </li> <li><code>ai_response_node:</code> Usa el estado actual (historial) para generar una respuesta de IA.  </li> </ul>"},{"location":"curso1/tema4_memory/#anadiendo-memoria-al-grafo","title":"\ud83c\udfd7\ufe0f A\u00f1adiendo Memoria al Grafo","text":"<p>Ahora implementaremos el <code>MemorySaver</code> para agregar memoria de corto alcance a nuestro grafo.  </p>"},{"location":"curso1/tema4_memory/#grafo-sin-memoria","title":"\ud83d\udccb Grafo sin Memoria","text":"<p>En el siguiente ejemplo, vamos a ver un grafo b\u00e1sico, como ya hemos visto hasta ahora, solamente para poder ver la diferencia para cuando le apliquemos nuestra memoria.</p> <pre><code>from langgraph.graph import StateGraph, START, END\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"user_input\", user_input_node)\nbuilder.add_node(\"ai_response\", ai_response_node)\n\nbuilder.add_edge(START, \"user_input\")\nbuilder.add_edge(\"user_input\", \"ai_response\")\nbuilder.add_edge(\"ai_response\", END)\n\ngraph_no_memory = builder.compile()\n</code></pre>"},{"location":"curso1/tema4_memory/#grafo-con-memoria","title":"\ud83d\udccb Grafo con Memoria","text":"<p>Ahora, agregaremos una memoria temporal y comprobaremos si nuestro grafo es capaz de retener informaci\u00f3n para poderla usar mas adelante.</p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\n\nbuilder_mem = StateGraph(State)\nbuilder_mem.add_node(\"user_input\", user_input_node)\nbuilder_mem.add_node(\"ai_response\", ai_response_node)\n\nbuilder_mem.add_edge(START, \"user_input\")\nbuilder_mem.add_edge(\"user_input\", \"ai_response\")\nbuilder_mem.add_edge(\"ai_response\", END)\n\ngraph_memory = builder_mem.compile(checkpointer=memory)\n</code></pre>"},{"location":"curso1/tema4_memory/#visualizacion-del-grafo","title":"\ud83d\udcc8 Visualizaci\u00f3n del Grafo","text":"<p>Para observar c\u00f3mo se estructura nuestro grafo, generamos una visualizaci\u00f3n: <pre><code>from IPython.display import Image, display\ndisplay(Image(graph_no_memory.get_graph().draw_mermaid_png()))\ndisplay(Image(graph_memory.get_graph().draw_mermaid_png()))\n</code></pre></p> <p></p>"},{"location":"curso1/tema4_memory/#invocando-el-grafo-y-comparando-resultados","title":"\ud83d\ude80 Invocando el Grafo y Comparando Resultados","text":""},{"location":"curso1/tema4_memory/#sin-memoria","title":"\ud83d\udccb Sin Memoria","text":"<pre><code>graph_no_memory.invoke({\"messages\": [HumanMessage(content=\"Hola, me llamo Raul.\")]})\ngraph_no_memory.invoke({\"messages\": [HumanMessage(content=\"\u00bfC\u00f3mo me llamo?\")]})\n</code></pre> Respuesta<pre><code>--- Usuario dice ---\nHola, me llamo Raul.\n--- IA responde ---\n\u00a1Hola, Ra\u00fal! \u00bfC\u00f3mo est\u00e1s? \u00bfEn qu\u00e9 puedo ayudarte hoy?\n--- Usuario dice ---\n\u00bfC\u00f3mo me llamo?\n--- IA responde ---\nNo tengo acceso a informaci\u00f3n personal sobre los usuarios, as\u00ed que no s\u00e9 c\u00f3mo te llamas. Pero si quieres, puedes decirme tu nombre. \u00a1Estoy aqu\u00ed para ayudarte!\n</code></pre> <p>Como vemos el grafo no guarda el hist\u00f3rico del chat, haciendo que el LLM no pueda tener acceso a informaci\u00f3n previa, por eso no es capaz de recordar mi nombre.</p>"},{"location":"curso1/tema4_memory/#con-memoria","title":"\ud83d\udccb Con Memoria","text":"<pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Agregamos el config a nuestros invokes\ngraph_memory.invoke({\"messages\": [HumanMessage(content=\"Hola, me llamo Raul.\")]},config)\ngraph_memory.invoke({\"messages\": [HumanMessage(content=\"\u00bfC\u00f3mo me llamo?\")]},config)\n</code></pre> Respuesta<pre><code>--- Usuario dice ---\nHola, me llamo Raul.\n--- IA responde ---\n\u00a1Hola, Ra\u00fal! \u00bfC\u00f3mo est\u00e1s? \u00bfEn qu\u00e9 puedo ayudarte hoy?\n--- Usuario dice ---\n\u00bfC\u00f3mo me llamo?\n--- IA responde ---\nTe llamas Ra\u00fal. \u00bfHay algo m\u00e1s en lo que te pueda ayudar?\n</code></pre> <p>El par\u00e1metro <code>config</code> es fundamental para habilitar el seguimiento de sesiones individuales cuando utilizamos memoria con <code>MemorySaver</code>. - <code>thread_id</code> act\u00faa como un identificador \u00fanico para cada sesi\u00f3n o conversaci\u00f3n. - Esto permite que LangGraph recuerde informaci\u00f3n espec\u00edfica de una sesi\u00f3n y la distinga de otras conversaciones paralelas.  </p>"},{"location":"curso1/tema4_memory/#que-sucede-sin-thread_id","title":"\ud83e\udde9 \u00bfQu\u00e9 sucede sin <code>thread_id</code>?","text":"<p>Si no agregamos <code>thread_id</code>: - Cada invocaci\u00f3n del grafo se trata como una nueva sesi\u00f3n, incluso si estamos usando <code>MemorySaver</code>. - El grafo no podr\u00e1 recordar mensajes previos porque no tendr\u00e1 forma de asociarlos a una misma sesi\u00f3n.  </p>"},{"location":"curso1/tema4_memory/#cuando-usar-thread_id","title":"\ud83d\ude80 \u00bfCu\u00e1ndo usar <code>thread_id</code>?","text":"<ul> <li>Chats personalizados: Cuando manejamos m\u00faltiples usuarios o conversaciones.  </li> <li>Bots conversacionales: Para mantener el contexto de usuarios durante largas interacciones.  </li> <li>Flujos multi-sesi\u00f3n: Si queremos permitir que un usuario retome una conversaci\u00f3n anterior.  </li> </ul>"},{"location":"curso1/tema4_memory/#alternativa-a-memorysaver","title":"\ud83e\udde9 Alternativa a MemorySaver","text":"<p>La forma m\u00e1s efectiva de aplicar memoria de corto alcance en LangGraph es utilizando <code>MemorySaver</code>. Sin embargo, existen otras alternativas que pueden resultar \u00fatiles en ciertos casos.  </p> <p>Una de las opciones m\u00e1s simples es almacenar el historial de la conversaci\u00f3n en una variable y pasarla continuamente durante cada invocaci\u00f3n. De esta manera, el modelo de lenguaje (LLM) mantiene el contexto de la conversaci\u00f3n sin depender de sistemas de memoria externos.  </p> <p>A continuaci\u00f3n, veremos un ejemplo sencillo para entender c\u00f3mo implementar esta t\u00e9cnica:  </p> <pre><code># Almacenamos las preguntas y respuestas en una variable.\nmessages = [HumanMessage(content=\"Hola, me llamo Raul.\")]\nresponse = graph_no_memory.invoke({\"messages\": messages})\nmessages.append(response.get('messages')[-1])\nmessages.append(HumanMessage(content=\"\u00bfC\u00f3mo me llamo?\"))\nresponse = graph_no_memory.invoke({\"messages\": messages})\nmessages.append(response.get('messages')[-1])\n</code></pre> Respuesta<pre><code>--- Usuario dice ---\nHola, me llamo Raul.\n--- IA responde ---\n\u00a1Hola, Ra\u00fal! \u00bfC\u00f3mo est\u00e1s? \u00bfEn qu\u00e9 puedo ayudarte hoy?\n--- Usuario dice ---\n\u00bfC\u00f3mo me llamo?\n--- IA responde ---\nTe llamas Ra\u00fal. \u00bfHay algo m\u00e1s con lo que te pueda ayudar?\n</code></pre> <ul> <li>Guardamos los mensajes en una lista (<code>messages</code>) que act\u00faa como historial de conversaci\u00f3n.  </li> <li>Tras cada invocaci\u00f3n al grafo, agregamos la respuesta generada por la IA al historial.  </li> <li>Cuando el usuario realiza una nueva pregunta, enviamos el historial completo al grafo para que el modelo mantenga el contexto.  </li> </ul>"},{"location":"curso1/tema4_memory/#observaciones-importantes","title":"\ud83e\uddd1\u200d\ud83c\udfeb Observaciones Importantes","text":"<ul> <li>Esta t\u00e9cnica proporciona memoria temporal, pero requiere gesti\u00f3n manual del historial de mensajes.  </li> <li>El historial puede almacenarse:  <ul> <li>En una variable. </li> <li>En una base de datos. </li> <li>En un archivo externo. </li> </ul> </li> </ul> <p>Aunque m\u00e1s adelante exploraremos m\u00e9todos m\u00e1s avanzados y optimizados (como memoria persistente con bases de datos), esta soluci\u00f3n puede ser una forma sencilla y efectiva para proyectos que no requieren una arquitectura compleja.  </p>"},{"location":"curso1/tema4_memory/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> How-to-guide: MemorySaver</li> </ul>"},{"location":"curso1/tema4_memory/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Memoria de Estado (State): Solo persiste durante la ejecuci\u00f3n de una invocaci\u00f3n.  </li> <li>MemorySaver: Proporciona memoria de corto alcance para retener datos durante m\u00faltiples invocaciones.  </li> <li>Diferencias Clave: Sin memoria, cada invocaci\u00f3n es independiente. Con memoria, el grafo recuerda datos anteriores.  </li> </ul>"},{"location":"curso1/tema4_memory/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el siguiente tema exploraremos Chains y Flujos de Trabajo, donde aprenderemos a encadenar nodos y construir pipelines complejos. Tambi\u00e9n abordaremos el tema de Tools (Herramientas), pero lo veremos despu\u00e9s de Chains, ya que comprender primero el flujo de trabajo har\u00e1 que la integraci\u00f3n de herramientas sea m\u00e1s sencilla y pr\u00e1ctica.  </p>"},{"location":"curso1/tema5_chains/","title":"\ud83d\udd17 Tema 5: Chains \u2013 Construcci\u00f3n de Flujos de Trabajo","text":""},{"location":"curso1/tema5_chains/#que-son-las-chains-en-langgraph","title":"\ud83d\ude80 \u00bfQu\u00e9 son las Chains en LangGraph?","text":"<p>Las Chains (cadenas) son uno de los componentes m\u00e1s esenciales dentro de LangGraph. Permiten encadenar m\u00faltiples nodos y funciones para crear flujos de trabajo complejos y estructurados.  </p> <p>Piensa en las chains como una l\u00ednea de montaje, donde cada nodo realiza una tarea espec\u00edfica y pasa el resultado al siguiente.  </p> <p>Esto permite dividir procesos en pasos m\u00e1s manejables y reutilizables. </p> <p>Sin embargo, no todos los flujos siguen un camino lineal. A veces, es necesario bifurcar el flujo en funci\u00f3n de ciertas condiciones.  </p>"},{"location":"curso1/tema5_chains/#por-que-usar-chains","title":"\ud83e\udde0 \u00bfPor qu\u00e9 usar Chains?","text":"<ul> <li>Modularidad: Dividir grandes procesos en nodos peque\u00f1os facilita el mantenimiento y la depuraci\u00f3n.  </li> <li>Reutilizaci\u00f3n: Las chains pueden componerse de nodos reutilizables en diferentes flujos.  </li> <li>Escalabilidad: Permiten construir flujos extensibles que pueden crecer f\u00e1cilmente a\u00f1adiendo m\u00e1s nodos.  </li> </ul>"},{"location":"curso1/tema5_chains/#como-se-define-una-chain","title":"\u2699\ufe0f \u00bfC\u00f3mo se Define una Chain?","text":"<p>Crear una chain en LangGraph implica: 1. Definir nodos individuales. 2. Encadenar esos nodos en una secuencia l\u00f3gica. 3. Permitir bifurcaciones condicionales si es necesario. </p> <p>Adem\u00e1s de encadenar nodos de forma secuencial, podemos agregar bifurcaciones condicionales para que el grafo tome diferentes rutas seg\u00fan el estado actual.  </p>"},{"location":"curso1/tema5_chains/#ejemplo-practico-creacion-de-una-chain","title":"\ud83d\udccb Ejemplo Pr\u00e1ctico: Creaci\u00f3n de una Chain","text":"<p>Vamos a construir un flujo simple que: 1. Reciba el nombre del usuario. 2. Salude al usuario. 3. Sugiera una actividad diferente seg\u00fan la hora del d\u00eda (ma\u00f1ana o tarde). </p> <pre><code>from typing_extensions import TypedDict\nfrom datetime import datetime\n\nclass State(TypedDict):\n    name: str\n    message: str\n\ndef user_input(state: State):\n    print(\"--- Nodo 1: Entrada del Usuario ---\")\n    state[\"name\"] = \"Raul\"\n    return state\n\ndef greet_user(state: State):\n    print(\"--- Nodo 2: Saludo ---\")\n    state[\"message\"] = f\"Hola {state['name']}, \u00a1bienvenido de nuevo!\"\n    return state\n\ndef suggest_morning(state: State):\n    print(\"--- Nodo 3: Ma\u00f1ana ---\")\n    state[\"message\"] += \" Te recomiendo empezar el d\u00eda con un caf\u00e9 \u2615\ufe0f\"\n    return state\n\ndef suggest_afternoon(state: State):\n    print(\"--- Nodo 4: Tarde ---\")\n    state[\"message\"] += \" \u00bfQu\u00e9 tal salir a dar un paseo por la tarde? \ud83d\udeb6\u200d\u2642\ufe0f\"\n    return state\n\ndef decide_path(state: State) -&gt; bool:\n    hour = datetime.now().hour\n    return True if hour &lt; 12 else False\n</code></pre> <ul> <li>Nodo 1 (user_input): Recoge el nombre del usuario.  </li> <li>Nodo 2 (greet_user): Genera un saludo personalizado.  </li> <li>Nodo 3 y Nodo 4 (suggest_morning/suggest_afternoon): <ul> <li>Si es por la ma\u00f1ana, el flujo sugiere tomar un caf\u00e9.  </li> <li>Si es por la tarde, se recomienda dar un paseo.  </li> </ul> </li> <li>Funci\u00f3n <code>decide_path</code>: Eval\u00faa la hora actual para bifurcar el flujo hacia el nodo adecuado.   </li> </ul>"},{"location":"curso1/tema5_chains/#construccion-del-grafo-con-chains","title":"\ud83c\udfd7\ufe0f Construcci\u00f3n del Grafo con Chains","text":"<p>Encadenamos los nodos en secuencia para formar la chain completa. <pre><code>from langgraph.graph import StateGraph, START, END\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"user_input\", user_input)\nbuilder.add_node(\"greet_user\", greet_user)\nbuilder.add_node(\"suggest_morning\", suggest_morning)\nbuilder.add_node(\"suggest_afternoon\", suggest_afternoon)\n\nbuilder.add_edge(START, \"user_input\")\nbuilder.add_edge(\"user_input\", \"greet_user\")\nbuilder.add_conditional_edges(\"greet_user\", decide_path, {True: \"suggest_morning\", False: \"suggest_afternoon\"})\nbuilder.add_edge(\"suggest_morning\", END)\nbuilder.add_edge(\"suggest_afternoon\", END)\n\ngraph = builder.compile()\n</code></pre></p>"},{"location":"curso1/tema5_chains/#visualizacion-del-grafo","title":"\ud83d\udcc8 Visualizaci\u00f3n del Grafo","text":"<p>Para observar c\u00f3mo se estructura nuestro grafo, generamos una visualizaci\u00f3n: <pre><code>from IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre></p> <p></p>"},{"location":"curso1/tema5_chains/#invocando-el-grafo","title":"\ud83d\ude80 Invocando el Grafo","text":"<p>Ahora ejecutamos el grafo con una entrada simple. <pre><code>graph.invoke({\"name\": \"\", \"message\": \"\"})\n</code></pre></p> <p>Resultado 1<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo 2: Saludo ---\n--- Nodo 3: Ma\u00f1ana ---\n{'name': 'Raul', 'message': 'Hola Raul, \u00a1bienvenido de nuevo! Te recomiendo empezar el d\u00eda con un caf\u00e9 \u2615\ufe0f'}\n</code></pre> Resultado 2<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo 2: Saludo ---\n--- Nodo 4: Tarde ---\n{'name': 'Raul', 'message': 'Hola Raul, \u00a1bienvenido de nuevo! \u00bfQu\u00e9 tal salir a dar un paseo por la tarde? \ud83d\udeb6\u200d\u2642\ufe0f'}\n</code></pre></p> <p>El grafo procesa cada nodo secuencialmente, generando una respuesta estructurada para el usuario.  </p>"},{"location":"curso1/tema5_chains/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Definici\u00f3n Condition Edge</li> <li> Clase Condition Edge</li> </ul>"},{"location":"curso1/tema5_chains/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Chains: Permiten conectar nodos en secuencia, formando flujos de trabajo escalables y reutilizables.  </li> <li>Modularidad: Los nodos pueden realizar tareas peque\u00f1as que, al combinarse, crean flujos m\u00e1s complejos.  </li> <li>Chains Condicionales: Permiten bifurcar el flujo en funci\u00f3n de valores din\u00e1micos (como la hora o entradas del usuario).  </li> <li>Encadenamiento Secuencial y Din\u00e1mico: Los flujos no siempre son lineales, y LangGraph permite construir grafos que se adaptan a diferentes situaciones.  </li> </ul>"},{"location":"curso1/tema5_chains/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el siguiente tema, exploraremos el uso de Routers, donde aprenderemos c\u00f3mo crear bifurcaciones y poder redirigir el flujo para que tome diferentes caminos.  </p>"},{"location":"curso1/tema6_routers/","title":"\ud83d\udea6 Tema 6: Routers \u2013 Dirigiendo el Flujo del Grafo","text":""},{"location":"curso1/tema6_routers/#que-es-un-router-en-langgraph","title":"\ud83d\ude80 \u00bfQu\u00e9 es un Router en LangGraph?","text":"<p>Los routers en LangGraph son nodos especiales que permiten redirigir el flujo hacia diferentes caminos en funci\u00f3n de condiciones din\u00e1micas. A diferencia de los edges condicionales, los routers centralizan y gestionan m\u00faltiples bifurcaciones desde un solo punto, haciendo que los grafos sean m\u00e1s modulares, limpios y escalables.</p>"},{"location":"curso1/tema6_routers/#por-que-usar-routers-en-lugar-de-condicionales","title":"\ud83e\udde0 \u00bfPor qu\u00e9 usar Routers en lugar de Condicionales?","text":"<ul> <li>Centralizaci\u00f3n: Un solo nodo puede gestionar m\u00faltiples caminos, evitando bifurcaciones dispersas.</li> <li>Escalabilidad: A medida que crecen los caminos posibles, los routers permiten expandir el grafo sin a\u00f1adir complejidad.</li> <li>Reutilizaci\u00f3n: Un mismo router puede ser usado en diferentes partes del grafo.</li> <li>Legibilidad: Mantiene el grafo organizado y f\u00e1cil de seguir.</li> </ul> <p>\ud83d\udc49 Piensa en el router como un sem\u00e1foro \ud83d\udea6 que dirige el tr\u00e1fico hacia diferentes carriles en funci\u00f3n de las se\u00f1ales recibidas.</p>"},{"location":"curso1/tema6_routers/#como-funciona-un-router","title":"\u2699\ufe0f \u00bfC\u00f3mo Funciona un Router?","text":"<p>Un router funciona tomando una decisi\u00f3n basada en el estado del grafo o en par\u00e1metros espec\u00edficos de entrada. Define m\u00faltiples caminos posibles y selecciona uno, enviando el flujo hacia el nodo correspondiente.</p>"},{"location":"curso1/tema6_routers/#ejemplo-practico-chatbot-con-router-de-departamentos","title":"\ud83d\udccb Ejemplo Pr\u00e1ctico: Chatbot con Router de Departamentos","text":"<p>Vamos a crear un chatbot que redirige al usuario a diferentes departamentos seg\u00fan el tipo de pregunta:</p> <ol> <li>Soporte T\u00e9cnico: Para resolver problemas t\u00e9cnicos.</li> <li>Ventas: Para consultas de productos o servicios.</li> <li>Consultas Generales: Para cualquier otra pregunta.</li> </ol> <p>El router analizar\u00e1 el mensaje del usuario y lo dirigir\u00e1 al nodo adecuado.</p> <pre><code>from typing_extensions import TypedDict\n\nclass State(TypedDict):\n    message: str\n    response: str\n\ndef user_input(state: State):\n    print(\"--- Nodo 1: Entrada del Usuario ---\")\n    print(state[\"message\"])\n    return state\n\ndef route_request(state: State) -&gt; str:\n    message = state[\"message\"].lower()\n    if \"soporte\" in message or \"problema\" in message:\n        return \"support_node\"\n    elif \"comprar\" in message or \"precio\" in message:\n        return \"sales_node\"\n    else:\n        return \"general_node\"\n\ndef support_node(state: State):\n    print(\"--- Nodo de Soporte T\u00e9cnico ---\")\n    state[\"response\"] = \"Te estamos transfiriendo a soporte t\u00e9cnico.\"\n    return state\n\ndef sales_node(state: State):\n    print(\"--- Nodo de Ventas ---\")\n    state[\"response\"] = \"Te conectamos con el equipo de ventas.\"\n    return state\n\ndef general_node(state: State):\n    print(\"--- Nodo de Consultas Generales ---\")\n    state[\"response\"] = \"Tu consulta ser\u00e1 respondida a la brevedad.\"\n    return state\n</code></pre>"},{"location":"curso1/tema6_routers/#explicacion-del-ejemplo","title":"\ud83d\udd0d Explicaci\u00f3n del Ejemplo:","text":"<ul> <li>Nodo 1 (user_input): Recibe el mensaje del usuario.</li> <li>Router (route_request): Eval\u00faa el mensaje y redirige al nodo correspondiente: <code>support_node</code>, <code>sales_node</code> o <code>general_node</code>.</li> <li>Nodos de Respuesta: Cada nodo proporciona una respuesta basada en el departamento al que se redirigi\u00f3 al usuario.</li> </ul>"},{"location":"curso1/tema6_routers/#construccion-del-grafo-con-router","title":"\ud83c\udfd7\ufe0f Construcci\u00f3n del Grafo con Router","text":"<p>Agregamos el router y conectamos cada nodo de respuesta al flujo.</p> <pre><code>from langgraph.graph import StateGraph, START, END\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"user_input\", user_input)\nbuilder.add_node(\"support_node\", support_node)\nbuilder.add_node(\"sales_node\", sales_node)\nbuilder.add_node(\"general_node\", general_node)\n\nbuilder.add_edge(START, \"user_input\")\nbuilder.add_conditional_edges(\"user_input\", route_request)\nbuilder.add_edge(\"support_node\", END)\nbuilder.add_edge(\"sales_node\", END)\nbuilder.add_edge(\"general_node\", END)\n\ngraph = builder.compile()\n</code></pre>"},{"location":"curso1/tema6_routers/#visualizacion-del-grafo","title":"\ud83d\udcc8 Visualizaci\u00f3n del Grafo","text":"<p>Para observar c\u00f3mo se estructura nuestro grafo, generamos una visualizaci\u00f3n:</p> <pre><code>from IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p>"},{"location":"curso1/tema6_routers/#invocando-el-grafo","title":"\ud83d\ude80 Invocando el Grafo","text":"<p>Probamos el grafo con diferentes entradas para ver c\u00f3mo el router dirige el flujo.</p> <pre><code>graph.invoke({\"message\": \"\", \"response\": \"\"})\n</code></pre> Resultado<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo de Ventas ---\n{'message': 'Quiero comprar un producto', 'response': 'Te conectamos con el equipo de ventas.'}\n</code></pre> Resultado<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo de Ventas ---\n{'message': 'Tengo un problema con mi cuenta', 'response': 'Te estamos transfiriendo a soporte t\u00e9cnico.'}\n</code></pre> Resultado<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo de Ventas ---\n{'message': '\u00bfCu\u00e1l es el horario de atenci\u00f3n?', 'response': 'Tu consulta ser\u00e1 respondida a la brevedad.'}\n</code></pre> <p>Dependiendo del contenido del mensaje del usuario, el flujo tomar\u00e1 caminos distintos, demostrando el poder y flexibilidad de los routers.</p>"},{"location":"curso1/tema6_routers/#usando-command-para-modificar-el-estado-durante-el-enrutamiento","title":"\ud83d\udee0\ufe0f Usando \"Command\" para Modificar el Estado Durante el Enrutamiento","text":"<p>Adem\u00e1s de redirigir el flujo del grafo, LangGraph permite modificar el estado directamente desde el router usando Command. Esto es \u00fatil cuando, adem\u00e1s de dirigir al usuario a un nodo espec\u00edfico, queremos actualizar atributos del estado sin necesidad de crear nodos adicionales.</p>"},{"location":"curso1/tema6_routers/#que-es-un-command-en-langgraph","title":"\ud83d\ude80 \u00bfQu\u00e9 es un Command en LangGraph?","text":"<p>Un command es una instrucci\u00f3n que permite realizar dos acciones simult\u00e1neas:</p> <ol> <li>Enrutar el flujo del grafo hacia un nodo espec\u00edfico.</li> <li>Actualizar o modificar atributos del estado.</li> </ol>"},{"location":"curso1/tema6_routers/#por-que-es-util","title":"\ud83e\udde9 \u00bfPor Qu\u00e9 es \u00datil?","text":"<ul> <li>Optimizaci\u00f3n: Evita la necesidad de crear nodos adicionales solo para actualizar el estado.</li> <li>Simplicidad: Mantiene el flujo del grafo m\u00e1s limpio y con menos nodos intermedios.</li> <li>Eficiencia: Reduce el n\u00famero de pasos y permite que el grafo sea m\u00e1s din\u00e1mico y reactivo.</li> </ul>"},{"location":"curso1/tema6_routers/#ejemplo-practico-con-command","title":"\ud83d\udccb Ejemplo Pr\u00e1ctico con Command","text":"<p>Modificaremos nuestro chatbot para que, adem\u00e1s de redirigir al usuario a un departamento, actualice el estado con un mensaje indicando a qu\u00e9 secci\u00f3n fue transferido.</p> <pre><code>from typing_extensions import TypedDict, Literal\nfrom langgraph.types import Command\n\nclass State(TypedDict):\n    message: str\n    response: str\n    department: str\n\ndef user_input(state: State):\n    print(\"--- Nodo 1: Entrada del Usuario ---\")\n    #state[\"message\"] = \"Necesito soporte t\u00e9cnico\"\n    return state\n\ndef route_request(state: State) -&gt; Command[Literal[\"support_node\", \"sales_node\",\"general_node\"]]:\n    message = state[\"message\"].lower()\n    print(\"--- Nodo 2: Ruta de la Solicitud ---\")\n    if \"soporte\" in message:\n        return Command(goto=\"support_node\", update={\"department\": \"Soporte T\u00e9cnico\"})\n    elif \"comprar\" in message:\n        return Command(goto=\"sales_node\", update={\"department\": \"Ventas\"})\n    else:\n        return Command(goto=\"general_node\", update={\"department\": \"Consultas Generales\"})\n\ndef support_node(state: State):\n    print(\"--- Nodo de Soporte T\u00e9cnico ---\")\n    print(state[\"department\"])\n    state[\"response\"] = \"Te estamos transfiriendo a soporte t\u00e9cnico.\"\n    return state\n\ndef sales_node(state: State):\n    print(\"--- Nodo de Ventas ---\")\n    print(state[\"department\"])\n    state[\"response\"] = \"Te conectamos con el equipo de ventas.\"\n    return state\n\ndef general_node(state: State):\n    print(\"--- Nodo de Consultas Generales ---\")\n    print(state[\"department\"])\n    state[\"response\"] = \"Tu consulta ser\u00e1 respondida a la brevedad.\"\n    return state\n</code></pre>"},{"location":"curso1/tema6_routers/#explicacion-del-ejemplo_1","title":"\ud83d\udd0d Explicaci\u00f3n del Ejemplo","text":"<ul> <li>Router con Command: Ahora, el router no solo dirige el flujo sino que tambi\u00e9n actualiza el estado con el nombre del departamento seleccionado.</li> <li>Actualizaci\u00f3n Directa del Estado: Esto nos permite guardar un registro de a qu\u00e9 nodo fue transferido el usuario, lo que puede ser \u00fatil para an\u00e1lisis o futuros pasos en el flujo.</li> </ul>"},{"location":"curso1/tema6_routers/#invocando-el-grafo_1","title":"\ud83d\ude80 Invocando el Grafo","text":"<p>Probamos el grafo para observar c\u00f3mo se actualiza el estado durante el enrutamiento.</p> <pre><code>from langgraph.graph import StateGraph, START, END\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"user_input\", user_input)\nbuilder.add_node(\"route_request\", route_request)\nbuilder.add_node(\"support_node\", support_node)\nbuilder.add_node(\"sales_node\", sales_node)\nbuilder.add_node(\"general_node\", general_node)\n\nbuilder.add_edge(START, \"user_input\")\nbuilder.add_edge(\"user_input\",\"route_request\")\n# NOTE: No hay edges entre route_request y el resto de nodos!\nbuilder.add_edge(\"support_node\", END)\nbuilder.add_edge(\"sales_node\", END)\nbuilder.add_edge(\"general_node\", END)\n\ngraph = builder.compile()\n\nfrom IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <pre><code>graph.invoke({\"message\": \"\", \"response\": \"\", \"department\": \"\"})\n</code></pre> Resultado<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo 2: Ruta de la Solicitud ---\n--- Nodo de Soporte T\u00e9cnico ---\n{'message': 'Necesito soporte t\u00e9cnico',\n 'response': 'Te estamos transfiriendo a soporte t\u00e9cnico.',\n 'department': 'Soporte T\u00e9cnico'}\n</code></pre> Resultado<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo 2: Ruta de la Solicitud ---\n--- Nodo de Ventas ---\n{'message': 'Quiero comprar un producto',\n 'response': 'Te conectamos con el equipo de ventas.',\n 'department': 'Ventas'}\n</code></pre> Resultado<pre><code>--- Nodo 1: Entrada del Usuario ---\n--- Nodo 2: Ruta de la Solicitud ---\n--- Nodo de Consultas Generales ---\n{'message': 'Tengo una pregunta general',\n 'response': 'Tu consulta ser\u00e1 respondida a la brevedad.',\n 'department': 'Consultas Generales'}\n</code></pre> <p>El estado reflejar\u00e1 tanto la respuesta del nodo como el departamento al que fue transferido el usuario.</p>"},{"location":"curso1/tema6_routers/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Definici\u00f3n Command</li> <li> How-to-guide Command</li> </ul>"},{"location":"curso1/tema6_routers/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Routers: Son nodos que gestionan m\u00faltiples bifurcaciones desde un solo punto.</li> <li>Centralizaci\u00f3n y Escalabilidad: Los routers permiten mantener un grafo limpio y organizado, facilitando la expansi\u00f3n.</li> <li>Flujos Din\u00e1micos: Los routers toman decisiones en tiempo real basadas en el estado o entrada del usuario.</li> </ul>"},{"location":"curso1/tema6_routers/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, hablaremos de Reducers, que permiten consolidar datos y resultados de diferentes caminos dentro del grafo. Los reducers trabajan en conjunto con los routers, formando flujos completos y optimizados.</p>"},{"location":"curso1/tema7_reducers/","title":"\ud83d\udd04 Tema 7: Reducers","text":""},{"location":"curso1/tema7_reducers/#que-es-un-reducer-en-langgraph","title":"\ud83d\ude80 \u00bfQu\u00e9 es un Reducer en LangGraph?","text":"<p>Los reducers en LangGraph permiten consolidar y procesar datos provenientes de diferentes nodos o caminos. Cuando un grafo sigue flujos paralelos o m\u00faltiples bifurcaciones, los reducers recogen, combinan o modifican los resultados, facilitando una salida coherente y organizada.  </p> Consejo <p>Piensa en un reducer como un colector de datos que une los resultados de distintas partes del grafo en un solo punto.  </p>"},{"location":"curso1/tema7_reducers/#por-que-son-importantes-los-reducers","title":"\ud83e\udde0 \u00bfPor qu\u00e9 son Importantes los Reducers?","text":"<ul> <li>Consolidaci\u00f3n de Datos: Re\u00fane informaci\u00f3n de diferentes nodos y la almacena en el estado.  </li> <li>Flujos Complejos: Permite manejar y combinar respuestas de m\u00faltiples caminos paralelos.  </li> <li>Evitan Sobrescrituras No Deseadas: Sin un reducer, cada actualizaci\u00f3n sobrescribe el valor existente.  </li> </ul>"},{"location":"curso1/tema7_reducers/#como-funciona-un-reducer","title":"\u2699\ufe0f \u00bfC\u00f3mo Funciona un Reducer?","text":"<p>Cuando un nodo devuelve datos parciales del estado, el reducer decide c\u00f3mo aplicar esos datos: - Sin Reducer: El valor existente se sobrescribe. - Con Reducer: Los datos se combinan o acumulan siguiendo una l\u00f3gica definida.  </p> <p>\ud83d\udc49 Ejemplo: Si tienes una lista de mensajes, un reducer puede a\u00f1adir nuevos mensajes al final, en lugar de borrar el historial anterior.  </p>"},{"location":"curso1/tema7_reducers/#ejemplo-a-sin-reducer-sobrescritura-directa","title":"\ud83d\udd0d Ejemplo A: Sin Reducer (Sobrescritura Directa)","text":"<p>En este caso, cada vez que un nodo devuelve un valor, sobrescribe el valor anterior:  </p> <pre><code>from typing_extensions import TypedDict\nfrom langchain_core.messages import AnyMessage\n\nclass State(TypedDict):\n    messages: list[AnyMessage]  # Sin reducer (sobrescribe)\n    response: str\n    department: str\n</code></pre>"},{"location":"curso1/tema7_reducers/#explicacion","title":"\ud83d\udd0e Explicaci\u00f3n:","text":"<ul> <li>La clave <code>messages</code> no tiene un reducer asignado.  </li> <li>Cada actualizaci\u00f3n de mensajes reemplaza por completo la lista anterior. </li> <li>El \u00faltimo mensaje ser\u00e1 el \u00fanico que quede en el estado.  </li> </ul>"},{"location":"curso1/tema7_reducers/#ejemplo-b-con-reducer-acumulacion-de-mensajes","title":"\ud83d\udccb Ejemplo B: Con Reducer (Acumulaci\u00f3n de Mensajes)","text":"<p>Usamos <code>Annotated</code> y <code>add_messages</code> para acumular mensajes en lugar de sobrescribirlos:  </p> <pre><code>from typing import Annotated\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]  # Con reducer (acumula)\n    response: str\n    department: str\n</code></pre> <ul> <li><code>Annotated</code> permite a\u00f1adir metadatos a <code>messages</code> para definir un reducer.  </li> <li><code>add_messages</code> acumula mensajes en el historial, evitando sobrescrituras.  </li> <li>Cada vez que un nodo a\u00f1ade un mensaje, este se agrega al final de la lista.  </li> </ul>"},{"location":"curso1/tema7_reducers/#el-papel-del-reducer-add_messages","title":"\ud83d\udd0d El Papel del Reducer (add_messages)","text":"<p>Cuando definimos:</p> <pre><code>messages: Annotated[list[AnyMessage], add_messages]\n</code></pre> <p>Estamos indicando que cuando un nodo devuelve una actualizaci\u00f3n parcial del estado, el reducer <code>add_messages</code> se encargar\u00e1 de combinar el valor nuevo con el existente.</p> <p>Esto implica que:</p> <ul> <li>Si un nodo devuelve <code>{\"messages\": [...]}</code>, el reducer <code>add_messages</code> sumar\u00e1 los mensajes nuevos a la lista actual.</li> <li>Si no hay un reducer definido, el nuevo valor reemplazar\u00e1 al valor anterior.</li> </ul> Info <p>Los reducers solo se activan cuando un nodo devuelve una actualizaci\u00f3n parcial o el estado completo. Es importante tener esto en cuenta si planeamos realizar otras modificaciones manualmente, ya que los cambios directos en el estado no pasar\u00e1n por el reducer.  </p>"},{"location":"curso1/tema7_reducers/#ejemplo-completo-comparando-con-y-sin-reducer","title":"\ud83d\udee0\ufe0f Ejemplo Completo: Comparando con y sin Reducer","text":"<p>Vamos a construir un grafo de chatbot que: 1. Recibe el mensaje del usuario. 2. Redirige al nodo correspondiente (soporte, ventas, general). 3. A\u00f1ade respuestas al historial de mensajes usando un reducer.  </p> <p>Veremos c\u00f3mo cambia el resultado al usar un reducer para <code>messages</code>.  </p> <p><pre><code>from langchain_core.messages import HumanMessage, AIMessage\nfrom langgraph.graph import StateGraph, START, END\n\ndef user_input(state: State):\n    print(\"--- Nodo 1: Entrada del Usuario ---\")\n    return {\"messages\": HumanMessage(content=\"Hola, tengo una pregunta.\")}\n\ndef support_node(state: State):\n    print(\"--- Nodo de Soporte T\u00e9cnico ---\")\n    state[\"response\"] = \"Soporte\"\n    return {\"messages\": AIMessage(content=\"Te estamos transfiriendo a soporte.\")}\n\n\ndef route_request(state: State) -&gt; str:\n    return \"support_node\"\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"user_input\", user_input)\nbuilder.add_node(\"support_node\", support_node)\n\nbuilder.add_edge(START, \"user_input\")\nbuilder.add_edge(\"user_input\", \"support_node\")\nbuilder.add_edge(\"support_node\", END)\n\ngraph = builder.compile()\n</code></pre> </p>"},{"location":"curso1/tema7_reducers/#invocando-el-grafo","title":"\ud83d\ude80 Invocando el Grafo","text":"<p>Probamos el grafo sin reducer y luego con <code>add_messages</code> para observar la diferencia.  </p> <pre><code>graph.invoke({\"messages\": [], \"response\": \"\", \"department\": \"\"})\n</code></pre> <p><pre><code># Sin reducer (sobrescribe)\n{'messages': [AIMessage(content=\"Te estamos transfiriendo a soporte.\")],\n 'response': 'Soporte',\n 'department': ''}\n</code></pre> <pre><code># Con reducer (acumula)\n{'messages': [\n  HumanMessage(content=\"Hola, tengo una pregunta.\"),\n  AIMessage(content=\"Te estamos transfiriendo a soporte.\")\n ],\n 'response': 'Soporte',\n 'department': ''}\n</code></pre></p> <ul> <li>Sin Reducer: Solo el \u00faltimo mensaje aparece en el estado.  </li> <li>Con Reducer: El historial acumula todos los mensajes y respuestas.  </li> </ul>"},{"location":"curso1/tema7_reducers/#como-crear-un-reducer-personalizado","title":"\u2728 \u00bfC\u00f3mo Crear un Reducer Personalizado?","text":"<p>Si necesitas una l\u00f3gica espec\u00edfica que <code>add_messages</code> no cubre, puedes crear tu propio reducer.  </p> <p>A continuaci\u00f3n, definimos un reducer personalizado que cuenta cu\u00e1ntos mensajes se han procesado y actualiza el estado con ese total:  </p> <pre><code>from typing import Annotated, Union\nfrom typing_extensions import TypedDict\nfrom langchain_core.messages import AnyMessage, HumanMessage, AIMessage\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import StateGraph, START, END\n\n# Definir un reducer personalizado que cuenta los mensajes\ndef count_messages(existing: int, new: Union[list[AnyMessage], AnyMessage]) -&gt; int:\n    # Si 'new' es una lista, sumamos su longitud. Si es un solo mensaje, sumamos 1.\n    if isinstance(new, list):\n        return existing + len(new)\n    return existing + 1\n\n# Definir el estado con historial de mensajes y contador total\nclass StateCustomReducer(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    total_messages: Annotated[int, count_messages]\n    response: str\n\n# Nodo de entrada del usuario\ndef user_input(state: StateCustomReducer):\n    print(\"--- Nodo 1: Entrada del Usuario ---\")\n    state[\"messages\"].append(HumanMessage(content=\"Hola, tengo una consulta.\"))\n    return state\n\n# Nodo de respuesta de soporte\ndef support_node(state: StateCustomReducer):\n    print(\"--- Nodo de Soporte T\u00e9cnico ---\")\n    state[\"messages\"].append(AIMessage(content=\"Te estamos transfiriendo a soporte t\u00e9cnico.\"))\n    state[\"response\"] = \"Soporte\"\n    return state\n\n# Nodo de ventas\ndef sales_node(state: StateCustomReducer):\n    print(\"--- Nodo de Ventas ---\")\n    state[\"messages\"].append(AIMessage(content=\"Te conectamos con el equipo de ventas.\"))\n    state[\"response\"] = \"Ventas\"\n    return state\n\n# Router que decide a qu\u00e9 nodo enviar\ndef route_request(state: StateCustomReducer) -&gt; str:\n    if \"compra\" in state[\"messages\"][-1].content.lower():\n        return \"sales_node\"\n    return \"support_node\"\n\n# Construcci\u00f3n del grafo\nbuilder = StateGraph(StateCustomReducer)\nbuilder.add_node(\"user_input\", user_input)\nbuilder.add_node(\"support_node\", support_node)\nbuilder.add_node(\"sales_node\", sales_node)\n\nbuilder.add_edge(START, \"user_input\")\nbuilder.add_conditional_edges(\"user_input\", route_request)\nbuilder.add_edge(\"support_node\", END)\nbuilder.add_edge(\"sales_node\", END)\n\n# Compilar el grafo\ngraph_with_custom_reducer = builder.compile()\n\nfrom IPython.display import Image, display\ndisplay(Image(graph_with_custom_reducer.get_graph().draw_mermaid_png()))\n</code></pre> <p>Ejecutamos el grafo y verificamos c\u00f3mo el estado se actualiza con el n\u00famero total de mensajes.  </p> <pre><code>graph_with_custom_reducer.invoke({\"messages\": [], \"total_messages\": 0, \"response\": \"\"})\n</code></pre> Resultado<pre><code>{'messages': [\n    HumanMessage(content=\"Hola, tengo una pregunta.\"),\n    AIMessage(content=\"Te estamos transfiriendo a soporte.\")\n  ],\n  'total_messages': 2,\n  'response': 'Soporte'\n}\n</code></pre> <p>El estado reflejar\u00e1 tanto el historial de <code>mensages</code> como el conteo total.  </p>"},{"location":"curso1/tema7_reducers/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Definici\u00f3n Reducers</li> </ul>"},{"location":"curso1/tema7_reducers/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Reducers: Controlan c\u00f3mo se actualiza el estado durante el flujo del grafo.  </li> <li>add_messages: Permite acumular mensajes, evitando sobrescrituras.  </li> <li>Reducers Personalizados: Proporcionan flexibilidad para definir cualquier l\u00f3gica de actualizaci\u00f3n.  </li> </ul>"},{"location":"curso1/tema7_reducers/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Tools (Herramientas), viendo c\u00f3mo integrar funciones externas y APIs dentro del grafo para ampliar sus capacidades.  </p>"},{"location":"curso1/tema8_tools/","title":"\ud83d\udee0\ufe0f Tema 7: Tools \u2013 Integraci\u00f3n de Herramientas Externas en el Grafo","text":""},{"location":"curso1/tema8_tools/#que-son-las-tools-en-langgraph","title":"\ud83d\ude80 \u00bfQu\u00e9 son las Tools en LangGraph?","text":"<p>Las tools (herramientas) son funciones externas que el grafo puede invocar para realizar tareas espec\u00edficas, como consultas a bases de datos, c\u00e1lculos matem\u00e1ticos o llamadas a APIs externas. Las tools permiten que el grafo extienda sus capacidades m\u00e1s all\u00e1 de los nodos internos, integrando l\u00f3gica personalizada o servicios de terceros.</p>"},{"location":"curso1/tema8_tools/#por-que-son-importantes-las-tools","title":"\ud83e\udde0 \u00bfPor qu\u00e9 son Importantes las Tools?","text":"<ul> <li>Extensibilidad: Permiten que el grafo interact\u00fae con APIs externas y sistemas avanzados.</li> <li>Modularidad: Las tools se definen de forma independiente y se integran f\u00e1cilmente en el flujo del grafo.</li> <li>Eficiencia: Delegan tareas espec\u00edficas a funciones externas, reduciendo la complejidad dentro de los nodos.</li> </ul> Consejo <p>Piensa en las tools como plugins que a\u00f1aden nuevas capacidades al grafo sin necesidad de modificar su estructura central.</p>"},{"location":"curso1/tema8_tools/#como-funcionan-las-tools","title":"\u2699\ufe0f \u00bfC\u00f3mo Funcionan las Tools?","text":"<ol> <li>Definici\u00f3n de Tools: Creamos funciones externas que pueden recibir par\u00e1metros y devolver resultados.</li> <li>Vinculaci\u00f3n al Modelo de Lenguaje (LLM): Asociamos estas tools al LLM para que pueda invocarlas durante el flujo de trabajo.</li> <li>Ejecuci\u00f3n Condicional: Si el LLM detecta que es necesario usar una tool, el grafo redirige el flujo para ejecutarla y procesar el resultado.</li> </ol> Nota <p>Es fundamental entender que es el LLM quien decide qu\u00e9 tool invocar en funci\u00f3n del mensaje o prompt proporcionado. El grafo facilita la integraci\u00f3n de herramientas, pero la decisi\u00f3n de cu\u00e1l utilizar se basa en la interpretaci\u00f3n que hace el modelo del contexto y las instrucciones.</p>"},{"location":"curso1/tema8_tools/#ejemplo-practico-chatbot-con-tools-para-soporte-y-ventas","title":"\ud83d\udccb Ejemplo Pr\u00e1ctico: Chatbot con Tools para Soporte y Ventas","text":"<p>Vamos a construir un grafo que act\u00faa como un asistente virtual, redirigiendo solicitudes de los usuarios a diferentes herramientas seg\u00fan sus necesidades. El chatbot podr\u00e1:</p> <ol> <li>Consultar precios de productos.</li> <li>Verificar el estado de pedidos.</li> <li>Abrir tickets de soporte.</li> </ol> <pre><code>from langchain_openai import ChatOpenAI\n\n# Definimos las tools externas con docstrings\ndef check_price(product: str) -&gt; str:\n    \"\"\"Consulta el precio de un producto.\n\n    Args:\n        product: Nombre del producto a consultar.\n\n    Returns:\n        El precio del producto en formato texto.\n    \"\"\"\n    # Simulamos una consulta a nuestra base de datos.\n    return f\"El precio de {product} es de 100\u20ac.\"\n\ndef order_status(order_id: int) -&gt; str:\n    \"\"\"Consulta el estado de un pedido.\n\n    Args:\n        order_id: ID del pedido.\n\n    Returns:\n        El estado actual del pedido.\n    \"\"\"\n    # Simulamos una consulta a nuestro ORM.\n    return f\"El pedido con ID {order_id} est\u00e1 en camino.\"\n\ndef open_ticket(issue: str) -&gt; str:\n    \"\"\"Abre un ticket de soporte.\n\n    Args:\n        issue: Descripci\u00f3n del problema o incidencia.\n\n    Returns:\n        Confirmaci\u00f3n de apertura del ticket.\n    \"\"\"\n    # Simulamos una creacion de un ticket en nuestro sistema de soporte.\n    return f\"Se ha abierto un ticket de soporte para el problema: {issue}.\"\n\n\ntools = [check_price, order_status, open_ticket]\n\n# Vinculamos las tools al modelo de lenguaje\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nllm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n</code></pre> <ul> <li>Tools (<code>check_price</code>, <code>order_status</code>, <code>open_ticket</code>): Simulan herramientas externas que realizan distintas tareas para ventas y soporte.</li> </ul> Nota Importante <p>Cuando llamamos a <code>bind_tools</code>, no se modifica el modelo original (<code>llm</code>), sino que se genera una nueva instancia con las herramientas vinculadas. Por esta raz\u00f3n, es necesario asignarlo a una nueva variable (<code>llm_with_tools</code>).</p> <p>Esto garantiza que el modelo original permanezca sin cambios y podamos reutilizarlo o aplicar diferentes herramientas en otros contextos.</p> <p>Ejemplo:</p> <pre><code>llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n</code></pre> <p>\ud83d\udc49 <code>llm</code> sigue siendo el modelo base, mientras que <code>llm_with_tools</code> es la versi\u00f3n extendida con las tools activas.</p>"},{"location":"curso1/tema8_tools/#construccion-del-grafo","title":"\ud83c\udfd7\ufe0f Construcci\u00f3n del Grafo","text":"<p>Creamos los nodos y edges necesarios para integrar las tools al flujo del grafo.</p> <pre><code>from langgraph.graph import MessagesState\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.prebuilt import tools_condition\nfrom langgraph.prebuilt import ToolNode\n\nsys_msg = SystemMessage(content=\"Eres un asistente de ventas y soporte. Responde usando las herramientas disponibles.\")\n\ndef assistant(state: MessagesState):\n    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n\nbuilder = StateGraph(MessagesState)\n\n# A\u00f1adimos nodos\nbuilder.add_node(\"assistant\", assistant)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\n# Definimos los edges y el flujo del grafo\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_conditional_edges(\"assistant\", tools_condition)\nbuilder.add_edge(\"tools\", \"assistant\")\n\ngraph_with_tools = builder.compile()\n\nfrom IPython.display import Image, display\ndisplay(Image(graph_with_tools.get_graph(xray=True).draw_mermaid_png()))\n</code></pre> <p></p> <ul> <li>Assistant Node: El nodo principal procesa el mensaje del usuario y decide si debe invocar alguna tool.</li> <li>Router Condicional: El flujo se redirige al nodo de tools si se detecta una llamada a alguna de ellas.</li> </ul>"},{"location":"curso1/tema8_tools/#invocando-el-grafo","title":"\ud83d\ude80 Invocando el Grafo","text":"<p>Probamos el grafo enviando un mensaje del usuario para ver c\u00f3mo se invocan las tools en el flujo.</p> <pre><code>messages = [HumanMessage(content=\"Quiero saber el precio del producto 'RTX4070' y abrir un ticket de soporte.\")]\nresponse = graph_with_tools.invoke({\"messages\": messages})\n\nfor msg in response[\"messages\"]:\n    msg.pretty_print()\n</code></pre> Response<pre><code>================================ Human Message =================================\n\nQuiero saber el precio del producto 'RTX4070' y abrir un ticket de soporte.\n================================== Ai Message ==================================\nTool Calls:\n  check_price (call_0Ws3GDPS0sZHn3xhK6M10pxX)\n Call ID: call_0Ws3GDPS0sZHn3xhK6M10pxX\n  Args:\n    product: RTX4070\n================================= Tool Message =================================\nName: check_price\n\nEl precio de RTX4070 es de 100\u20ac.\n================================== Ai Message ==================================\n\nEl precio de la RTX4070 es de 100\u20ac.\n\nAhora, por favor, ind\u00edcame la descripci\u00f3n del problema o incidencia para abrir el ticket de soporte.\n</code></pre> <p>El chatbot analiza la solicitud, invoca las tools adecuadas y devuelve una respuesta consolidada al usuario.</p>"},{"location":"curso1/tema8_tools/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Ver ejemplo calculadora en Google Colab</li> <li> Definici\u00f3n: Tools</li> <li> Definici\u00f3n: tools_condition</li> <li> Definici\u00f3n: ToolNode</li> <li> How-to-guide: Tools</li> </ul>"},{"location":"curso1/tema8_tools/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Tools: Permiten extender el grafo con funciones externas que realizan tareas espec\u00edficas.</li> <li>Integraci\u00f3n con LLM: Las tools se vinculan directamente al modelo de lenguaje, permitiendo respuestas m\u00e1s avanzadas.</li> <li>Flujo Din\u00e1mico: El grafo puede invocar tools de forma condicional, adapt\u00e1ndose a las necesidades del usuario.</li> </ul>"},{"location":"curso1/tema8_tools/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Trim y Filter Messages, t\u00e9cnicas que permiten controlar y optimizar el historial de mensajes en el grafo. Esto es clave para gestionar conversaciones largas y garantizar que el modelo reciba solo la informaci\u00f3n m\u00e1s relevante.</p>"},{"location":"curso1/tema9_trim_filter/","title":"\u2702\ufe0f Tema 8: Trim y Filter Messages \u2013 Optimizaci\u00f3n del Historial de Mensajes","text":""},{"location":"curso1/tema9_trim_filter/#que-es-el-trimming-y-filtering-de-mensajes","title":"\ud83d\ude80 \u00bfQu\u00e9 es el Trimming y Filtering de Mensajes?","text":"<p>A medida que un grafo interact\u00faa con los usuarios, el historial de mensajes puede crecer considerablemente, lo que puede:</p> <ul> <li>Aumentar el costo de procesamiento para los modelos LLM.</li> <li>Reducir la eficiencia del grafo en conversaciones largas.</li> <li>Diluir el contexto, incluyendo mensajes irrelevantes.</li> </ul> <p>Para solucionar esto, LangGraph ofrece herramientas que permiten gestionar y optimizar el historial de mensajes:</p> <ol> <li><code>RemoveMessages</code> \u2013 Elimina mensajes espec\u00edficos.</li> <li><code>trim_messages</code> \u2013 Recorta el historial manteniendo solo los mensajes m\u00e1s recientes.</li> <li><code>summarize_conversation</code> \u2013 Resume la conversaci\u00f3n para reducir la longitud del historial, manteniendo el contexto.</li> </ol> Nota Importante <p>Al eliminar mensajes del historial, es crucial asegurarse de que la estructura de los mensajes siga siendo v\u00e1lida para el modelo LLM. Algunos modelos, como los chatbots basados en LLM, requieren que el primer mensaje sea de un Humano (<code>HumanMessage</code>).</p> <p>\ud83d\udc49 Antes de procesar el historial, verifica que la estructura cumpla con los requisitos del modelo para evitar errores en su funcionamiento.</p>"},{"location":"curso1/tema9_trim_filter/#por-que-es-importante","title":"\ud83e\udde0 \u00bfPor qu\u00e9 es Importante?","text":"<ul> <li>Optimizaci\u00f3n del Rendimiento: Reduce la cantidad de datos enviados al LLM.</li> <li>Mejora de la Precisi\u00f3n: Mantiene el contexto relevante, eliminando informaci\u00f3n redundante.</li> <li>Ahorro de Costos: Menos tokens procesados significa menos consumo de recursos.</li> </ul>"},{"location":"curso1/tema9_trim_filter/#como-funciona-el-trimming-y-filtering","title":"\u2699\ufe0f \u00bfC\u00f3mo Funciona el Trimming y Filtering?","text":"<p>LangGraph ofrece diferentes enfoques seg\u00fan el escenario:</p> <ul> <li>Eliminar mensajes irrelevantes o antiguos.</li> <li>Recortar autom\u00e1ticamente despu\u00e9s de alcanzar un l\u00edmite.</li> <li>Resumir el historial para mantener el contexto clave.</li> </ul>"},{"location":"curso1/tema9_trim_filter/#ejemplo-practico-chatbot-con-gestion-del-historial-de-mensajes","title":"\ud83d\udccb Ejemplo Pr\u00e1ctico: Chatbot con Gesti\u00f3n del Historial de Mensajes","text":"<p>Vamos a crear un chatbot que interact\u00faa con el usuario, pero gestiona el historial para mantener solo los mensajes relevantes y no saturar el flujo del grafo.</p>"},{"location":"curso1/tema9_trim_filter/#opcion-1-eliminar-mensajes-especificos-con-removemessages","title":"\ud83d\udee0\ufe0f Opci\u00f3n 1: Eliminar Mensajes Espec\u00edficos con <code>RemoveMessages</code>","text":"<p>Permite eliminar ciertos mensajes del historial, ideal para eliminar mensajes duplicados o irrelevantes.</p> <pre><code>from langchain_core.messages import HumanMessage, AIMessage, RemoveMessage\nfrom IPython.display import Image, display\nfrom langgraph.graph import MessagesState\nfrom langgraph.graph import StateGraph, START, END\n\n# Nodes\ndef filter_messages(state: MessagesState):\n    # Eliminamos todo el historial menos los 2 mas recientes\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"messages\": delete_messages}\n\ndef chat_model_node(state: MessagesState):\n    # Simulamos una respuesta de un LLM.\n    return {\"messages\": [\n        AIMessage(\n          content=\"El producto B cuesta 150\u20ac y est\u00e1 disponible en color rojo y azul. \u00bfTe interesa alguna de estas opciones?\",\n          id=\"accce84f-3e31-4cf2-b16d-5aed5a4b890a\"\n          )\n        ]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"filter\", filter_messages)\nbuilder.add_node(\"chat_model\", chat_model_node)\nbuilder.add_edge(START, \"filter\")\nbuilder.add_edge(\"filter\", \"chat_model\")\nbuilder.add_edge(\"chat_model\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <pre><code>messages = [\n    HumanMessage(content='Hola, necesito informaci\u00f3n sobre varios productos.', additional_kwargs={}, response_metadata={}, id='66a48850-bcf9-4500-9a0b-6a18ecad0d4a'),\n    AIMessage(content='Claro, puedo ayudarte con eso.', additional_kwargs={}, response_metadata={}, id='46bc8fa9-452b-4f16-b5d9-65f527812b8e'),\n    HumanMessage(content='Perfecto. Empecemos con el producto A.', additional_kwargs={}, response_metadata={}, id='ef3742dd-87ac-443a-bbff-72282fb4c1ca'),\n    AIMessage(content='El producto A cuesta 100\u20ac y est\u00e1 disponible.', additional_kwargs={}, response_metadata={}, id='038bff18-4498-4ca5-afa8-448c9118b1bc'),\n    HumanMessage(content='\u00bfY qu\u00e9 pasa con el producto B?', additional_kwargs={}, response_metadata={}, id='1d315b43-906e-48f0-a801-b2807e8abf0a')\n    ]\n\nresponse = graph.invoke({'messages': messages})\n\nfor m in response['messages']:\n    m.pretty_print()\n</code></pre> Resuldato<pre><code> ================================== Ai Message ==================================\n\nEl producto A cuesta 100\u20ac y est\u00e1 disponible.\n================================ Human Message =================================\n\n\u00bfY qu\u00e9 pasa con el producto B?\n================================== Ai Message ==================================\n\nEl producto B cuesta 150\u20ac y est\u00e1 disponible en color rojo y azul. \u00bfTe interesa alguna de estas opciones?\n</code></pre>"},{"location":"curso1/tema9_trim_filter/#opcion-2-recorte-de-mensajes-con-trim_messages","title":"\ud83d\udee0\ufe0f Opci\u00f3n 2: Recorte de Mensajes con <code>trim_messages</code>","text":"<p>Recorta el historial de mensajes manteniendo solo los \u00faltimos N mensajes. Esto es \u00fatil para evitar que el historial crezca sin control.</p> <pre><code>from langchain_core.messages import trim_messages\nfrom langchain_core.messages import HumanMessage, AIMessage\n\n# Estado con muchos mensajes\nstate = {\n    \"messages\": [\n      HumanMessage(content='Hola, necesito informaci\u00f3n sobre varios productos.', additional_kwargs={}, response_metadata={}, id='66a48850-bcf9-4500-9a0b-6a18ecad0d4a'),\n      AIMessage(content='Claro, puedo ayudarte con eso.', additional_kwargs={}, response_metadata={}, id='46bc8fa9-452b-4f16-b5d9-65f527812b8e'),\n      HumanMessage(content='Perfecto. Empecemos con el producto A.', additional_kwargs={}, response_metadata={}, id='ef3742dd-87ac-443a-bbff-72282fb4c1ca'),\n      AIMessage(content='El producto A cuesta 100\u20ac y est\u00e1 disponible.', additional_kwargs={}, response_metadata={}, id='038bff18-4498-4ca5-afa8-448c9118b1bc'),\n      HumanMessage(content='\u00bfY qu\u00e9 pasa con el producto B?', additional_kwargs={}, response_metadata={}, id='1d315b43-906e-48f0-a801-b2807e8abf0a')\n    ]\n}\n\n# Limitamos a los ultimos 3 mensajes para no sobrecargar la llamada.\nmessages = trim_messages(\n            state['messages'],\n            token_counter=len,  # Contamos mensajes (simple, no tokens)\n            max_tokens=3,       # L\u00edmite de 3 mensajes\n            strategy=\"last\",\n            #start_on=\"system\",   # Empieza en un mensaje humano o sistema-humano\n            include_system=True,\n            allow_partial=False\n        )\n\nfor m in messages:\n    m.pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nPerfecto. Empecemos con el producto A.\n================================== Ai Message ==================================\n\nEl producto A cuesta 100\u20ac y est\u00e1 disponible.\n================================ Human Message =================================\n\n\u00bfY qu\u00e9 pasa con el producto B?\n</code></pre>"},{"location":"curso1/tema9_trim_filter/#opcion-3-resumir-conversacion-con-summarize_conversation","title":"\ud83d\udee0\ufe0f Opci\u00f3n 3: Resumir Conversaci\u00f3n con <code>summarize_conversation</code>","text":"<p>En lugar de eliminar mensajes, genera un resumen de la conversaci\u00f3n manteniendo el contexto en menos palabras. Perfecto para mantener el contexto en conversaciones extensas.</p> <pre><code>def summarize_conversation(state: State):\n\n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n        # A summary already exists\n        summary_message = (\n            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n</code></pre> Tips <p>Veremos en m\u00e1s profuncidad este tema en el cap\u00edtulo: Tema 1: Chatbot Summarizing</p>"},{"location":"curso1/tema9_trim_filter/#opcion-4-filtrar-mensajes-con-filter_messages","title":"\ud83d\udee0\ufe0f Opci\u00f3n 4: Filtrar Mensajes con <code>filter_messages</code>","text":"<p>La funci\u00f3n <code>filter_messages</code> permite aplicar filtros personalizados al historial de mensajes en el grafo. Esto es \u00fatil para eliminar mensajes irrelevantes, mantener solo ciertos tipos de mensajes o aplicar l\u00f3gica m\u00e1s avanzada para estructurar el historial antes de enviarlo al LLM.</p> <pre><code>state = {\n    \"messages\": [\n      HumanMessage(content='Hola, necesito informaci\u00f3n sobre varios productos.', additional_kwargs={}, response_metadata={}, id='66a48850-bcf9-4500-9a0b-6a18ecad0d4a'),\n      AIMessage(content='Claro, puedo ayudarte con eso.', additional_kwargs={}, response_metadata={}, id='46bc8fa9-452b-4f16-b5d9-65f527812b8e'),\n      HumanMessage(content='Perfecto. Empecemos con el producto A.', additional_kwargs={}, response_metadata={}, id='ef3742dd-87ac-443a-bbff-72282fb4c1ca'),\n      AIMessage(content='El producto A cuesta 100\u20ac y est\u00e1 disponible.', additional_kwargs={}, response_metadata={}, id='038bff18-4498-4ca5-afa8-448c9118b1bc'),\n      HumanMessage(content='\u00bfY qu\u00e9 pasa con el producto B?', additional_kwargs={}, response_metadata={}, id='1d315b43-906e-48f0-a801-b2807e8abf0a')\n    ]\n}\n\n# Resumir la conversaci\u00f3n\nmessages = filter_messages(state[\"messages\"], include_types=\"human\")\n\nfor m in messages:\n    m.pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nHola, necesito informaci\u00f3n sobre varios productos.\n================================ Human Message =================================\n\nPerfecto. Empecemos con el producto A.\n================================ Human Message =================================\n\n\u00bfY qu\u00e9 pasa con el producto B?\n</code></pre>"},{"location":"curso1/tema9_trim_filter/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Mas ejemplos: Google Colab</li> <li> Definici\u00f3n: trim_messages</li> <li> Definici\u00f3n: RemoveMessages</li> <li> Definici\u00f3n: filter_messages</li> <li> How-to-guide: filter_messages</li> </ul>"},{"location":"curso1/tema9_trim_filter/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>RemoveMessages: Elimina mensajes espec\u00edficos del historial.</li> <li>trim_messages: Recorta el historial a los \u00faltimos mensajes.</li> <li>summarize_conversation: Genera res\u00famenes que conservan el contexto sin saturar el historial.</li> <li>filter_messages: Aplica filtros personalizados a un historial.</li> </ul>"},{"location":"curso1/tema9_trim_filter/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>\ud83c\udf89 \u00a1Has completado el Curso 1: Fundamentos de LangGraph! En este curso, aprendiste los conceptos esenciales de LangGraph, desde la definici\u00f3n de nodos y edges, hasta el uso de chains, routers, reducers y herramientas avanzadas como trim y filter messages.</p> <p>En el Curso 2, llevaremos tus conocimientos al siguiente nivel: Aprender\u00e1s a integrar LangGraph en aplicaciones reales, explorar el uso de memoria avanzada, flujos paralelos, y c\u00f3mo emplear LangGraph Studio para monitorizar y optimizar tus grafos. \u00a1Prep\u00e1rate para descubrir todo lo que LangGraph puede ofrecer en escenarios complejos y din\u00e1micos!</p>"},{"location":"curso2/","title":"\u2699\ufe0f Curso 2: Aplicaciones Avanzadas","text":""},{"location":"curso2/#bienvenida-al-curso-2","title":"\ud83d\udc4b Bienvenida al Curso 2","text":"<p>Bienvenido al segundo curso de nuestra serie sobre LangGraph. \ud83c\udf10 En este m\u00f3dulo, llevaremos tus habilidades al siguiente nivel al explorar aplicaciones avanzadas como chatbots, paralelismo, subgraf\u00edas y mucho m\u00e1s. Este curso se centra en maximizar la eficiencia y escalabilidad de tus flujos de trabajo.  </p>"},{"location":"curso2/#que-aprenderemos","title":"\ud83c\udfaf \u00bfQu\u00e9 Aprenderemos?","text":"<ul> <li>Desarrollar chatbots con capacidad de resumen y generaci\u00f3n de respuestas.  </li> <li>Implementar Human in the Loop para correcciones manuales.  </li> <li>Integrar streaming y puntos de interrupci\u00f3n din\u00e1micos.  </li> <li>Crear flujos paralelos y subgraf\u00edas interconectadas.  </li> <li>Usar LangGraph Studio y LangSmith para monitoreo y pruebas.  </li> </ul>"},{"location":"curso2/#objetivo-del-curso","title":"\ud83c\udfc6 Objetivo del Curso","text":"<p>El objetivo es ense\u00f1arte c\u00f3mo crear soluciones complejas con flujos paralelos, implementar intervenci\u00f3n humana en tiempo real y desarrollar chatbots de alto rendimiento integrando t\u00e9cnicas avanzadas.  </p>"},{"location":"curso2/#temario","title":"\ud83d\udccb Temario","text":"<ol> <li>Tema 1: Chatbot Summarizing </li> <li>Tema 2: Streaming </li> <li>Tema 3: Breakpoint </li> <li>Tema 4: Human in the Loop </li> <li>Tema 5: Paralelismo </li> <li>Tema 6: Subgraf\u00edas </li> <li>Tema 7: Mapas </li> <li>Tema 8: LangGraph Studio </li> <li>Tema 9: LangSmith </li> </ol>"},{"location":"curso2/#resultado-final-del-curso","title":"\ud83c\udfc1 Resultado Final del Curso","text":"<ul> <li>Crear\u00e1s chatbots avanzados y flujos con intervenci\u00f3n humana.  </li> <li>Ser\u00e1s capaz de implementar flujos de trabajo en paralelo y sistemas din\u00e1micos.  </li> <li>Aprender\u00e1s a manejar herramientas gr\u00e1ficas para monitorizar tus flujos de LangGraph.  </li> </ul>"},{"location":"curso2/#tecnologias-utilizadas","title":"\u2699\ufe0f Tecnolog\u00edas Utilizadas","text":"<ul> <li>LangGraph 0.3.x </li> <li>Python 3.10+ </li> <li>LangGraph Studio </li> <li>LangSmith </li> <li>FastAPI y websockets para streaming  </li> </ul>"},{"location":"curso2/tema1_summarizing/","title":"\ud83e\udde0 Tema 1: Chatbot Summarizing","text":""},{"location":"curso2/tema1_summarizing/#que-es-el-resumen-en-un-chatbot","title":"\ud83d\ude80 \u00bfQu\u00e9 es el Resumen en un Chatbot?","text":"<p>El resumen de conversaciones es una t\u00e9cnica que permite reducir el historial de mensajes, manteniendo el contexto clave de la conversaci\u00f3n. Esto es particularmente \u00fatil cuando trabajamos con LLMs que tienen un l\u00edmite de tokens o cuando deseamos optimizar el rendimiento del grafo.  </p>"},{"location":"curso2/tema1_summarizing/#por-que-es-importante-resumir","title":"\ud83e\udde0 \u00bfPor Qu\u00e9 es Importante Resumir?","text":"<ol> <li>Ahorro de Recursos: Los modelos LLM procesan menos datos, reduciendo costos y tiempo de respuesta.  </li> <li>Contexto Clave: Conserva la informaci\u00f3n m\u00e1s relevante sin perder el flujo de la conversaci\u00f3n.  </li> <li>Escalabilidad: Permite manejar conversaciones largas sin que el historial crezca de forma descontrolada.  </li> </ol> Tips <p>El resumen convierte un historial largo en un contexto breve, relevante y efectivo.</p>"},{"location":"curso2/tema1_summarizing/#como-funciona","title":"\u2699\ufe0f \u00bfC\u00f3mo Funciona?","text":"<p>En este ejemplo pr\u00e1ctico, veremos un grafo que: 1. Procesa una conversaci\u00f3n y genera respuestas. 2. Cuando el historial supera un l\u00edmite, crea un resumen de la conversaci\u00f3n anterior. 3. Contin\u00faa la conversaci\u00f3n manteniendo solo los mensajes m\u00e1s recientes y el resumen generado.  </p> <p>Esto garantiza que el modelo trabaje con informaci\u00f3n relevante y no con mensajes redundantes.  </p>"},{"location":"curso2/tema1_summarizing/#componentes-clave-del-grafo","title":"\ud83d\udee0\ufe0f Componentes Clave del Grafo","text":"<ol> <li> <p>Nodo de Conversaci\u00f3n (<code>call_model</code>): </p> <ul> <li>Procesa los mensajes del usuario y genera respuestas usando el modelo LLM.  </li> <li>Si existe un resumen previo, se a\u00f1ade al historial para mantener el contexto.  </li> </ul> </li> <li> <p>Nodo de Resumen (<code>summarize_conversation</code>): </p> <ul> <li>Genera un resumen del historial y elimina los mensajes antiguos, manteniendo los m\u00e1s recientes.  </li> </ul> </li> <li> <p>Condici\u00f3n de Continuaci\u00f3n (<code>should_continue</code>): </p> <ul> <li>Decide si continuar la conversaci\u00f3n o resumir el historial basado en la cantidad de mensajes.  </li> </ul> </li> <li> <p>Memoria Persistente (<code>MemorySaver</code>): </p> <ul> <li>Almacena el historial y el resumen para garantizar que el grafo recuerde el contexto a lo largo de la interacci\u00f3n.  </li> </ul> </li> </ol>"},{"location":"curso2/tema1_summarizing/#ejemplo-practico-chatbot-con-resumen-automatico","title":"\ud83d\udccb Ejemplo Pr\u00e1ctico: Chatbot con Resumen Autom\u00e1tico","text":"<p>Vamos a construir un grafo que implemente estas t\u00e9cnicas.  </p> <pre><code>from langgraph.graph import MessagesState\nfrom langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage, AIMessage\nfrom langgraph.graph import END\nfrom IPython.display import Image, display\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nclass State(MessagesState):\n    summary: str\n\n\n# Definimos la logica de nuetro grafo.\ndef call_model(state: State):\n    print(\"---call_model---\")\n    # Get summary if it exists\n    summary = state.get(\"summary\", \"\")\n\n    # If there is summary, then we add it\n    if summary:\n        # Add summary to system message\n        system_message = f\"Resumen de la conversaci\u00f3n anterior: {summary}\"\n        # Append summary to any newer messages\n        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n    else:\n        messages = state[\"messages\"]\n\n    response = llm.invoke(messages)\n    return {\"messages\": response}\n\ndef summarize_conversation(state: State):\n    print(\"---summarize_conversation---\")\n    # Primero, cogemos si existe algun resumen previo.\n    summary = state.get(\"summary\", \"\")\n    # Create our summarization prompt \n    if summary:\n        summary_message = (\n            f\"Este es el resumen de la conversaci\u00f3n hasta la fecha: {summary}\\n\\n\"\n              \"Ampl\u00ede el resumen teniendo en cuenta los nuevos mensajes anteriores:\"\n        )\n    else:\n        summary_message = \"Crea un resumen de la conversaci\u00f3n anterior:\"\n    # A\u00f1adimos el prompt a nuestra conversaci\u00f3n.\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = llm.invoke(messages)\n    # Eliminamos todos los mensajes antiguos menos los \u00faltimos dos.\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n\n# Determina si tenemos que resumir o no.\ndef should_continue(state: State):\n    print(\"---should_continue---\")\n    \"\"\"Return the next node to execute.\"\"\"\n    messages = state[\"messages\"]\n    # If there are more than six messages, then we summarize the conversation\n    if len(messages) &gt; 6:\n        return \"summarize_conversation\"\n    # Otherwise we can just end\n    return END\n\n\n# Define a new graph\nworkflow = StateGraph(State)\nworkflow.add_node(\"conversation\", call_model)\nworkflow.add_node(summarize_conversation)\n\n# Set the entrypoint as conversation\nworkflow.add_edge(START, \"conversation\")\nworkflow.add_conditional_edges(\"conversation\", should_continue)\nworkflow.add_edge(\"summarize_conversation\", END)\n\n# Compile\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p>"},{"location":"curso2/tema1_summarizing/#explicacion-del-codigo","title":"\ud83d\udd0d Explicaci\u00f3n del C\u00f3digo","text":"<ol> <li> <p>Clase <code>State</code>: </p> <ul> <li>Define el estado del grafo, que incluye el historial de mensajes (<code>messages</code>) y el resumen (<code>summary</code>).  </li> </ul> </li> <li> <p>Nodo <code>call_model</code>: </p> <ul> <li>Genera respuestas basadas en el historial.  </li> <li>Si existe un resumen, lo incluye como mensaje del sistema.  </li> </ul> </li> <li> <p>Nodo <code>summarize_conversation</code>: </p> <ul> <li>Crea un resumen usando el modelo LLM.  </li> <li>Elimina los mensajes antiguos, manteniendo solo los m\u00e1s recientes y el resumen.  </li> </ul> </li> <li> <p>Condici\u00f3n <code>should_continue</code>: </p> <ul> <li>Si el historial supera 6 mensajes, dirige el flujo al nodo de resumen.  </li> <li>De lo contrario, finaliza la conversaci\u00f3n.  </li> </ul> </li> <li> <p>Grafo y Memoria: </p> <ul> <li>El grafo utiliza <code>MemorySaver</code> para persistir el estado entre interacciones.  </li> </ul> </li> </ol>"},{"location":"curso2/tema1_summarizing/#veamos-como-funciona-con-un-ejemplo","title":"\ud83d\ude80 Veamos C\u00f3mo Funciona Con Un Ejemplo","text":"<p>Iniciamos una conversaci\u00f3n ficticia entre un usuario y un asistente virtual para observar c\u00f3mo el grafo gestiona y resume el historial.  </p>"},{"location":"curso2/tema1_summarizing/#primera-iteracion-conversacion-inicial","title":"\ud83d\udde3\ufe0f Primera Iteraci\u00f3n: Conversaci\u00f3n Inicial","text":"<p>Comenzamos con un intercambio sencillo, donde el usuario solicita informaci\u00f3n y el asistente responde con detalles sobre los servicios ofrecidos.  </p> <pre><code># Creamos un hilo\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Inicializamos una conversaci\u00f3n\nmessages = [\n    HumanMessage(content=\"Hola, soy Ra\u00fal.\"),\n    AIMessage(content=\"\u00a1Hola Ra\u00fal! \u00bfEn qu\u00e9 puedo ayudarte hoy?\"),\n    HumanMessage(content=\"Estoy interesado en conocer m\u00e1s sobre vuestros servicios.\"),\n    AIMessage(content=\"Claro, ofrecemos varios servicios. \u00bfHay alguno en particular que te interese?\"),\n    HumanMessage(content=\"S\u00ed, quiero saber m\u00e1s sobre el servicio de soporte t\u00e9cnico.\"),\n]\nresponse = graph.invoke({\"messages\": messages}, config) \n\nfor m in response[\"messages\"]:\n    m.pretty_print()\n</code></pre> Respuesta<pre><code>---call_model---\n---should_continue---\n================================ Human Message =================================\n\nHola, soy Ra\u00fal.\n================================== Ai Message ==================================\n\n\u00a1Hola Ra\u00fal! \u00bfEn qu\u00e9 puedo ayudarte hoy?\n================================ Human Message =================================\n\nEstoy interesado en conocer m\u00e1s sobre vuestros servicios.\n================================== Ai Message ==================================\n\nClaro, ofrecemos varios servicios. \u00bfHay alguno en particular que te interese?\n================================ Human Message =================================\n\nS\u00ed, quiero saber m\u00e1s sobre el servicio de soporte t\u00e9cnico.\n================================== Ai Message ==================================\n\nNuestro servicio de soporte t\u00e9cnico proporciona asistencia para resolver problemas relacionados con software, hardware y otras tecnolog\u00edas. Esto incluye:\n\n1. **Asistencia Remota**: Te ayudamos a trav\u00e9s de conexi\u00f3n remota para solucionar problemas en tiempo real.\n\n2. **Asesor\u00eda y Diagn\u00f3stico**: Evaluamos el problema y te ofrecemos recomendaciones y soluciones adecuadas.\n\n3. **Mantenimiento Preventivo**: Realizamos chequeos peri\u00f3dicos para prevenir fallos y optimizar el rendimiento de tus equipos.\n\n4. **Soporte 24/7**: Ofrecemos asistencia en cualquier momento para resolver emergencias.\n\n5. **Capacitaci\u00f3n**: Proporcionamos formaci\u00f3n sobre el uso de software y hardware, as\u00ed como buenas pr\u00e1cticas de seguridad.\n\nSi necesitas m\u00e1s detalles o tienes alguna pregunta espec\u00edfica, h\u00e1zmelo saber.\n</code></pre> Nota <p>En esta etapa, no se activa la funci\u00f3n de resumen, ya que el historial a\u00fan no supera los 6 mensajes.</p>"},{"location":"curso2/tema1_summarizing/#segunda-iteracion-mas-preguntas","title":"\ud83d\udde3\ufe0f Segunda Iteraci\u00f3n: M\u00e1s Preguntas","text":"<p>El usuario realiza una nueva pregunta sobre los costos, y el grafo comienza a acercarse al l\u00edmite del historial.</p> <pre><code>input_message = HumanMessage(content=\"\u00bfCu\u00e1nto cuesta el soporte t\u00e9cnico mensual?\")\nreponse = graph.invoke({\"messages\": [input_message]}, config) \n\nfor m in reponse['messages']:\n    m.pretty_print()\n</code></pre> Respuesta<pre><code>---call_model---\n---should_continue---\n---summarize_conversation---\n================================ Human Message =================================\n\n\u00bfCu\u00e1nto cuesta el soporte t\u00e9cnico mensual?\n================================== Ai Message ==================================\n\nLos precios del soporte t\u00e9cnico pueden variar seg\u00fan el proveedor y el tipo de servicios que elijas. Generalmente, las tarifas pueden depender de factores como:\n\n1. **Nivel de soporte**: Soporte b\u00e1sico vs. soporte avanzado.\n2. **Tipo de asistencia**: Remota o en sitio.\n3. **N\u00famero de dispositivos**: Cu\u00e1ntos equipos o usuarios necesitan soporte.\n4. **Frecuencia**: Si se requiere asistencia continua o espor\u00e1dica.\n\nPara darte un precio m\u00e1s espec\u00edfico, ser\u00eda \u00fatil saber m\u00e1s sobre tus necesidades: \u00bfcu\u00e1ntos dispositivos necesitas cubrir? \u00bfqu\u00e9 tipo de soporte buscas? Si tienes un proveedor espec\u00edfico en mente, te recomendar\u00eda consultar sus tarifas directamente. Si necesitas ayuda para encontrar opciones, puedo ofrecerte orientaci\u00f3n.\n</code></pre>"},{"location":"curso2/tema1_summarizing/#resumen-automatico","title":"\ud83d\uddc2\ufe0f Resumen Autom\u00e1tico","text":"<p>Al superar los 6 mensajes, el grafo activa el nodo de resumen. El historial completo se procesa, y se genera un resumen que reemplaza los mensajes antiguos. Solo las dos entradas m\u00e1s recientes permanecen en el historial.</p> <p>Podemos observar el resumen accediendo a la memoria:</p> <pre><code>graph.get_state(config).values.get(\"summary\",\"\")\n</code></pre> Resumen<pre><code>Claro, aqu\u00ed tienes un resumen de nuestra conversaci\u00f3n:\n\n1. **Introducci\u00f3n**: Ra\u00fal expres\u00f3 inter\u00e9s en conocer m\u00e1s sobre los servicios ofrecidos.\n2. **Detalles del Soporte T\u00e9cnico**: Proporcion\u00e9 informaci\u00f3n sobre el servicio de soporte t\u00e9cnico, que incluye asistencia remota, asesor\u00eda y diagn\u00f3stico, mantenimiento preventivo, soporte 24/7 y capacitaci\u00f3n.\n3. **Costo del Soporte T\u00e9cnico**: Ra\u00fal pregunt\u00f3 sobre los precios del soporte t\u00e9cnico mensual. Le inform\u00e9 que los precios pueden variar dependiendo de diferentes factores, como el nivel de soporte, tipo de asistencia, n\u00famero de dispositivos y frecuencia de uso, y le suger\u00ed que indagara con proveedores espec\u00edficos.\n\nSi necesitas algo m\u00e1s o deseas ampliar alg\u00fan punto, \u00a1hazmelo saber!\n</code></pre>"},{"location":"curso2/tema1_summarizing/#tercera-iteracion-continuacion-del-contexto","title":"\ud83d\udde3\ufe0f Tercera Iteraci\u00f3n: Continuaci\u00f3n del Contexto","text":"<p>El asistente contin\u00faa ofreciendo respuestas relevantes, manteniendo el contexto del resumen generado.</p> <p><pre><code>input_message = HumanMessage(content=\"\u00bfOfrecen alg\u00fan descuento si contrato varios meses?\")\nreponse = graph.invoke({\"messages\": [input_message]}, config) \n\nfor m in reponse['messages']:\n    m.pretty_print()\n</code></pre> Respuesta<pre><code>---call_model---\n---should_continue---\n================================ Human Message =================================\n\n\u00bfCu\u00e1nto cuesta el soporte t\u00e9cnico mensual?\n================================== Ai Message ==================================\n\nLos precios del soporte t\u00e9cnico pueden variar seg\u00fan el proveedor y el tipo de servicios que elijas. Generalmente, las tarifas pueden depender de factores como:\n\n1. **Nivel de soporte**: Soporte b\u00e1sico vs. soporte avanzado.\n2. **Tipo de asistencia**: Remota o en sitio.\n3. **N\u00famero de dispositivos**: Cu\u00e1ntos equipos o usuarios necesitan soporte.\n4. **Frecuencia**: Si se requiere asistencia continua o espor\u00e1dica.\n\nPara darte un precio m\u00e1s espec\u00edfico, ser\u00eda \u00fatil saber m\u00e1s sobre tus necesidades: \u00bfcu\u00e1ntos dispositivos necesitas cubrir? \u00bfqu\u00e9 tipo de soporte buscas? Si tienes un proveedor espec\u00edfico en mente, te recomendar\u00eda consultar sus tarifas directamente. Si necesitas ayuda para encontrar opciones, puedo ofrecerte orientaci\u00f3n.\n================================ Human Message =================================\n\n\u00bfOfrecen alg\u00fan descuento si contrato varios meses?\n================================== Ai Message ==================================\n\nMuchos proveedores de servicios de soporte t\u00e9cnico s\u00ed ofrecen descuentos por contrataci\u00f3n de servicios por varios meses o en paquetes anuales. Algunos beneficios comunes pueden incluir:\n\n1. **Descuentos por volumen**: Reducir el costo mensual si contratas un n\u00famero mayor de dispositivos o usuarios.\n2. **Descuentos por contrato a largo plazo**: Tarifas m\u00e1s bajas si eliges un contrato de 6 meses, 12 meses o m\u00e1s.\n3. **Paquetes promocionales**: Ofertas especiales que combinan diferentes servicios a un costo reducido.\n\nTe recomendar\u00eda preguntar directamente al proveedor de soporte t\u00e9cnico que est\u00e9s considerando para obtener detalles espec\u00edficos sobre posibles descuentos y promociones. Si necesitas ayuda con alg\u00fan proveedor en particular, h\u00e1zmelo saber.\n</code></pre></p>"},{"location":"curso2/tema1_summarizing/#conclusion","title":"\ud83c\udf93 Conclusi\u00f3n","text":"<p>Como hemos visto, resumir el historial de mensajes es una estrategia muy \u00fatil para manejar conversaciones largas sin perder el contexto y evitando exceder los l\u00edmites de tokens del modelo.  </p> <p>Con este enfoque, el chatbot podr\u00eda incluso responder preguntas como \"\u00bfC\u00f3mo me llamo?\" sin problemas, ya que conserva la informaci\u00f3n clave en el resumen.  </p> <p>Es importante destacar que el prompt utilizado para generar el resumen puede ser refinado y personalizado para adaptarse mejor a nuestras necesidades espec\u00edficas, lo que demuestra el potencial y la flexibilidad de esta t\u00e9cnica.  </p>"},{"location":"curso2/tema1_summarizing/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> M\u00e1s ejemplos: Google Colab</li> <li> Definici\u00f3n del concepto: Summarizing</li> <li> How-to-guide: Summarizing</li> </ul>"},{"location":"curso2/tema1_summarizing/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Resumir Conversaciones: Reduce el historial manteniendo el contexto relevante.  </li> <li>Grafo Din\u00e1mico: El flujo del grafo se adapta seg\u00fan la cantidad de mensajes en el historial.  </li> <li>Persistencia de Memoria: Usa memoria persistente para recordar informaci\u00f3n entre interacciones.  </li> </ul>"},{"location":"curso2/tema1_summarizing/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Human in the Loop, una t\u00e9cnica para integrar intervenci\u00f3n humana en el flujo de un grafo, permitiendo un control m\u00e1s preciso en interacciones complejas.  </p>"},{"location":"curso2/tema2_streaming/","title":"\ud83c\udf0a Tema 3: Streaming \u2013 Procesamiento en Tiempo Real","text":""},{"location":"curso2/tema2_streaming/#que-es-el-streaming-en-langgraph","title":"\ud83d\ude80 \u00bfQu\u00e9 es el Streaming en LangGraph?","text":"<p>El streaming en LangGraph es una t\u00e9cnica que permite procesar datos en tiempo real, emitiendo resultados de manera progresiva mientras el grafo se ejecuta. A diferencia de los flujos tradicionales, donde el resultado se genera solo al finalizar, el streaming proporciona una respuesta parcial y continua, lo que mejora la interacci\u00f3n con el usuario y permite manejar flujos din\u00e1micos de datos.  </p>"},{"location":"curso2/tema2_streaming/#por-que-es-importante-el-streaming","title":"\ud83e\udde0 \u00bfPor Qu\u00e9 es Importante el Streaming?","text":"<ol> <li>Interactividad Inmediata: Mejora la experiencia del usuario al ofrecer resultados parciales en lugar de esperar a que termine todo el flujo.  </li> <li>Optimizaci\u00f3n del Tiempo: Permite iniciar acciones mientras el grafo sigue trabajando.  </li> <li>Escenarios Complejos: Ideal para aplicaciones en tiempo real como chatbots, procesamiento de grandes vol\u00famenes de datos, y m\u00e1s.  </li> </ol> <p>El streaming convierte un grafo en una herramienta m\u00e1s reactiva y adaptable.  </p>"},{"location":"curso2/tema2_streaming/#streaming-graph-outputs","title":"\ud83c\udf1f Streaming Graph Outputs","text":"<p>El streaming de outputs permite procesar y emitir los resultados del grafo de manera progresiva, a medida que los nodos generan informaci\u00f3n.  </p> <p>LangGraph proporciona cinco modos principales de streaming:  </p> <ul> <li><code>stream_mode='updates'</code>: Emite actualizaciones parciales del estado a medida que se procesan los nodos.  </li> <li><code>stream_mode='values'</code>: Emite los valores completos del estado resultante despu\u00e9s de cada nodo o al finalizar el grafo.  </li> <li><code>stream_mode='custom'</code>: Emite datos personalizados definidos dentro de los nodos del grafo, proporcionando flexibilidad para transmitir informaci\u00f3n espec\u00edfica seg\u00fan las necesidades del flujo.  </li> <li><code>stream_mode='messages'</code>: Transmite los tokens generados por el modelo LLM junto con los metadatos asociados al nodo en el que se invoca el LLM.  </li> <li><code>stream_mode='debug'</code>: Proporciona informaci\u00f3n detallada de depuraci\u00f3n, transmitiendo la mayor cantidad de datos posible durante la ejecuci\u00f3n del grafo.  </li> </ul> Nota <p>Todos los modos son accesibles mediante la funci\u00f3n <code>stream</code> o <code>astream</code>, que facilita la captura y manejo de estos resultados en tiempo real.  </p>"},{"location":"curso2/tema2_streaming/#ejemplo-practico","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico","text":"<p>Vamos a utilizar la calculadora que creamos en el tema8: tools y probaremos algunos modos de streaming.  </p>"},{"location":"curso2/tema2_streaming/#ejemplo-1-stream_modeupdates","title":"Ejemplo 1: <code>stream_mode='updates'</code>","text":"<p>En este modo, el grafo emite actualizaciones parciales del estado mientras procesa cada nodo.  </p> <pre><code>config1 = {\"configurable\": {\"thread_id\": \"1\"}}\nfor chunk in calculadora.stream({\"messages\": [HumanMessage(content=\"Me puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\")]}, config1, stream_mode=\"updates\"):\n    print(chunk)\n</code></pre> Resultado<pre><code>{'assistant': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gn7dz3d2ZttNgSvzIHgknjqI', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 394, 'total_tokens': 413, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-972d7d8b-8307-476f-9366-93930f33f304-0', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_gn7dz3d2ZttNgSvzIHgknjqI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 394, 'output_tokens': 19, 'total_tokens': 413, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n{'tools': {'messages': [ToolMessage(content='5', name='sumar', id='c64de46f-9490-4a2d-83dc-e6eb9afe4b80', tool_call_id='call_gn7dz3d2ZttNgSvzIHgknjqI')]}}\n{'assistant': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_OMRcNRCNCtoNSqF5HGvY9c0V', 'function': {'arguments': '{\"a\":5,\"b\":2}', 'name': 'multiplicar'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 421, 'total_tokens': 441, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2235a802-d120-4162-98dd-3b8bd8985f87-0', tool_calls=[{'name': 'multiplicar', 'args': {'a': 5, 'b': 2}, 'id': 'call_OMRcNRCNCtoNSqF5HGvY9c0V', 'type': 'tool_call'}], usage_metadata={'input_tokens': 421, 'output_tokens': 20, 'total_tokens': 441, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n{'tools': {'messages': [ToolMessage(content='10', name='multiplicar', id='065ec0a2-f4e6-4b15-8933-257a145f63a3', tool_call_id='call_OMRcNRCNCtoNSqF5HGvY9c0V')]}}\n{'assistant': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QUPAVI0aEgBjZGCXU0l223H8', 'function': {'arguments': '{\"a\":10,\"b\":5}', 'name': 'dividir'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 449, 'total_tokens': 468, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e9604c6a-3947-43a7-9d97-f54bc686bd89-0', tool_calls=[{'name': 'dividir', 'args': {'a': 10, 'b': 5}, 'id': 'call_QUPAVI0aEgBjZGCXU0l223H8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 449, 'output_tokens': 19, 'total_tokens': 468, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n{'tools': {'messages': [ToolMessage(content='2.0', name='dividir', id='eae662a1-3ea0-4cb0-b26f-72ff75fa44cf', tool_call_id='call_QUPAVI0aEgBjZGCXU0l223H8')]}}\n{'assistant': {'messages': [AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(((3+2)*2)/5\\\\) es \\\\(2.0\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 478, 'total_tokens': 503, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d7e4b5a5-15b8-43fc-a3fc-1c55cf939620-0', usage_metadata={'input_tokens': 478, 'output_tokens': 25, 'total_tokens': 503, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n</code></pre>"},{"location":"curso2/tema2_streaming/#ejemplo-2-stream_modevalues","title":"Ejemplo 2: <code>stream_mode='values'</code>","text":"<p>En este modo, el grafo emite valores completos del estado resultante despu\u00e9s de cada nodo o al final del flujo.  </p> <pre><code>config2 = {\"configurable\": {\"thread_id\": \"2\"}}\nfor chunk in calculadora.stream({\"messages\": [HumanMessage(content=\"Me puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\")]}, config2, stream_mode=\"values\"):\n    for m in chunk['messages']:\n        m.pretty_print()\n    print(\"//\"*40)\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n////////////////////////////////////////////////////////////////////////////////\n================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_QIY2zprLQo9zC3eu7hnLK4up)\n Call ID: call_QIY2zprLQo9zC3eu7hnLK4up\n  Args:\n    a: 3\n    b: 2\n////////////////////////////////////////////////////////////////////////////////\n================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_QIY2zprLQo9zC3eu7hnLK4up)\n Call ID: call_QIY2zprLQo9zC3eu7hnLK4up\n  Args:\n    a: 3\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n5\n////////////////////////////////////////////////////////////////////////////////\n================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_QIY2zprLQo9zC3eu7hnLK4up)\n Call ID: call_QIY2zprLQo9zC3eu7hnLK4up\n  Args:\n    a: 3\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n5\n================================== Ai Message ==================================\nTool Calls:\n  multiplicar (call_lqhLe5Qq8avnfzcACBCWEALs)\n Call ID: call_lqhLe5Qq8avnfzcACBCWEALs\n  Args:\n    a: 5\n    b: 2\n////////////////////////////////////////////////////////////////////////////////\n================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_QIY2zprLQo9zC3eu7hnLK4up)\n Call ID: call_QIY2zprLQo9zC3eu7hnLK4up\n  Args:\n    a: 3\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n5\n================================== Ai Message ==================================\nTool Calls:\n  multiplicar (call_lqhLe5Qq8avnfzcACBCWEALs)\n Call ID: call_lqhLe5Qq8avnfzcACBCWEALs\n  Args:\n    a: 5\n    b: 2\n================================= Tool Message =================================\nName: multiplicar\n\n10\n////////////////////////////////////////////////////////////////////////////////\n================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_QIY2zprLQo9zC3eu7hnLK4up)\n Call ID: call_QIY2zprLQo9zC3eu7hnLK4up\n  Args:\n    a: 3\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n5\n================================== Ai Message ==================================\nTool Calls:\n  multiplicar (call_lqhLe5Qq8avnfzcACBCWEALs)\n Call ID: call_lqhLe5Qq8avnfzcACBCWEALs\n  Args:\n    a: 5\n    b: 2\n================================= Tool Message =================================\nName: multiplicar\n\n10\n================================== Ai Message ==================================\nTool Calls:\n  dividir (call_sGwYH4gwnjh5ii9WBc2TkXPB)\n Call ID: call_sGwYH4gwnjh5ii9WBc2TkXPB\n  Args:\n    a: 10\n    b: 5\n////////////////////////////////////////////////////////////////////////////////\n================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_QIY2zprLQo9zC3eu7hnLK4up)\n Call ID: call_QIY2zprLQo9zC3eu7hnLK4up\n  Args:\n    a: 3\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n5\n================================== Ai Message ==================================\nTool Calls:\n  multiplicar (call_lqhLe5Qq8avnfzcACBCWEALs)\n Call ID: call_lqhLe5Qq8avnfzcACBCWEALs\n  Args:\n    a: 5\n    b: 2\n================================= Tool Message =================================\nName: multiplicar\n\n10\n================================== Ai Message ==================================\nTool Calls:\n  dividir (call_sGwYH4gwnjh5ii9WBc2TkXPB)\n Call ID: call_sGwYH4gwnjh5ii9WBc2TkXPB\n  Args:\n    a: 10\n    b: 5\n================================= Tool Message =================================\nName: dividir\n\n2.0\n////////////////////////////////////////////////////////////////////////////////\n================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_QIY2zprLQo9zC3eu7hnLK4up)\n Call ID: call_QIY2zprLQo9zC3eu7hnLK4up\n  Args:\n    a: 3\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n5\n================================== Ai Message ==================================\nTool Calls:\n  multiplicar (call_lqhLe5Qq8avnfzcACBCWEALs)\n Call ID: call_lqhLe5Qq8avnfzcACBCWEALs\n  Args:\n    a: 5\n    b: 2\n================================= Tool Message =================================\nName: multiplicar\n\n10\n================================== Ai Message ==================================\nTool Calls:\n  dividir (call_sGwYH4gwnjh5ii9WBc2TkXPB)\n Call ID: call_sGwYH4gwnjh5ii9WBc2TkXPB\n  Args:\n    a: 10\n    b: 5\n================================= Tool Message =================================\nName: dividir\n\n2.0\n================================== Ai Message ==================================\n\nLa soluci\u00f3n de la ecuaci\u00f3n \\(((3+2)*2)/5\\) es \\(2.0\\).\n////////////////////////////////////////////////////////////////////////////////\n</code></pre>"},{"location":"curso2/tema2_streaming/#ejemplo-3-stream_modedebug","title":"Ejemplo 3: <code>stream_mode='debug'</code>","text":"<p>En este modo, el grafo proporciona informaci\u00f3n detallada de depuraci\u00f3n del estado resultante durante la ejecuci\u00f3n del grafo.  </p> <pre><code>config3 = {\"configurable\": {\"thread_id\": \"3\"}}\nfor chunk in calculadora.stream({\"messages\": [HumanMessage(content=\"Me puedes resolver esta ecuaci\u00f3n: ((3+2)*2)/5?\")]}, config1, stream_mode=\"debug\"):\n    print(chunk)\n</code></pre> <pre><code>{'type': 'task', 'timestamp': '2025-01-16T11:21:54.720593+00:00', 'step': 1, 'payload': {'id': '8ecbef52-7a8c-0647-27d6-cdb28051ea9e', 'name': 'node1', 'input': {'messages': [HumanMessage(content='---inicio---', additional_kwargs={}, response_metadata={}, id='7774d4e0-11f5-4a3d-a898-be405bf9599a')]}, 'triggers': ['start:node1']}}\n{'type': 'task_result', 'timestamp': '2025-01-16T11:21:54.722972+00:00', 'step': 1, 'payload': {'id': '8ecbef52-7a8c-0647-27d6-cdb28051ea9e', 'name': 'node1', 'error': None, 'result': [('messages', '---node1---')], 'interrupts': []}}\n{'type': 'task', 'timestamp': '2025-01-16T11:21:54.724961+00:00', 'step': 2, 'payload': {'id': '9adfad56-9c9c-9795-8d42-6c329e9fbcc2', 'name': 'node2', 'input': {'messages': [HumanMessage(content='---inicio---', additional_kwargs={}, response_metadata={}, id='7774d4e0-11f5-4a3d-a898-be405bf9599a'), HumanMessage(content='---node1---', additional_kwargs={}, response_metadata={}, id='5995d43e-6f2b-41a3-b12c-8ff9bb090d4f')]}, 'triggers': ['node1']}}\n{'type': 'task_result', 'timestamp': '2025-01-16T11:21:54.726201+00:00', 'step': 2, 'payload': {'id': '9adfad56-9c9c-9795-8d42-6c329e9fbcc2', 'name': 'node2', 'error': None, 'result': [('messages', '---node2---')], 'interrupts': []}}\n{'type': 'task', 'timestamp': '2025-01-16T11:21:54.726641+00:00', 'step': 3, 'payload': {'id': 'c8066a74-accd-fe92-2ffa-af9112d97a26', 'name': 'node3', 'input': {'messages': [HumanMessage(content='---inicio---', additional_kwargs={}, response_metadata={}, id='7774d4e0-11f5-4a3d-a898-be405bf9599a'), HumanMessage(content='---node1---', additional_kwargs={}, response_metadata={}, id='5995d43e-6f2b-41a3-b12c-8ff9bb090d4f'), HumanMessage(content='---node2---', additional_kwargs={}, response_metadata={}, id='bf320179-1488-4700-ac3c-e6b69c3a821c')]}, 'triggers': ['node2']}}\n{'type': 'task_result', 'timestamp': '2025-01-16T11:21:54.727115+00:00', 'step': 3, 'payload': {'id': 'c8066a74-accd-fe92-2ffa-af9112d97a26', 'name': 'node3', 'error': None, 'result': [('messages', '---node3---')], 'interrupts': []}}\n</code></pre>"},{"location":"curso2/tema2_streaming/#eventos-en-streaming","title":"\ud83c\udfaf Eventos en Streaming","text":""},{"location":"curso2/tema2_streaming/#que-son-los-eventos-en-streaming","title":"\ud83d\udd0d \u00bfQu\u00e9 son los Eventos en Streaming?","text":"<p>Los eventos en streaming son notificaciones generadas durante la ejecuci\u00f3n del grafo. Estos eventos proporcionan informaci\u00f3n detallada sobre: - Qu\u00e9 nodos se est\u00e1n ejecutando. - Qu\u00e9 tipo de actividad est\u00e1 ocurriendo (inicio, finalizaci\u00f3n, error, etc.). - Detalles adicionales como el nombre del nodo y metadatos relevantes.  </p> <p>LangGraph gestiona los eventos mediante la funci\u00f3n <code>aStream_events</code>, que permite capturarlos y procesarlos en tiempo real.  </p>"},{"location":"curso2/tema2_streaming/#ejemplo1-capturando-eventos-en-streaming","title":"\ud83d\udee0\ufe0f Ejemplo1: Capturando Eventos en Streaming","text":"<p>Usaremos el siguiente ejemplo para observar c\u00f3mo los eventos nos permiten monitorear el flujo del grafo:  </p> <p><pre><code>config5 = {\"configurable\": {\"thread_id\": \"5\"}}\nasync for event in calculadora.astream_events({\"messages\": [HumanMessage(content=\"Me puedes resolver esta ecuaci\u00f3n: 3+2?\")]}, config5, version=\"v2\"):\n    print(event)\n</code></pre> <pre><code>{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'name': 'LangGraph', 'tags': [], 'run_id': 'e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'metadata': {'thread_id': '5'}, 'parent_ids': []}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'name': '__start__', 'tags': ['graph:step:0', 'langsmith:hidden'], 'run_id': '983889a7-ca3a-47c5-bb09-67339220dfee', 'metadata': {'thread_id': '5', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:6df9b3fd-426d-a817-b365-da3423177628'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'name': '_write', 'tags': ['seq:step:1', 'langsmith:hidden', 'langsmith:hidden'], 'run_id': 'a43d3bf9-9f5f-4c6d-8f5b-839bf690cb91', 'metadata': {'thread_id': '5', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:6df9b3fd-426d-a817-b365-da3423177628'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '983889a7-ca3a-47c5-bb09-67339220dfee']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}, 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'run_id': 'a43d3bf9-9f5f-4c6d-8f5b-839bf690cb91', 'name': '_write', 'tags': ['seq:step:1', 'langsmith:hidden', 'langsmith:hidden'], 'metadata': {'thread_id': '5', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:6df9b3fd-426d-a817-b365-da3423177628'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '983889a7-ca3a-47c5-bb09-67339220dfee']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'name': '_write', 'tags': ['seq:step:3', 'langsmith:hidden', 'langsmith:hidden'], 'run_id': '240b22ae-0f65-49bd-b93f-74713ccb190b', 'metadata': {'thread_id': '5', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:6df9b3fd-426d-a817-b365-da3423177628'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '983889a7-ca3a-47c5-bb09-67339220dfee']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}, 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'run_id': '240b22ae-0f65-49bd-b93f-74713ccb190b', 'name': '_write', 'tags': ['seq:step:3', 'langsmith:hidden', 'langsmith:hidden'], 'metadata': {'thread_id': '5', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:6df9b3fd-426d-a817-b365-da3423177628'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '983889a7-ca3a-47c5-bb09-67339220dfee']}\n{'event': 'on_chain_stream', 'run_id': '983889a7-ca3a-47c5-bb09-67339220dfee', 'name': '__start__', 'tags': ['graph:step:0', 'langsmith:hidden'], 'metadata': {'thread_id': '5', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:6df9b3fd-426d-a817-b365-da3423177628'}, 'data': {'chunk': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}, 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={})]}}, 'run_id': '983889a7-ca3a-47c5-bb09-67339220dfee', 'name': '__start__', 'tags': ['graph:step:0', 'langsmith:hidden'], 'metadata': {'thread_id': '5', 'langgraph_step': 0, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:6df9b3fd-426d-a817-b365-da3423177628'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825')]}}, 'name': 'assistant', 'tags': ['graph:step:1'], 'run_id': '1f41f0f9-d14a-4563-895b-f525f385255b', 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='\\n  Resuelve la ecuaci\u00f3n aplicando las siguientes reglas:\\n  1. Eval\u00faa las operaciones dentro de par\u00e9ntesis primero.\\n  2. Luego, eval\u00faa multiplicaciones y divisiones de izquierda a derecha.\\n  3. Finalmente, eval\u00faa sumas y restas de izquierda a derecha.\\n  4. Usa las herramientas proporcionadas: sumar, restar, multiplicar, dividir.\\n  ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825')]]}}, 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'sumar', 'args': '', 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}], tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'a', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': 'a', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'a', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '3', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': '3', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '3', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': ',\"', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': ',\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ',\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': 'b', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': 'b', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'b', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '\":', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '2', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': '2', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '2', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '}', 'name': None}, 'type': None}]}, response_metadata={}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742')}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chat_model_end', 'data': {'output': AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), 'input': {'messages': [[HumanMessage(content='\\n  Resuelve la ecuaci\u00f3n aplicando las siguientes reglas:\\n  1. Eval\u00faa las operaciones dentro de par\u00e9ntesis primero.\\n  2. Luego, eval\u00faa multiplicaciones y divisiones de izquierda a derecha.\\n  3. Finalmente, eval\u00faa sumas y restas de izquierda a derecha.\\n  4. Usa las herramientas proporcionadas: sumar, restar, multiplicar, dividir.\\n  ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825')]]}}, 'run_id': '683ffa42-bf54-4c94-ab20-02074c2b2742', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}, 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'run_id': '3cc6f127-fbe2-46b3-bdc1-ec6ba62e1db3', 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}, 'input': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}, 'run_id': '3cc6f127-fbe2-46b3-bdc1-ec6ba62e1db3', 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}, 'name': 'tools_condition', 'tags': ['seq:step:4'], 'run_id': 'd5595dbf-ca33-4c79-a9b0-0bab58b24309', 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chain_end', 'data': {'output': 'tools', 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}, 'run_id': 'd5595dbf-ca33-4c79-a9b0-0bab58b24309', 'name': 'tools_condition', 'tags': ['seq:step:4'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', '1f41f0f9-d14a-4563-895b-f525f385255b']}\n{'event': 'on_chain_stream', 'run_id': '1f41f0f9-d14a-4563-895b-f525f385255b', 'name': 'assistant', 'tags': ['graph:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962'}, 'data': {'chunk': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}, 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825')]}}, 'run_id': '1f41f0f9-d14a-4563-895b-f525f385255b', 'name': 'assistant', 'tags': ['graph:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:0e210a7f-649d-0ecb-f539-bc2d3f479962'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_stream', 'run_id': 'e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '5'}, 'data': {'chunk': {'assistant': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}}, 'parent_ids': []}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}, 'name': 'tools', 'tags': ['graph:step:2'], 'run_id': 'f5703885-cb8c-48f6-ac33-76855b10a7a1', 'metadata': {'thread_id': '5', 'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_tool_start', 'data': {'input': {'a': 3, 'b': 2}}, 'name': 'sumar', 'tags': ['seq:step:1'], 'run_id': 'bdbf1166-b132-40fc-8844-1252dc7f5510', 'metadata': {'thread_id': '5', 'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304', 'checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'f5703885-cb8c-48f6-ac33-76855b10a7a1']}\n{'event': 'on_tool_end', 'data': {'output': ToolMessage(content='5', name='sumar', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N'), 'input': {'a': 3, 'b': 2}}, 'run_id': 'bdbf1166-b132-40fc-8844-1252dc7f5510', 'name': 'sumar', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304', 'checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'f5703885-cb8c-48f6-ac33-76855b10a7a1']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [ToolMessage(content='5', name='sumar', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}}, 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'run_id': '76877fe7-cb89-4c03-b2b1-b3e9e280c379', 'metadata': {'thread_id': '5', 'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'f5703885-cb8c-48f6-ac33-76855b10a7a1']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [ToolMessage(content='5', name='sumar', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}, 'input': {'messages': [ToolMessage(content='5', name='sumar', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}}, 'run_id': '76877fe7-cb89-4c03-b2b1-b3e9e280c379', 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'thread_id': '5', 'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'f5703885-cb8c-48f6-ac33-76855b10a7a1']}\n{'event': 'on_chain_stream', 'run_id': 'f5703885-cb8c-48f6-ac33-76855b10a7a1', 'name': 'tools', 'tags': ['graph:step:2'], 'metadata': {'thread_id': '5', 'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304'}, 'data': {'chunk': {'messages': [ToolMessage(content='5', name='sumar', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [ToolMessage(content='5', name='sumar', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}, 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}])]}}, 'run_id': 'f5703885-cb8c-48f6-ac33-76855b10a7a1', 'name': 'tools', 'tags': ['graph:step:2'], 'metadata': {'thread_id': '5', 'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:69de5831-7838-8d27-6c04-b9760fd3e304'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_stream', 'run_id': 'e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '5'}, 'data': {'chunk': {'tools': {'messages': [ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}}}, 'parent_ids': []}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}}, 'name': 'assistant', 'tags': ['graph:step:3'], 'run_id': 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593', 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='\\n  Resuelve la ecuaci\u00f3n aplicando las siguientes reglas:\\n  1. Eval\u00faa las operaciones dentro de par\u00e9ntesis primero.\\n  2. Luego, eval\u00faa multiplicaciones y divisiones de izquierda a derecha.\\n  3. Finalmente, eval\u00faa sumas y restas de izquierda a derecha.\\n  4. Usa las herramientas proporcionadas: sumar, restar, multiplicar, dividir.\\n  ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]]}}, 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='La', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' soluci\u00f3n', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' de', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' la', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' ecu', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='aci\u00f3n', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' \\\\(', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='3', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' +', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='2', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\\\', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' es', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' \\\\(', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='5', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\\\', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=').', additional_kwargs={}, response_metadata={}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chat_model_end', 'data': {'output': AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f'), 'input': {'messages': [[HumanMessage(content='\\n  Resuelve la ecuaci\u00f3n aplicando las siguientes reglas:\\n  1. Eval\u00faa las operaciones dentro de par\u00e9ntesis primero.\\n  2. Luego, eval\u00faa multiplicaciones y divisiones de izquierda a derecha.\\n  3. Finalmente, eval\u00faa sumas y restas de izquierda a derecha.\\n  4. Usa las herramientas proporcionadas: sumar, restar, multiplicar, dividir.\\n  ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]]}}, 'run_id': '181383ba-5725-4032-a38c-566544bdfb3f', 'name': 'ChatOpenAI', 'tags': ['seq:step:1'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': None}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}}, 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'run_id': '68069bcd-789b-4bb7-9d9d-a1d26ea92d68', 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}, 'input': {'messages': [AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}}, 'run_id': '68069bcd-789b-4bb7-9d9d-a1d26ea92d68', 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N'), AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}}, 'name': 'tools_condition', 'tags': ['seq:step:4'], 'run_id': 'fedc83ad-8c10-4114-9841-0c7702d68c06', 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chain_end', 'data': {'output': '__end__', 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N'), AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}}, 'run_id': 'fedc83ad-8c10-4114-9841-0c7702d68c06', 'name': 'tools_condition', 'tags': ['seq:step:4'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593']}\n{'event': 'on_chain_stream', 'run_id': 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593', 'name': 'assistant', 'tags': ['graph:step:3'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a'}, 'data': {'chunk': {'messages': [AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}, 'input': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N')]}}, 'run_id': 'c5d7ff7e-92c9-4e2a-a13a-7d54665d2593', 'name': 'assistant', 'tags': ['graph:step:3'], 'metadata': {'thread_id': '5', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['tools'], 'langgraph_path': ('__pregel_pull', 'assistant'), 'langgraph_checkpoint_ns': 'assistant:f500bf2c-1e64-bbf4-69d5-1d004e945c1a'}, 'parent_ids': ['e30d2fbb-d70f-41cb-a5fc-3248a9991118']}\n{'event': 'on_chain_stream', 'run_id': 'e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '5'}, 'data': {'chunk': {'assistant': {'messages': [AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}}}, 'parent_ids': []}\n{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='Me puedes resolver esta ecuaci\u00f3n: 3+2?', additional_kwargs={}, response_metadata={}, id='da59875b-a82e-47f8-b9ba-0bc091583825'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'sumar'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-683ffa42-bf54-4c94-ab20-02074c2b2742', tool_calls=[{'name': 'sumar', 'args': {'a': 3, 'b': 2}, 'id': 'call_L4kVl8jzeGdxBc763RLp934N', 'type': 'tool_call'}]), ToolMessage(content='5', name='sumar', id='1b626575-b877-4adb-ae08-1e38cb1b76ac', tool_call_id='call_L4kVl8jzeGdxBc763RLp934N'), AIMessage(content='La soluci\u00f3n de la ecuaci\u00f3n \\\\(3 + 2\\\\) es \\\\(5\\\\).', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c'}, id='run-181383ba-5725-4032-a38c-566544bdfb3f')]}}, 'run_id': 'e30d2fbb-d70f-41cb-a5fc-3248a9991118', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '5'}, 'parent_ids': []}\n</code></pre></p>"},{"location":"curso2/tema2_streaming/#ejemplo2-capturando-eventos-en-streaming","title":"\ud83d\udee0\ufe0f Ejemplo2: Capturando Eventos en Streaming","text":"<p>Usaremos el siguiente ejemplo para observar c\u00f3mo los eventos nos permiten monitorear el flujo del grafo:  </p> <pre><code>config = {\"configurable\": {\"thread_id\": \"5\"}}\nasync for event in calculadora.astream_events({\"messages\": [HumanMessage(content=\"Me puedes resolver esta ecuaci\u00f3n: 3+2?\")]}, config, version=\"v2\"):\n    # Get chat model tokens from a particular node \n    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == \"assistant\":\n        data = event[\"data\"]\n        print(data[\"chunk\"].content, end=\"|\")\n</code></pre> Resultado<pre><code>||||||||||||La| soluci\u00f3n| de| la| ecu|aci\u00f3n| \\(|3| +| |2|\\|)| es| \\(|5|\\|).||\n</code></pre>"},{"location":"curso2/tema2_streaming/#conclusion","title":"\ud83d\udccb Conclusi\u00f3n","text":"<p>El streaming en LangGraph transforma la forma en que manejamos los flujos, proporcionando: - Resultados en Tiempo Real: Respuestas inmediatas y progresivas. - Control y Flexibilidad: Capacidad para monitorear y reaccionar a eventos en tiempo real. - Mejor Experiencia de Usuario: Interacciones m\u00e1s fluidas y din\u00e1micas.  </p>"},{"location":"curso2/tema2_streaming/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> M\u00e1s ejemplos: Google Colab</li> <li> Definici\u00f3n del concepto: Streaming</li> <li> How-to-guide: Streaming</li> </ul>"},{"location":"curso2/tema2_streaming/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Streaming Graph Outputs: C\u00f3mo emitir resultados progresivamente usando <code>stream_mode='updates'</code> y <code>stream_mode='values'</code>.  </li> <li>Eventos en Streaming: C\u00f3mo capturar y manejar eventos durante la ejecuci\u00f3n del grafo con <code>aStream_events</code>.  </li> </ul>"},{"location":"curso2/tema2_streaming/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos los Breakpoints, que nos permiten pausar el flujo del grafo en puntos espec\u00edficos para validar o intervenir en tiempo real.  </p>"},{"location":"curso2/tema3_breakpoint/","title":"\u23f8\ufe0f Tema 3: Breakpoints \u2013 Control Total del Flujo","text":""},{"location":"curso2/tema3_breakpoint/#que-son-los-breakpoints-en-langgraph","title":"\ud83d\ude80 \u00bfQu\u00e9 son los Breakpoints en LangGraph?","text":"<p>Los breakpoints en LangGraph son puntos de control que permiten pausar el flujo de ejecuci\u00f3n de un grafo en momentos espec\u00edficos. Esto brinda a los desarrolladores y sistemas: - Flexibilidad: Intervenir manualmente para validar, modificar o analizar datos. - Control Din\u00e1mico: Decidir si continuar, detener o reiniciar el flujo.  </p>"},{"location":"curso2/tema3_breakpoint/#por-que-usar-breakpoints","title":"\ud83e\udde0 \u00bfPor Qu\u00e9 Usar Breakpoints?","text":"<ol> <li>Validaci\u00f3n en Tiempo Real: Revisa los datos generados por el grafo antes de seguir con el flujo.  </li> <li>Intervenci\u00f3n Manual: Permite la participaci\u00f3n humana en puntos cr\u00edticos del grafo.  </li> <li>Depuraci\u00f3n Avanzada: Identifica y corrige errores durante la ejecuci\u00f3n.  </li> </ol>"},{"location":"curso2/tema3_breakpoint/#como-funcionan-los-breakpoints","title":"\ud83c\udf1f \u00bfC\u00f3mo Funcionan los Breakpoints?","text":"<p>Cuando un grafo alcanza un breakpoint: 1. El Flujo se Detiene: Se pausa la ejecuci\u00f3n en el nodo designado. 2. Acci\u00f3n del Usuario: Se puede realizar cualquier validaci\u00f3n o modificaci\u00f3n necesaria, que profundizaremos en el siguiente tema. 3. Reanudaci\u00f3n: Una vez que se completa la intervenci\u00f3n, el flujo puede continuar desde el punto en el que se detuvo.  </p> <p>LangGraph permite implementar breakpoints mediante funciones personalizadas y configuraciones en el grafo.  </p>"},{"location":"curso2/tema3_breakpoint/#ejemplo-practico-implementando-breakpoints","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico: Implementando Breakpoints","text":"<p>En este ejemplo pr\u00e1ctico, configuraremos un grafo con un breakpoint que detenga su ejecuci\u00f3n en un nodo espec\u00edfico. Esto nos permitir\u00e1 inspeccionar y, si es necesario, modificar el estado del grafo antes de reanudar su flujo.  </p>"},{"location":"curso2/tema3_breakpoint/#paso-1-configuracion-del-grafo-con-breakpoints","title":"Paso 1: Configuraci\u00f3n del Grafo con Breakpoints","text":"<p>Partiremos de la calculadora que ya conocemos, y a\u00f1adiremos un breakpoint para observar c\u00f3mo se detiene el flujo.  </p> <pre><code>[...]\n# Importamos el MemorySaver\nfrom langgraph.checkpoint.memory import MemorySaver\n[...]\n# Agregamos memoria a nuestro grafo.\nmemory = MemorySaver()\n# Definimos `interrupt_before` para definir nuestro breakpoint antes de entrar a\n# tools\ncalculadora = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n[...]\n</code></pre> <p></p> Atenci\u00f3n <p>Para habilitar los breakpoints en nuestro grafo, realizamos dos modificaciones clave:  </p> <ul> <li>Memoria (<code>MemorySaver</code>): Agregamos memoria al grafo para que pueda guardar su estado en el momento de la interrupci\u00f3n.  </li> <li><code>interrupt_before</code>: Definimos en qu\u00e9 punto del grafo queremos que se detenga. En este caso, configuramos el breakpoint justo antes de entrar al nodo <code>tools</code>. Alternativamente, podr\u00edamos usar (o combinar) <code>interrupt_after</code> para detener el flujo despu\u00e9s de un nodo espec\u00edfico.  </li> </ul>"},{"location":"curso2/tema3_breakpoint/#paso-2-invocamos-el-grafo-hasta-el-breakpoint","title":"Paso 2: Invocamos el Grafo Hasta el Breakpoint","text":"<p>Para ejecutar el grafo hasta el breakpoint, utilizamos el m\u00e9todo <code>stream</code>. Esto nos permite observar paso a paso c\u00f3mo se comporta el grafo y analizar los datos en tiempo real.  </p> <pre><code># Input\ninitial_input = {\"messages\": HumanMessage(content=\"Me puedes resolver esta ecuaci\u00f3n: (3+2)?\")}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in calculadora.stream(initial_input, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nMe puedes resolver esta ecuaci\u00f3n: (3+2)?\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_pgbhFPo0DX2XYdHRcTpGD25t)\n Call ID: call_pgbhFPo0DX2XYdHRcTpGD25t\n  Args:\n    a: 3\n    b: 2\n</code></pre> Nota <p>El grafo se detiene exactamente en el punto que definimos, en este caso, antes de entrar al nodo <code>tools</code>. Esto nos permite:</p> <ul> <li>Inspeccionar los datos que se pasar\u00e1n a tools.</li> <li>Realizar modificaciones o validaciones seg\u00fan sea necesario.</li> </ul> <p>Adem\u00e1s, podemos usar <code>get_state</code> para consultar el estado actual del grafo:</p> <pre><code>state = calculadora.get_state(thread)\nstate.next\n</code></pre> Resultado<pre><code>('tools',)\n</code></pre> <p>\ud83d\udd0e Aqu\u00ed vemos que el grafo est\u00e1 detenido, esperando a que el nodo tools termine su tarea antes de proceder al siguiente nodo.</p>"},{"location":"curso2/tema3_breakpoint/#paso-3-reanudar-el-flujo-desde-el-breakpoint","title":"Paso 3: Reanudar el Flujo desde el Breakpoint","text":"<p>Una vez que hemos realizado las validaciones o modificaciones necesarias, podemos reanudar el flujo del grafo desde el punto donde se detuvo.</p> Truco <p>Al invocar el grafo con <code>None</code> como entrada, LangGraph reanudar\u00e1 la ejecuci\u00f3n autom\u00e1ticamente desde el \u00faltimo punto pausado.</p> <pre><code>for event in calculadora.stream(None, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print() \n</code></pre> Resultado<pre><code>================================== Ai Message ==================================\nTool Calls:\n  sumar (call_pgbhFPo0DX2XYdHRcTpGD25t)\n Call ID: call_pgbhFPo0DX2XYdHRcTpGD25t\n  Args:\n    a: 3\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n5\n================================== Ai Message ==================================\n\nLa soluci\u00f3n de la ecuaci\u00f3n (3 + 2) es 5.\n</code></pre> <p>El grafo se reanuda desde el \u00faltimo punto pausado, completa la ejecuci\u00f3n del nodo tools, y contin\u00faa con el flujo hasta llegar al final.</p> Nota <ul> <li>Al reanudar el grafo, LangGraph emitir\u00e1 nuevamente el estado actual, incluyendo el mensaje con la llamada a la herramienta (<code>AIMessage</code>).  </li> <li>Una vez ejecutado el nodo de la herramienta, el resultado se devuelve al modelo de chat, generando la respuesta final para el usuario.</li> </ul>"},{"location":"curso2/tema3_breakpoint/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>Los breakpoints ofrecen un nivel de control avanzado sobre el flujo del grafo, permiti\u00e9ndonos:</p> <ul> <li>Detener la ejecuci\u00f3n en puntos clave.</li> <li>Inspeccionar y modificar el estado del grafo.</li> <li>Reanudar el flujo de forma din\u00e1mica y eficiente.</li> </ul> <p>Esta funcionalidad es especialmente \u00fatil en escenarios donde se requiere intervenci\u00f3n manual, depuraci\u00f3n, o validaciones intermedias.</p>"},{"location":"curso2/tema3_breakpoint/#relacion-con-el-streaming","title":"\ud83d\udd04 Relaci\u00f3n con el Streaming","text":"<p>El concepto de breakpoints complementa el streaming al proporcionar un mecanismo para interrumpir y reanudar flujos din\u00e1micos. Esto es particularmente \u00fatil en flujos en tiempo real, donde la interacci\u00f3n humana (<code>Human in the loop</code> que veremos en el siguiente tema) o la validaci\u00f3n intermedia son cr\u00edticas para el \u00e9xito del sistema.  </p>"},{"location":"curso2/tema3_breakpoint/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> M\u00e1s ejemplos: Google Colab</li> <li> Definici\u00f3n del concepto: Breakpoints</li> <li> How-to-guide: Breakpoints</li> </ul>"},{"location":"curso2/tema3_breakpoint/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Breakpoints: Puntos de pausa en el grafo para validar, modificar o intervenir en el flujo.  </li> <li>Control del Flujo: C\u00f3mo detener y reanudar un grafo de manera din\u00e1mica.  </li> <li>Relaci\u00f3n con Streaming: C\u00f3mo integrar breakpoints con flujos en tiempo real para mayor flexibilidad.  </li> </ul>"},{"location":"curso2/tema3_breakpoint/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Human in the Loop, donde combinaremos todo lo aprendido para integrar intervenci\u00f3n humana en flujos din\u00e1micos y en tiempo real.  </p>"},{"location":"curso2/tema4_human_loop/","title":"\ud83d\udc69\u200d\ud83d\udcbb Tema 4: Human in the Loop \u2013 Integraci\u00f3n Humana en el Grafo","text":""},{"location":"curso2/tema4_human_loop/#que-es-human-in-the-loop","title":"\ud83d\ude80 \u00bfQu\u00e9 es Human in the Loop?","text":"<p>Human in the Loop (HITL) es un enfoque que permite combinar la automatizaci\u00f3n con la intervenci\u00f3n humana en flujos de grafos. Con LangGraph, es posible integrar HITL para: - Revisar decisiones generadas autom\u00e1ticamente por el grafo. - Incorporar intervenci\u00f3n humana en momentos cr\u00edticos. - Proveer mayor control en aplicaciones sensibles o complejas.  </p> <p>Es ideal para garantizar que las decisiones sean precisas, adaptadas al contexto y cumplan con los est\u00e1ndares esperados.  </p>"},{"location":"curso2/tema4_human_loop/#por-que-es-importante-human-in-the-loop","title":"\ud83e\udde0 \u00bfPor Qu\u00e9 es Importante Human in the Loop?","text":"<ol> <li>Validaci\u00f3n de Resultados: Permite revisar las decisiones del grafo antes de ejecutarlas.  </li> <li>Intervenci\u00f3n Cr\u00edtica: Proporciona la flexibilidad necesaria para abordar casos complejos o sensibles.  </li> <li>Control Mejorado: Combina lo mejor de los sistemas automatizados con la supervisi\u00f3n humana.  </li> </ol> <p>Este enfoque crea sistemas m\u00e1s confiables, eficaces y personalizables.  </p>"},{"location":"curso2/tema4_human_loop/#como-funciona-human-in-the-loop-en-langgraph","title":"\ud83c\udf1f \u00bfC\u00f3mo Funciona Human in the Loop en LangGraph?","text":"<p>LangGraph introduce HITL mediante breakpoints, eventos y l\u00f3gica personalizada en los nodos. El flujo se detiene en puntos clave del grafo para permitir: 1. Revisi\u00f3n de datos por parte del usuario. 2. Modificaci\u00f3n de par\u00e1metros o validaciones. 3. Reanudaci\u00f3n del flujo con los cambios realizados.  </p>"},{"location":"curso2/tema4_human_loop/#ejemplo-practico-incorporando-human-in-the-loop","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico: Incorporando Human in the Loop","text":"<p>En este ejemplo, configuramos un grafo que incluye un nodo de intervenci\u00f3n humana para validar una acci\u00f3n antes de continuar.  </p>"},{"location":"curso2/tema4_human_loop/#paso-1-configuracion-del-grafo-con-intervencion-humana","title":"Paso 1: Configuraci\u00f3n del Grafo con Intervenci\u00f3n Humana","text":"<p>El primer paso es definir un grafo con nodos que permitan: - Procesar la entrada inicial. - Detenerse en un punto cr\u00edtico para solicitar intervenci\u00f3n humana. - Continuar el flujo seg\u00fan la respuesta recibida.  </p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import Command, interrupt\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    input: str\n    user_feedback: str\n    respuesta: str\n\n# Nodo de procesamiento inicial\ndef initial_node(state):\n    print(\"---Nodo inicial: procesando datos.---\")\n    return state\n\n# Nodo de intervenci\u00f3n humana\ndef human_intervention(state):\n    print(\"---\u23f8\ufe0f Intervenci\u00f3n humana requerida.---\")\n    feedback = interrupt(\"Deseas validar esta acci\u00f3n?: (SI/NO)\")\n    return {\"user_feedback\": feedback}  # Aqu\u00ed el humano puede validar o modificar el estado\n\n# Nodo de continuaci\u00f3n\ndef final_node(state):\n    print(\"---\u25b6\ufe0f Nodo final: completando el flujo.---\")\n    if state[\"user_feedback\"] == \"SI\":\n        state[\"respuesta\"] = \"El usuario ha validado la acci\u00f3n\"\n    else:\n        state[\"respuesta\"] = \"El usuario NO ha validado la acci\u00f3n\"\n    return state\n\n# Configuraci\u00f3n del grafo\nbuilder = StateGraph(State)\nbuilder.add_node(\"start\", initial_node)\nbuilder.add_node(\"intervention\", human_intervention)\nbuilder.add_node(\"final_node\", final_node)\n\nbuilder.add_edge(START, \"start\")\nbuilder.add_edge(\"start\", \"intervention\")\nbuilder.add_edge(\"intervention\", \"final_node\")\nbuilder.add_edge(\"final_node\", END)\n\nmemory = MemorySaver()\n# Compilar el grafo\ngraph_with_hitl = builder.compile(checkpointer=memory)\n\nfrom IPython.display import Image, display\ndisplay(Image(graph_with_hitl.get_graph(xray=True).draw_mermaid_png()))\n</code></pre> <p></p> Nota <p>En este ejemplo: - Usamos el m\u00e9todo <code>interrupt</code> para pausar el flujo en el nodo de intervenci\u00f3n. - Integramos una memoria persistente (<code>MemorySaver</code>) para garantizar que el grafo recuerde su estado durante las interrupciones.  </p>"},{"location":"curso2/tema4_human_loop/#paso-2-pausando-el-grafo-para-la-intervencion","title":"Paso 2: Pausando el Grafo para la Intervenci\u00f3n","text":"<p>Cuando ejecutamos el grafo, este se detiene en el nodo de intervenci\u00f3n humana, enviando un mensaje al usuario para validar la acci\u00f3n.   </p> <pre><code>initial_input = {\"input\": \"Felicitar a Raul por el curso.\"}\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\nevents = []\n\n# Ejecutamos el grafo hasta el punto de intervenci\u00f3n\nfor event in graph_with_hitl.stream(initial_input, thread, stream_mode=\"updates\"):\n    print(event)\n    events.append(event)\n    print(\"\\n\")\n</code></pre> Resultado<pre><code>---Nodo inicial: procesando datos.---\n{'start': {'input': 'Felicitar a Raul por el curso.'}}\n\n---\u23f8\ufe0f Intervenci\u00f3n humana requerida.---\n{'__interrupt__': (Interrupt(value='Deseas validar esta acci\u00f3n?: (SI/NO)', resumable=True, ns=['intervention:81d7c2a0-7cc5-cdd4-a7d9-aeedd3aa2944'], when='during'),)}\n</code></pre> <p>El grafo responde con un objeto <code>Interrupt</code>, que contiene el mensaje para el usuario. Podemos mostrar este mensaje usando el siguiente fragmento:  </p> <pre><code># Mostramos el mensaje de nuestro Interrupt.\nfor event in events:\n    if '__interrupt__' in event:  # Verificamos si el evento tiene un '__interrupt__'\n        interrupt_obj = event['__interrupt__'][0]  # Obtenemos el objeto Interrupt           \n        # Formateamos y mostramos la informaci\u00f3n\n        formatted_output = f\"Interrupt Message: {interrupt_obj.value}\"\n        print(formatted_output.strip())\n</code></pre> Resultado<pre><code>Interrupt Message: Deseas validar esta acci\u00f3n?: (SI/NO)\n</code></pre> Nota <p>Este punto es esencial para integrar la intervenci\u00f3n humana en tu aplicaci\u00f3n, ya que el mensaje de <code>Interrupt</code> es lo que el sistema env\u00eda al usuario para interactuar.  </p>"},{"location":"curso2/tema4_human_loop/#paso-3-comprobando-el-estado-actual","title":"Paso 3: Comprobando el estado actual","text":"<p>Para asegurarnos de que el grafo est\u00e1 detenido correctamente en el nodo esperado, podemos consultar su estado con <code>get_state</code>:  </p> <pre><code># Comprobamos que realmente esta pendiente de intervenci\u00f3n.\nstate = graph_with_hitl.get_state(thread)\nstate.next\n</code></pre> Resultado<pre><code>('intervention',)\n</code></pre> <p>Esto confirmar\u00e1 que el grafo est\u00e1 bloqueado en el nodo <code>intervention</code>, esperando la respuesta del usuario.  </p>"},{"location":"curso2/tema4_human_loop/#paso-4-reanudando-el-flujo-tras-la-intervencion","title":"Paso 4: Reanudando el Flujo Tras la Intervenci\u00f3n","text":"<p>Despu\u00e9s de que el usuario responde a la consulta, reanudamos el flujo desde el punto donde se detuvo.  </p> <pre><code># Continuamos con la ejecuci\u00f3n del grafo agregando nuestra respuesta.\n# Podeis probar respondiendo un \"NO\" para ver el caso contrario.\n#for event in graph_with_hitl.stream(Command(resume=\"NO\"), thread, stream_mode=\"updates\"):\nfor event in graph_with_hitl.stream(Command(resume=\"SI\"), thread, stream_mode=\"updates\"):\n    print(event)\n    events.append(event)\n    print(\"\\n\")\n</code></pre> Resultado<pre><code>---\u23f8\ufe0f Intervenci\u00f3n humana requerida.---\n{'intervention': {'user_feedback': 'SI'}}\n\n\n---\u25b6\ufe0f Nodo final: completando el flujo.---\n{'final_node': {'input': '\u00bfEsta decisi\u00f3n es correcta?', 'user_feedback': 'SI', 'respuesta': 'El usuario ha validado la acci\u00f3n'}}\n</code></pre> <p>\ud83d\udca1 Truco: Usamos el objeto <code>Command</code> para enviar la respuesta del usuario al grafo, lo que permite continuar el flujo.  </p> Nota <p>La integraci\u00f3n de HITL en una aplicaci\u00f3n requiere considerar dos puntos clave: 1. Inicio del Flujo: Inicia el grafo con la entrada del usuario. 2. Continuaci\u00f3n del Flujo: Reanuda el grafo desde donde se detuvo, enviando la respuesta del usuario.  </p> <p>Esto implica que tu aplicaci\u00f3n debe tener dos endpoints para interactuar con el grafo: - Endpoint Inicial: Para iniciar el flujo (por ejemplo, <code>miapp.com/start</code>). - Endpoint Continuaci\u00f3n: Para reanudar el flujo tras la intervenci\u00f3n (por ejemplo, <code>miapp.com/continue</code>).  </p>"},{"location":"curso2/tema4_human_loop/#casos-de-uso-reales","title":"\ud83c\udfaf Casos de Uso Reales","text":"<ul> <li>Revisi\u00f3n de Decisiones Automatizadas: Garantizar que las decisiones cumplen con est\u00e1ndares antes de ejecutarlas.  </li> <li>Procesos Empresariales Sensibles: Validaci\u00f3n humana en flujos que manejan datos financieros o legales.  </li> <li>Personalizaci\u00f3n: Incorporar preferencia o experiencia humana para mejorar la experiencia del usuario.  </li> </ul>"},{"location":"curso2/tema4_human_loop/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Human in the Loop: C\u00f3mo integrar intervenci\u00f3n humana en un grafo automatizado.  </li> <li>Intervenci\u00f3n Din\u00e1mica: Detener el flujo, solicitar validaciones y reanudar seg\u00fan las respuestas recibidas.  </li> <li>Aplicaci\u00f3n Pr\u00e1ctica: Implementar control humano en flujos cr\u00edticos y mejorar la confiabilidad de los sistemas automatizados.  </li> </ul>"},{"location":"curso2/tema4_human_loop/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>Human in the Loop (HITL) es una funcionalidad esencial para equilibrar la automatizaci\u00f3n y el control humano en flujos de trabajo complejos. A lo largo de este tema, hemos visto c\u00f3mo LangGraph permite integrar puntos de intervenci\u00f3n humana para garantizar: - Decisiones Validas: Validaci\u00f3n de resultados automatizados en tiempo real. - Flexibilidad Din\u00e1mica: Capacidad de detener, modificar y reanudar flujos seg\u00fan las necesidades del usuario. - Control Personalizado: Mayor confiabilidad en procesos cr\u00edticos o sensibles.  </p> <p>HITL no solo mejora la precisi\u00f3n de los sistemas automatizados, sino que tambi\u00e9n los hace m\u00e1s adaptables y orientados al usuario, permitiendo una colaboraci\u00f3n m\u00e1s eficiente entre humanos y tecnolog\u00eda.  </p>"},{"location":"curso2/tema4_human_loop/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Ejemplo avanzado para pedir permiso al usuario de uso de herramientas: Google Colab</li> <li> Definici\u00f3n del concepto: Human-in-the-loop</li> <li> How-to-guide: Humman-approval</li> </ul>"},{"location":"curso2/tema4_human_loop/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Paralelismo, una t\u00e9cnica clave para optimizar flujos mediante la ejecuci\u00f3n simult\u00e1nea de nodos. Veremos c\u00f3mo dividir tareas en paralelo, consolidar resultados y mejorar la eficiencia en grafos complejos.  </p>"},{"location":"curso2/tema5_parallelism/","title":"\ud83d\ude80 Tema 5: Paralelismo \u2013 Ejecutando Nodos Simult\u00e1neamente","text":""},{"location":"curso2/tema5_parallelism/#que-es-el-paralelismo-en-langgraph","title":"\ud83c\udf1f \u00bfQu\u00e9 es el Paralelismo en LangGraph?","text":"<p>El paralelismo en LangGraph permite que m\u00faltiples nodos se ejecuten simult\u00e1neamente dentro del mismo grafo. Esto es especialmente \u00fatil en situaciones donde:  </p> <ul> <li>Existen tareas independientes que pueden ejecutarse en paralelo.  </li> <li>Se busca optimizar el rendimiento y reducir el tiempo de ejecuci\u00f3n.  </li> <li>Queremos aprovechar mejor los recursos computacionales sin sacrificar la estructura del flujo.  </li> </ul> <p>Al permitir que varias operaciones se realicen al mismo tiempo, el paralelismo convierte un grafo en una herramienta m\u00e1s r\u00e1pida y eficiente.  </p>"},{"location":"curso2/tema5_parallelism/#por-que-es-importante-el-paralelismo","title":"\ud83e\udde0 \u00bfPor Qu\u00e9 es Importante el Paralelismo?","text":"<ol> <li>Eficiencia: Reduce el tiempo total de ejecuci\u00f3n al permitir m\u00faltiples procesos concurrentes.  </li> <li>Escalabilidad: Facilita la ejecuci\u00f3n de tareas en paralelo, lo que es clave en flujos de trabajo complejos.  </li> <li>Modularidad: Permite separar diferentes procesos sin afectar la l\u00f3gica general del grafo.  </li> </ol> <p>Al aprovechar estas ventajas, podemos dise\u00f1ar sistemas m\u00e1s flexibles y de alto rendimiento.  </p>"},{"location":"curso2/tema5_parallelism/#como-funciona-el-paralelismo-en-langgraph","title":"\ud83c\udf1f \u00bfC\u00f3mo Funciona el Paralelismo en LangGraph?","text":"<p>LangGraph gestiona el paralelismo mediante nodos paralelos, que permiten que m\u00faltiples procesos se ejecuten simult\u00e1neamente.  </p> <ul> <li>Los subnodos se ejecutan en paralelo y devuelven resultados independientes.  </li> <li>Los resultados de los subnodos se consolidan en el estado global del grafo.  </li> </ul> <p>Para implementar esto, utilizamos el ParallelNode, una herramienta integrada en LangGraph espec\u00edficamente dise\u00f1ada para gestionar la ejecuci\u00f3n simult\u00e1nea de nodos.  </p>"},{"location":"curso2/tema5_parallelism/#ejemplo-practico-implementando-paralelismo","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico: Implementando Paralelismo","text":"<p>A continuaci\u00f3n, configuraremos un grafo con varios nodos que se ejecutar\u00e1n en paralelo.  </p>"},{"location":"curso2/tema5_parallelism/#paso-1-definicion-de-los-nodos-y-el-estado","title":"Paso 1: Definici\u00f3n de los Nodos y el Estado","text":"<p>Creamos un conjunto de nodos que realizar\u00e1n tareas simult\u00e1neamente y generar\u00e1n resultados independientes.  </p> <pre><code>from langgraph.graph import StateGraph, START, END, MessagesState\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nimport operator\n\n\nclass State(TypedDict):\n    aggregate: str\n\ndef nodo_1(state: State):\n    return {\"aggregate\": [\"---NODO_1---\"]}\n\ndef nodo_1_2(state: State):\n    return {\"aggregate\": [\"---NODO_1_2---\"]}\n\ndef nodo_2(state: State):\n    return {\"aggregate\": [\"---NODO_2---\"]}\n\ndef nodo_3(state: State):\n    return {\"aggregate\": [\"---NODO_3---\"]}\n\ndef aggregator(state: State):\n    return {\"aggregate\": [\"---AGREGADOR---\"]}\n</code></pre>"},{"location":"curso2/tema5_parallelism/#paso-2-prueba-1-intento-sin-un-agregador","title":"Paso 2: Prueba 1 - Intento sin un Agregador","text":"<p>Si intentamos ejecutar el grafo sin un reducer, se generar\u00e1 un error debido a la escritura concurrente en el mismo estado.  </p> <p><pre><code># Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"nodo_1\", nodo_1)\nparallel_builder.add_node(\"nodo_1_2\", nodo_1_2)\nparallel_builder.add_node(\"nodo_2\", nodo_2)\nparallel_builder.add_node(\"nodo_3\", nodo_3)\nparallel_builder.add_node(\"Agregador\", aggregator)\n\n# Add edges to connect nodes\nparallel_builder.add_edge(START, \"nodo_1\")\nparallel_builder.add_edge(START, \"nodo_2\")\nparallel_builder.add_edge(START, \"nodo_3\")\nparallel_builder.add_edge(\"nodo_1\", \"nodo_1_2\")\nparallel_builder.add_edge(\"nodo_1_2\", \"Agregador\")\nparallel_builder.add_edge(\"nodo_2\", \"Agregador\")\nparallel_builder.add_edge(\"nodo_3\", \"Agregador\")\nparallel_builder.add_edge(\"Agregador\", END)\nparallel_workflow = parallel_builder.compile()\n\n# Show workflow\nfrom IPython.display import Image, display\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n</code></pre> </p> <p>Invocamos nuestro grafo para ver nuestra primera prueba a ver que pasa.</p> <pre><code># Invoke\nfrom langgraph.errors import InvalidUpdateError\ntry:\n    state = parallel_workflow.invoke({\"aggregate\": []})\nexcept InvalidUpdateError as e:\n    print(f\"An error occurred: {e}\")\n</code></pre> Resultado<pre><code>An error occurred: At key 'aggregate': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE\n</code></pre> Nota <p>El error ocurre porque varios nodos intentan modificar la misma clave en el estado al mismo tiempo. LangGraph no sabe cu\u00e1l de los valores conservar, lo que provoca un conflicto.</p>"},{"location":"curso2/tema5_parallelism/#paso-3-prueba-2-anadiendo-un-reducer","title":"Paso 3: Prueba 2 - A\u00f1adiendo un Reducer","text":"<p>Para solucionar este problema, a\u00f1adimos un reducer que gestione la actualizaci\u00f3n del estado de manera estructurada.</p> <pre><code># Agregamos un reducer al state\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n\n# Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"nodo_1\", nodo_1)\nparallel_builder.add_node(\"nodo_1_2\", nodo_1_2)\nparallel_builder.add_node(\"nodo_2\", nodo_2)\nparallel_builder.add_node(\"nodo_3\", nodo_3)\nparallel_builder.add_node(\"Agregador\", aggregator)\n\n# Add edges to connect nodes\nparallel_builder.add_edge(START, \"nodo_1\")\nparallel_builder.add_edge(START, \"nodo_2\")\nparallel_builder.add_edge(START, \"nodo_3\")\nparallel_builder.add_edge(\"nodo_1\", \"nodo_1_2\")\nparallel_builder.add_edge(\"nodo_1_2\", \"Agregador\")\nparallel_builder.add_edge(\"nodo_2\", \"Agregador\")\nparallel_builder.add_edge(\"nodo_3\", \"Agregador\")\nparallel_builder.add_edge(\"Agregador\", END)\nparallel_workflow = parallel_builder.compile()\n\nstate = parallel_workflow.invoke({\"aggregate\": []})\nprint(state)\n</code></pre> Resultado<pre><code>{'aggregate': ['---NODO_1---', '---NODO_2---', '---NODO_3---', '---AGREGADOR---', '---NODO_1_2---', '---AGREGADOR---']}\n</code></pre> Nota <p>Ahora el flujo se ejecuta correctamente, pero si observamos detenidamente los resultados, podemos notar que:</p> <ul> <li>El nodo Agregador se ejecuta dos veces.</li> <li>nodo_1_2 aparece en un momento inesperado dentro de la ejecuci\u00f3n.</li> </ul> <p>Esto indica que todav\u00eda podemos mejorar el orden del flujo.</p>"},{"location":"curso2/tema5_parallelism/#paso-4-prueba-3-asegurar-que-los-nodos-terminen-antes-de-avanzar","title":"Paso 4: Prueba 3 - Asegurar que los Nodos Terminen Antes de Avanzar","text":"<p>El siguiente paso es garantizar que el nodo Agregador solo se ejecute despu\u00e9s de que todos los nodos paralelos hayan completado su trabajo.</p> <pre><code># Agregamos un reducer al state\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n\n# Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"nodo_1\", nodo_1)\nparallel_builder.add_node(\"nodo_1_2\", nodo_1_2)\nparallel_builder.add_node(\"nodo_2\", nodo_2)\nparallel_builder.add_node(\"nodo_3\", nodo_3)\nparallel_builder.add_node(\"Agregador\", aggregator)\n\n# Add edges to connect nodes\nparallel_builder.add_edge(START, \"nodo_1\")\nparallel_builder.add_edge(START, \"nodo_2\")\nparallel_builder.add_edge(START, \"nodo_3\")\nparallel_builder.add_edge(\"nodo_1\", \"nodo_1_2\")\nparallel_builder.add_edge([\"nodo_1_2\",\"nodo_2\",\"nodo_3\"], \"Agregador\")\nparallel_builder.add_edge(\"Agregador\", END)\nparallel_workflow = parallel_builder.compile()\n\nstate = parallel_workflow.invoke({\"aggregate\": []})\nprint(state)\n</code></pre> Resultado<pre><code>{'aggregate': ['---NODO_1---', '---NODO_2---', '---NODO_3---', '---NODO_1_2---', '---AGREGADOR---']}\n</code></pre> Nota <p>Hemos asegurado que el nodo <code>Agregador</code> no se ejecute hasta que todos los nodos anteriores hayan finalizado su tarea.</p> <p>Sin embargo, el orden de ejecuci\u00f3n de los nodos paralelos sigue siendo indeterminado, lo que podr\u00eda causar incoherencias en ciertos casos.</p>"},{"location":"curso2/tema5_parallelism/#paso-5-prueba-4-definir-un-orden-de-ejecucion","title":"Paso 5: Prueba 4 - Definir un Orden de Ejecuci\u00f3n","text":"<p>Dado que dentro de cada nodo no tenemos control espec\u00edfico sobre el orden de actualizaci\u00f3n del estado, LangGraph determina un orden determinista basado en la topolog\u00eda del grafo.</p> <p>Si necesitamos un control m\u00e1s preciso, podemos crear un reducer personalizado que organice las actualizaciones en un orden predefinido.</p> <pre><code># Definimos un orden manual \nORDERED_NODES = [\n    \"---NODO_1---\",\n    \"---NODO_1_2---\",\n    \"---NODO_2---\",\n    \"---NODO_3---\",\n    \"---AGREGADOR---\"\n]\n\ndef custom_order_reducer(left, right):\n    \"\"\" Combina los valores y los ordena en el orden predefinido en ORDERED_NODES \"\"\"\n    if not isinstance(left, list):\n        left = [left]\n\n    if not isinstance(right, list):\n        right = [right]\n\n    # Combinamos los valores eliminando duplicados y preservando el orden de aparici\u00f3n\n    combined = list(dict.fromkeys(left + right)) \n\n    # Ordenamos la lista seg\u00fan ORDERED_NODES\n    return sorted(combined, key=lambda x: ORDERED_NODES.index(x) if x in ORDERED_NODES else float('inf'))\n\n# Agregamos un reducer al state\nclass State(TypedDict):\n    aggregate: Annotated[list, custom_order_reducer]\n\n# Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"nodo_1\", nodo_1)\nparallel_builder.add_node(\"nodo_1_2\", nodo_1_2)\nparallel_builder.add_node(\"nodo_2\", nodo_2)\nparallel_builder.add_node(\"nodo_3\", nodo_3)\nparallel_builder.add_node(\"Agregador\", aggregator)\n\n# Add edges to connect nodes\nparallel_builder.add_edge(START, \"nodo_1\")\nparallel_builder.add_edge(START, \"nodo_2\")\nparallel_builder.add_edge(START, \"nodo_3\")\nparallel_builder.add_edge(\"nodo_1\", \"nodo_1_2\")\nparallel_builder.add_edge([\"nodo_1_2\",\"nodo_2\",\"nodo_3\"], \"Agregador\")\nparallel_builder.add_edge(\"Agregador\", END)\nparallel_workflow = parallel_builder.compile()\n\nstate = parallel_workflow.invoke({\"aggregate\": []})\nprint(state)\n</code></pre> Resultado<pre><code>{'aggregate': ['---NODO_1---', '---NODO_1_2---', '---NODO_2---', '---NODO_3---', '---AGREGADOR---']}\n</code></pre> Nota <ul> <li>Implementamos un orden manual de ejecuci\u00f3n con un reducer personalizado.</li> <li>Ahora los nodos se ejecutan en el orden deseado, asegurando la coherencia del flujo.</li> </ul>"},{"location":"curso2/tema5_parallelism/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>El paralelismo en LangGraph es una t\u00e9cnica fundamental para optimizar flujos de trabajo y reducir tiempos de ejecuci\u00f3n.</p> <ul> <li>Nos permite ejecutar m\u00faltiples tareas simult\u00e1neamente, maximizando el uso de recursos.</li> <li>A trav\u00e9s de los reducers, podemos gestionar conflictos en la actualizaci\u00f3n del estado.</li> <li>Si es necesario, podemos definir manualmente el orden de ejecuci\u00f3n para garantizar coherencia en los resultados.</li> </ul> <p>Dominar el paralelismo nos permite construir grafos m\u00e1s eficientes, escalables y adaptables a flujos de trabajo complejos.</p>"},{"location":"curso2/tema5_parallelism/#casos-de-uso-reales","title":"\ud83c\udfaf Casos de Uso Reales","text":"<ol> <li>Procesamiento de Datos en Paralelo: Dividir grandes vol\u00famenes de datos en tareas m\u00e1s peque\u00f1as y manejarlas simult\u00e1neamente.</li> <li>Consultas Concurrentes: Realizar m\u00faltiples consultas a bases de datos o APIs al mismo tiempo.</li> <li>Flujos de Trabajo Empresariales: Manejar tareas independientes en sistemas complejos, como soporte t\u00e9cnico y ventas en paralelo.</li> </ol>"},{"location":"curso2/tema5_parallelism/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Paralelismo en LangGraph: C\u00f3mo ejecutar nodos simult\u00e1neamente para optimizar flujos de trabajo.</li> <li>ParallelNode: Herramienta clave para implementar tareas en paralelo dentro de un grafo.</li> <li>Consolidaci\u00f3n de Resultados: C\u00f3mo combinar los resultados de nodos paralelos para continuar el flujo.</li> </ul>"},{"location":"curso2/tema5_parallelism/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> M\u00e1s ejemplos: Google Colab</li> <li> Definici\u00f3n del concepto: Parallelism</li> <li> How-to-guide: Parallelism</li> </ul>"},{"location":"curso2/tema5_parallelism/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Sub-Graphs, una t\u00e9cnica avanzada que permite dividir grafos grandes en secciones m\u00e1s peque\u00f1as y manejables.</p>"},{"location":"curso2/tema6_sub_graphs/","title":"\ud83d\udd17 Tema 7: Subgraphs \u2013 Modularizando Grafos en LangGraph","text":""},{"location":"curso2/tema6_sub_graphs/#que-es-un-subgraph-en-langgraph","title":"\ud83c\udf1f \u00bfQu\u00e9 es un Subgraph en LangGraph?","text":"<p>Un Subgraph en LangGraph es un grafo independiente que puede integrarse dentro de un grafo principal como si fuera un nodo m\u00e1s. Esto permite dividir flujos complejos en secciones m\u00e1s peque\u00f1as y reutilizables.  </p> <p></p> <p>Los subgrafos son \u00fatiles para: - Modularizar flujos de trabajo grandes y hacerlos m\u00e1s manejables. - Reutilizar l\u00f3gica en diferentes partes del grafo. - Mejorar la legibilidad y mantenimiento del c\u00f3digo. </p> <p>Con esta t\u00e9cnica, podemos construir flujos m\u00e1s organizados y eficientes.  </p>"},{"location":"curso2/tema6_sub_graphs/#por-que-usar-subgraphs","title":"\ud83e\udde0 \u00bfPor Qu\u00e9 Usar Subgraphs?","text":"<ol> <li>Reutilizaci\u00f3n de C\u00f3digo: Permite definir procesos independientes y reutilizarlos en distintos grafos.  </li> <li>Escalabilidad: Ayuda a estructurar flujos complejos en partes m\u00e1s manejables.  </li> <li>Mantenimiento y Legibilidad: Hace que los grafos sean m\u00e1s f\u00e1ciles de entender y modificar.  </li> </ol> <p>En proyectos grandes, dividir el flujo en subgrafos facilita la depuraci\u00f3n y la expansi\u00f3n del sistema.  </p>"},{"location":"curso2/tema6_sub_graphs/#ejemplo-practico-implementando-un-subgraph","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico: Implementando un Subgraph","text":"<p>A continuaci\u00f3n, crearemos un subgrafo que se ejecutar\u00e1 dentro de un grafo principal.  </p>"},{"location":"curso2/tema6_sub_graphs/#paso-1-creacion-del-subgraph","title":"Paso 1: Creaci\u00f3n del Subgraph","text":"<p>Definimos un subgrafo con dos nodos que trabajan con una parte espec\u00edfica del estado.  </p> <pre><code>from langgraph.graph import START, StateGraph, END\nfrom typing import TypedDict\n\n# ---- Definimos el subgraph ----\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\n\ndef subgraph_node_1(state: SubgraphState):\n    print(\"---SUBGRAPH 1---\")\n    return {\"bar\": \"bar\"}\n\n\ndef subgraph_node_2(state: SubgraphState):\n    print(\"---SUBGRAPH 2---\")\n    # Nota: este nodo est\u00e1 utilizando una clave de estado (`bar`) que solo est\u00e1 disponible dentro del subgrafo  \n    # y est\u00e1 enviando una actualizaci\u00f3n a la clave de estado compartida (`foo`). \n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n# ---- Fin Definici\u00f3n del subgraph ----\n</code></pre> Nota <ul> <li><code>SubgraphState</code> define un State especifico para el subgrafo.</li> <li><code>subgraph_node_1</code> introduce una nueva clave de estado (<code>bar</code>).  </li> <li><code>subgraph_node_2</code> usa <code>bar</code> y modifica el estado compartido con el grafo principal (<code>foo</code>).  </li> </ul> <p>Este subgrafo puede utilizarse en cualquier otro flujo sin necesidad de reescribir su l\u00f3gica.  </p>"},{"location":"curso2/tema6_sub_graphs/#paso-2-integracion-del-subgraph-en-el-grafo-principal","title":"Paso 2: Integraci\u00f3n del Subgraph en el Grafo Principal","text":"<p>Ahora que tenemos el subgrafo listo, lo a\u00f1adimos al grafo principal como si fuera un nodo m\u00e1s.  </p> <pre><code># ---- Definimos el grafo princial ----\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    print (\"---NODE 1---\")\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\n# Agregamos el subgraph compliado previamnete como si fuese un nodo.\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", END)\ngraph = builder.compile()\n\n# ---- Fin Definici\u00f3n del grafo principal ----\n</code></pre> Nota <ul> <li><code>ParentState</code> define el State principal de nuestro grafo.</li> <li><code>node_1</code> inicializa el estado antes de pasar el control al subgrafo.  </li> <li><code>subgraph</code> se integra en el flujo sin necesidad de definir nuevamente sus nodos internos.  </li> <li>LangGraph gestiona autom\u00e1ticamente la transici\u00f3n de estado entre el grafo principal y el subgrafo.  </li> </ul>"},{"location":"curso2/tema6_sub_graphs/#paso-3-visualizacion-del-grafo","title":"Paso 3: Visualizaci\u00f3n del Grafo","text":"<p>Podemos generar una visualizaci\u00f3n del grafo maestro para entender mejor la estructura de nodos y conexiones.  </p> <pre><code>from IPython.display import Image, display\ndisplay(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n</code></pre> <p></p> Nota <p>En la imagen generada, vemos c\u00f3mo el subgrafo aparece integrado dentro del grafo principal como si fuera un nodo corriente.  </p> <p>Y aqu\u00ed podemos visualizar el sub-grafo con todos sus nodos.</p> <pre><code>from IPython.display import Image, display\ndisplay(Image(subgraph.get_graph(xray=True).draw_mermaid_png()))\n</code></pre> <p></p>"},{"location":"curso2/tema6_sub_graphs/#paso-4-ejecucion-del-grafo","title":"Paso 4: Ejecuci\u00f3n del Grafo","text":"<p>Al ejecutar el grafo, podemos invocarlo de dos formas:  </p> <ol> <li> <p>Sin mostrar la estructura del subgraph: </p> <ul> <li>Se muestra el flujo general, pero sin detallar la ejecuci\u00f3n interna del subgrafo.  </li> </ul> <pre><code>for chunk in graph.stream({\"foo\": \"foo\"}):\n     print(chunk)\n</code></pre> Resultado<pre><code>---NODE 1---\n{'node_1': {'foo': 'hi! foo'}}\n---SUBGRAPH 1---\n---SUBGRAPH 2---\n{'node_2': {'foo': 'hi! foobar'}}\n</code></pre> </li> <li> <p>Mostrando la estructura del subgraph: </p> <ul> <li>Se pueden ver los nodos internos del subgrafo a medida que se ejecutan.  </li> </ul> <pre><code>for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n     print(chunk)\n</code></pre> Resultado<pre><code>---NODE 1---\n((), {'node_1': {'foo': 'hi! foo'}})\n---SUBGRAPH 1---\n---SUBGRAPH 2---\n(('node_2:24a9cba2-16e4-3b12-bfd7-9eaedb63e5fd',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:24a9cba2-16e4-3b12-bfd7-9eaedb63e5fd',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\n</code></pre> </li> </ol> Nota <p>Si activamos la opci\u00f3n <code>subgraphs=True</code>, podemos inspeccionar la ejecuci\u00f3n del subgrafo como una parte separada dentro del flujo.  </p>"},{"location":"curso2/tema6_sub_graphs/#otras-formas-de-usar-un-subgraph","title":"\ud83d\udd04 Otras Formas de Usar un Subgraph","text":"<p>En el ejemplo anterior, vimos c\u00f3mo integrar un subgrafo dentro de un grafo principal agreg\u00e1ndolo como un nodo. Esta es la forma m\u00e1s sencilla de utilizar subgrafos, pero en flujos m\u00e1s complejos, puede ser necesario invocarlos manualmente dentro de un nodo en lugar de tratarlos como un nodo independiente.  </p> <p>LangGraph permite esta opci\u00f3n, ya que al compilar un subgrafo, este se convierte en un objeto invocable. Veamos c\u00f3mo podemos hacerlo:  </p> <pre><code>def node_2(state: ParentState):\n    # transform the state to the subgraph state\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n    # transform response back to the parent state\n    return {\"foo\": response[\"bar\"]}\n</code></pre> <p>\ud83d\udca1 Explicaci\u00f3n:</p> <ul> <li>Creamos un nodo que invoca manualmente al subgrafo.</li> <li>Pasamos los datos en el formato que requiere el subgrafo (bar).</li> <li>Recibimos la respuesta y la adaptamos al estado del grafo principal (foo).</li> </ul> Nota <p>Este enfoque nos permite ejecutar subgrafos de manera m\u00e1s flexible, integr\u00e1ndolos en flujos m\u00e1s din\u00e1micos sin necesidad de tratarlos siempre como nodos individuales.</p>"},{"location":"curso2/tema6_sub_graphs/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>Los subgraphs nos permiten dividir flujos complejos en m\u00f3dulos reutilizables, lo que mejora la escalabilidad, legibilidad y mantenimiento del c\u00f3digo.  </p> <ul> <li>Facilitan la modularizaci\u00f3n al permitir la reutilizaci\u00f3n de l\u00f3gica en distintos grafos.  </li> <li>Mejoran la organizaci\u00f3n del c\u00f3digo, evitando estructuras monol\u00edticas dif\u00edciles de mantener.  </li> <li>Nos permiten inspeccionar su ejecuci\u00f3n de manera independiente si es necesario.  </li> </ul> <p>En definitiva, los subgrafos son una herramienta clave para el dise\u00f1o de grafos escalables y eficientes en LangGraph.  </p>"},{"location":"curso2/tema6_sub_graphs/#casos-de-uso-reales","title":"\ud83c\udfaf Casos de Uso Reales","text":"<ol> <li>Modularizaci\u00f3n de Procesos: Separar distintas partes de un flujo complejo en m\u00f3dulos reutilizables.  </li> <li>Gesti\u00f3n de Tareas Repetitivas: Reutilizar subgrafos en distintos puntos del flujo sin duplicar c\u00f3digo.  </li> <li>Depuraci\u00f3n y Optimizaci\u00f3n: Inspeccionar la ejecuci\u00f3n de secciones espec\u00edficas del grafo sin afectar el resto.  </li> </ol>"},{"location":"curso2/tema6_sub_graphs/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>C\u00f3mo definir un subgrafo en LangGraph.  </li> <li>C\u00f3mo integrarlo dentro de un grafo principal como si fuera un nodo.  </li> <li>C\u00f3mo visualizar y ejecutar grafos con subgrafos de manera flexible.  </li> </ul>"},{"location":"curso2/tema6_sub_graphs/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Definici\u00f3n del concepto: Subgraphs</li> <li> How-to-guide: Subgraphs</li> </ul>"},{"location":"curso2/tema6_sub_graphs/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Mapas, una t\u00e9cnica avanzada que permite distribuir informaci\u00f3n entre m\u00faltiples nodos de manera eficiente.  </p>"},{"location":"curso2/tema7_maps/","title":"Tema 7: Map-Reduce \u2013 Distribuyendo y Procesando Datos en LangGraph","text":""},{"location":"curso2/tema7_maps/#que-es-el-patron-map-reduce-en-langgraph","title":"\ud83c\udf1f \u00bfQu\u00e9 es el Patr\u00f3n Map-Reduce en LangGraph?","text":"<p>La t\u00e9cnica Map-Reduce es una t\u00e9cnica que permite dividir una tarea en m\u00faltiples subtareas independientes (Map) y luego consolidar sus resultados en un solo valor final (Reduce).  </p> <p>En LangGraph, este patr\u00f3n es especialmente \u00fatil cuando: - Necesitamos procesar grandes vol\u00famenes de datos en paralelo. - Queremos dividir una tarea compleja en partes m\u00e1s peque\u00f1as y manejables. - Buscamos optimizar el rendimiento evitando bloqueos en el flujo de ejecuci\u00f3n.  </p>"},{"location":"curso2/tema7_maps/#por-que-usar-map-reduce","title":"\ud83e\udde0 \u00bfPor Qu\u00e9 Usar Map-Reduce?","text":"<ol> <li>Eficiencia: Permite distribuir la carga de trabajo entre varios nodos en paralelo.  </li> <li>Escalabilidad: Facilita el manejo de tareas que crecen en complejidad y volumen de datos.  </li> <li>Flexibilidad: Puede aplicarse a una gran variedad de problemas, desde el procesamiento de texto hasta an\u00e1lisis de datos.  </li> </ol> <p>Este patr\u00f3n es clave cuando trabajamos con sistemas que necesitan procesar informaci\u00f3n a gran escala sin perder rendimiento.  </p>"},{"location":"curso2/tema7_maps/#ejemplo-practico-implementando-map-reduce","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico: Implementando Map-Reduce","text":"<p>En este ejemplo, crearemos un grafo que: 1. Distribuye datos entre m\u00faltiples nodos de procesamiento (Map). 2. Recoge y consolida los resultados en un nodo final (Reduce). </p>"},{"location":"curso2/tema7_maps/#paso-1-configuracion-del-estado","title":"Paso 1: Configuraci\u00f3n del Estado","text":"<p>Definimos el estado inicial del grafo, asegur\u00e1ndonos de que pueda manejar m\u00faltiples resultados de manera eficiente.  </p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom typing import TypedDict, Annotated\nfrom typing_extensions import TypedDict\nimport operator\n\n# Definimos el estado del grafo\nclass State(TypedDict):\n    datos: list[str]  # Datos a procesar en la fase Map\n    resultados: Annotated[list[str], operator.add]  # Resultados consolidados en Reduce\n</code></pre> Nota <p>Los datos se dividen en partes m\u00e1s peque\u00f1as, permitiendo que cada nodo procese solo una porci\u00f3n de la informaci\u00f3n.  </p>"},{"location":"curso2/tema7_maps/#paso-2-implementacion-de-la-fase-map","title":"Paso 2: Implementaci\u00f3n de la Fase \"Map\"","text":"<p>Cada nodo en la fase Map procesa una parte de los datos de forma independiente.  </p> <pre><code># Definimos los nodos de la fase \"Map\"\ndef procesar_dato_1(state: State):\n    return {\"resultados\": [f\"Procesado: {state['datos'][0]}\"]}\n\ndef procesar_dato_2(state: State):\n    return {\"resultados\": [f\"Procesado: {state['datos'][1]}\"]}\n\ndef procesar_dato_3(state: State):\n    return {\"resultados\": [f\"Procesado: {state['datos'][2]}\"]}\n</code></pre> Nota <p>Ventaja del Paralelismo: Cada nodo se ejecuta simult\u00e1neamente, acelerando el tiempo total de procesamiento.  </p>"},{"location":"curso2/tema7_maps/#paso-3-implementacion-de-la-fase-reduce","title":"Paso 3: Implementaci\u00f3n de la Fase \"Reduce\"","text":"<p>Los resultados generados en la fase Map se consolidan en un \u00fanico valor final en la fase Reduce.  </p> <pre><code># Definimos el nodo de la fase \"Reduce\"\ndef combinar_resultados(state: State):\n    # Analizariamos los datos procesados y mostrariamos un resultado final.\n    return {\"resultados\": state[\"resultados\"] + [\"Resultado Final\"]}\n</code></pre> Nota <p>\u00bfC\u00f3mo Funciona el Reducer? - Recoge los resultados individuales de cada nodo. - Los combina en una \u00fanica salida final que representa la informaci\u00f3n procesada.  </p>"},{"location":"curso2/tema7_maps/#paso-4-visualizacion-del-grafo","title":"Paso 4: Visualizaci\u00f3n del Grafo","text":"<p>Podemos representar el flujo de ejecuci\u00f3n para ver c\u00f3mo los datos se transforman en cada etapa.  </p> <pre><code>from IPython.display import Image, display\n\n# Construimos el grafo\nbuilder = StateGraph(State)\n\n# Agregamos los nodos\nbuilder.add_node(\"map_1\", procesar_dato_1)\nbuilder.add_node(\"map_2\", procesar_dato_2)\nbuilder.add_node(\"map_3\", procesar_dato_3)\nbuilder.add_node(\"reduce\", combinar_resultados)\n\n# Definimos el flujo de ejecuci\u00f3n\nbuilder.add_node(\"map_1\", procesar_dato_1)\nbuilder.add_node(\"map_2\", procesar_dato_2)\nbuilder.add_node(\"map_3\", procesar_dato_3)\n# Nos aseguramos que todos los nodos han terminado el procesamiento de datos.\n# Tal y como vimos en el tema de 'paralelismos'\nbuilder.add_edge([\"map_1\", \"map_2\", \"map_3\"], \"reduce\")\nbuilder.add_edge(\"reduce\", END)\n\n# Compilamos el grafo\ngraph = builder.compile()\n\n# Mostramos la visualizaci\u00f3n del grafo\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>\ud83d\udd0d En la imagen generada, se puede observar claramente la distribuci\u00f3n de datos en m\u00faltiples nodos antes de su consolidaci\u00f3n final.  </p>"},{"location":"curso2/tema7_maps/#paso-5-ejecucion-del-grafo","title":"Paso 5: Ejecuci\u00f3n del Grafo","text":"<p>Finalmente, ejecutamos el grafo para ver c\u00f3mo se procesan los datos en las diferentes fases.  </p> <pre><code># Ejecutamos el grafo con datos de entrada\ninput_data = {\"datos\": [\"Dato 1\", \"Dato 2\", \"Dato 3\"], \"resultados\": []}\nresultado_final = graph.invoke(input_data)\n\nprint(resultado_final)\n</code></pre> Salida esperada<pre><code>{\n  \"resultados\": [\"Resultado 1\", \"Resultado 2\", \"Resultado 3\", \"Resultado Final\"]\n}\n</code></pre> <p>\ud83d\udca1 \u00bfQu\u00e9 Observamos?</p> <ul> <li>Cada dato se proces\u00f3 de manera independiente en los nodos Map.</li> <li>Los resultados individuales se agruparon en un \u00fanico valor final en el nodo Reduce.</li> </ul>"},{"location":"curso2/tema7_maps/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>La t\u00e9cnica Map-Reduce nos permite aplicar de manera pr\u00e1ctica varios conceptos clave que hemos aprendido en este curso, como paralelismo y reducers. Adem\u00e1s, podemos llevar esta t\u00e9cnica a\u00fan m\u00e1s lejos incorporando funcionalidades avanzadas como Human in the Loop (HITL) o llamadas a modelos de lenguaje (LLMs).  </p> <p>El prop\u00f3sito principal de esta t\u00e9cnica es dividir una entrada compleja en m\u00faltiples tareas independientes para procesarlas de manera \u00f3ptima y consolidar los resultados de forma eficiente.  </p> <p>Un ejemplo pr\u00e1ctico de Map-Reduce en el mundo real podr\u00eda ser un an\u00e1lisis de sentimiento en redes sociales, donde podemos analizar miles de <code>tweets</code> en paralelo y luego unificarlos en un resultado final que refleje el sentimiento general sobre un tema.  </p> <p>En resumen, Map-Reduce en LangGraph nos ayuda a estructurar flujos eficientes para distribuir y consolidar informaci\u00f3n de manera \u00f3ptima:  </p> <ul> <li>Fragmenta grandes tareas en subtareas peque\u00f1as, optimizando la ejecuci\u00f3n en paralelo.  </li> <li>Facilita la consolidaci\u00f3n de resultados, asegurando la coherencia en la informaci\u00f3n procesada.  </li> <li>Mejora el rendimiento y la escalabilidad, permitiendo manejar grandes vol\u00famenes de datos sin afectar el flujo general.  </li> </ul> <p>Es una t\u00e9cnica fundamental para cualquier sistema que requiera procesamiento distribuido y consolidaci\u00f3n de datos dentro de un grafo. \ud83d\ude80  </p>"},{"location":"curso2/tema7_maps/#casos-de-uso-reales","title":"\ud83c\udfaf Casos de Uso Reales","text":"<ol> <li>Procesamiento de Texto: Dividir documentos en fragmentos, analizarlos en paralelo y generar un resumen final.</li> <li>An\u00e1lisis de Datos: Procesar grandes vol\u00famenes de datos en diferentes nodos antes de generar un informe consolidado.</li> <li>Consultas en Bases de Datos: Ejecutar m\u00faltiples consultas en paralelo y combinar los resultados en un solo informe.</li> </ol>"},{"location":"curso2/tema7_maps/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>C\u00f3mo dividir tareas en m\u00faltiples nodos de procesamiento (\"Map\").</li> <li>C\u00f3mo consolidar los resultados en un nodo final (\"Reduce\").</li> <li>C\u00f3mo optimizar flujos en LangGraph para mejorar el rendimiento.</li> </ul>"},{"location":"curso2/tema7_maps/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> C\u00e1lculo de sentimiento de tweets en Google Colab</li> <li> Definici\u00f3n del concepto: Map-reduce</li> <li> How-to-guide: Map-reduce</li> </ul>"},{"location":"curso2/tema7_maps/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos LangGraph Studio, una herramienta visual que nos permitir\u00e1 analizar y depurar nuestros grafos de manera interactiva.</p>"},{"location":"curso2/tema8_langgraph_studio/","title":"\ud83d\udda5\ufe0f Tema 8: LangGraph Studio \u2013 Visualizando y Depurando Grafos","text":""},{"location":"curso2/tema8_langgraph_studio/#que-es-langgraph-studio","title":"\ud83c\udf1f \u00bfQu\u00e9 es LangGraph Studio?","text":"<p>LangGraph Studio es una herramienta interactiva que permite visualizar, depurar y probar grafos de manera intuitiva. Proporciona una interfaz gr\u00e1fica donde podemos observar en tiempo real c\u00f3mo fluye la informaci\u00f3n entre los nodos de nuestro grafo.  </p> <p>LangGraph Studio nos permite: \u2705 Inspeccionar la ejecuci\u00f3n de los nodos en tiempo real. \u2705 Visualizar el flujo de datos dentro del estado del grafo. \u2705 Depurar errores y optimizar la l\u00f3gica de manera eficiente. \u2705 Probar interacciones con el grafo sin necesidad de escribir c\u00f3digo adicional.  </p> <p>\ud83d\udd39 \u00bfPor qu\u00e9 es \u00fatil? En grafos complejos, donde m\u00faltiples nodos interact\u00faan de manera simult\u00e1nea, LangGraph Studio nos ayuda a entender su comportamiento y a detectar posibles fallos sin necesidad de imprimir logs manualmente.  </p> Atenci\u00f3n <p>Actualmente, LangGraph Studio solo est\u00e1 disponible para macOS.</p>"},{"location":"curso2/tema8_langgraph_studio/#como-se-ve-langgraph-studio","title":"\ud83d\uddbc\ufe0f \u00bfC\u00f3mo Se Ve LangGraph Studio?","text":"<p>LangGraph Studio presenta una interfaz donde podemos visualizar nuestros grafos con: \ud83d\udd39 Nodos y conexiones representadas gr\u00e1ficamente. \ud83d\udd39 Estados actualizados en tiempo real tras cada ejecuci\u00f3n. \ud83d\udd39 Opciones de inspecci\u00f3n para ver los valores almacenados en cada paso del flujo.  </p> <p>A continuaci\u00f3n, mostramos una imagen de LangGraph Studio en acci\u00f3n:  </p> <p> \ud83d\udcf7 Captura de pantalla de LangGraph Studio </p>"},{"location":"curso2/tema8_langgraph_studio/#como-configurar-langgraph-studio","title":"\ud83d\udee0\ufe0f \u00bfC\u00f3mo Configurar LangGraph Studio?","text":"<p>Para habilitar LangGraph Studio en nuestro entorno, simplemente debemos iniciar el servidor local y conectar nuestro grafo.  </p> <p>La estructura final de nuestro proyecto deber\u00eda ser algo as\u00ed:</p> Estrutura de archivos<pre><code>my-app/\n\u251c\u2500\u2500 my_agent # Todo el codigo de nuestro proyecto aqu\u00ed\n\u2502   \u251c\u2500\u2500 utils # Las \u00fatiles de nuestro grafo\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 tools.py # Definimos nuestras herramientas aqu\u00ed\n\u2502   \u2502   \u251c\u2500\u2500 nodes.py # Definimos nuestros nodos aqu\u00ed\n\u2502   \u2502   \u2514\u2500\u2500 state.py # Deinimos nuestros estados aqu\u00ed\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 requirements.txt # Agregamos nuestras dependencias aqu\u00ed\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 agent.py # El codigo de contrucci\u00f3n de nuestro agente aqu\u00ed\n\u251c\u2500\u2500 .env # Vairables de entorno\n\u2514\u2500\u2500 langgraph.json # Archivo de configuraci\u00f3n de LangGraph\n</code></pre>"},{"location":"curso2/tema8_langgraph_studio/#ejemplo-practico-visualizando-un-grafo","title":"\ud83c\udfaf Ejemplo Pr\u00e1ctico: Visualizando un Grafo","text":"<p>Veamos c\u00f3mo podemos ejecutar un grafo con LangGraph Studio y analizar su comportamiento.  </p> <p>\ud83d\udccc Pasos: 1\ufe0f\u20e3 Definir un grafo sencillo con algunos nodos b\u00e1sicos. 2\ufe0f\u20e3 Conectarlo a LangGraph Studio para ver su ejecuci\u00f3n en tiempo real. 3\ufe0f\u20e3 Ejecutar el grafo y analizar la visualizaci\u00f3n en la interfaz.  </p> requirements.txt<pre><code># Definimos todos los modulos de python que vamos a necesitar.\nlanggraph\nlangchain_core\nlangchain_openai\n</code></pre> .env<pre><code>OPENAI_API_KEY=key # Ponemos nuestra clave de open AI aqu\u00ed\n</code></pre> agent.py<pre><code># my_agent/agent.py\n# Nuestro ejemplo de calculadora\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import MessagesState\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langgraph.graph import START, StateGraph, END\nfrom langgraph.prebuilt import tools_condition\nfrom langgraph.prebuilt import ToolNode\n\n# Primero definimos nuestras herramientas:\ndef sumar(a, b):\n  \"\"\"Usamos esta funci\u00f3n para sumar dos n\u00fameros\n  Args:\n    a (int): Primer n\u00famero a sumar\n    b (int): Segundo n\u00famero a sumar\n  Returns:\n    Un int or float con la suma de los dos n\u00fameros\n  \"\"\"\n  return  a + b\n\ndef restar(a, b):\n  \"\"\"Usamos esta funci\u00f3n para restar dos n\u00fameros\n  Args:\n    a (int): Primer n\u00famero a restar\n    b (int): Segundo n\u00famero a restar\n  Returns:\n    Un int or float con la resta de los dos n\u00fameros\n  \"\"\"\n  return a - b\n\ndef multiplicar(a, b):\n  \"\"\"Usamos esta funci\u00f3n para multiplicar dos n\u00fameros\n  Args:\n    a (int): Primer n\u00famero a multiplicar\n    b (int): Segundo n\u00famero a multiplicar\n  Returns:\n    Un int or float con la multiplicaci\u00f3n de los dos n\u00fameros\n  \"\"\"\n  return a * b\n\ndef dividir(a, b):\n  \"\"\"Usamos esta funci\u00f3n para dividir dos n\u00fameros\n  Args:\n    a (int): Primer n\u00famero a dividir (Dividendo)\n    b (int): Segundo n\u00famero a dividir (Divisor)\n  Returns:\n    Un int or float con la divisi\u00f3n de los dos n\u00fameros\n  \"\"\"\n  if b == 0:\n    return \"No se puede dividir por cero\"\n  return a / b\n\n\n# Vinculamos las tools al modelo de lenguaje.\ntools = [sumar, restar, multiplicar, dividir]\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nllm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n\n# Le definimos al LLM como queremos que evalue las operaciones.\nsys_msg = \"\"\"\n  Resuelve la ecuaci\u00f3n aplicando las siguientes reglas:\n  1. Eval\u00faa las operaciones dentro de par\u00e9ntesis primero.\n  2. Luego, eval\u00faa multiplicaciones y divisiones de izquierda a derecha.\n  3. Finalmente, eval\u00faa sumas y restas de izquierda a derecha.\n  4. Usa las herramientas proporcionadas: sumar, restar, multiplicar, dividir.\n  \"\"\"\n\n# Creamos nuestro asistente que se encargar\u00e1 de llamar a las Tools necesarias\n# tantas veces como sea necesario.\ndef assistant(state: MessagesState):\n    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n\n\n# Configuramos el flujo\nbuilder = StateGraph(MessagesState)\n# A\u00f1adimos nodos\nbuilder.add_node(\"assistant\", assistant)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\n# Definimos los edges y el flujo del grafo\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_conditional_edges(\"assistant\", tools_condition)\nbuilder.add_edge(\"tools\", \"assistant\")\ngraph = builder.compile()\n</code></pre> langgraph.json<pre><code>{\n  \"dependencies\": [\"./my_agent\"],\n  \"graphs\": {\n    \"agent\": \"./my_agent/agent.py:graph\"\n  },\n  \"env\": \".env\"\n}\n</code></pre> <p>Una vez configurado, podremos ver en LangGraph Studio c\u00f3mo la informaci\u00f3n fluye entre los nodos y c\u00f3mo se actualizan los datos en cada paso.</p> <p>Para ello, simplemente: 1\ufe0f\u20e3 Abrimos la aplicaci\u00f3n de LangGraph Studio. 2\ufe0f\u20e3 Iniciamos sesi\u00f3n con nuestra cuenta de LangSmith. 3\ufe0f\u20e3 Subimos la carpeta de nuestro proyecto para comenzar a visualizar la ejecuci\u00f3n del grafo.  </p>"},{"location":"curso2/tema8_langgraph_studio/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>LangGraph Studio es una herramienta esencial para cualquier desarrollador que trabaje con grafos en LangGraph.  </p> <p>\u2705 Nos permite visualizar la ejecuci\u00f3n en tiempo real, facilitando la depuraci\u00f3n. \u2705 Mejora la comprensi\u00f3n del flujo de datos dentro del grafo. \u2705 Facilita la optimizaci\u00f3n y el ajuste de la l\u00f3gica de los nodos.  </p> <p>Si trabajamos con grafos complejos, LangGraph Studio se convierte en una herramienta indispensable para analizar, mejorar y depurar nuestros flujos. \ud83d\ude80  </p>"},{"location":"curso2/tema8_langgraph_studio/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Qu\u00e9 es LangGraph Studio y c\u00f3mo nos ayuda en el desarrollo de grafos. </li> <li>C\u00f3mo visualizar, depurar y probar la ejecuci\u00f3n de nuestros flujos. </li> <li>C\u00f3mo iniciar el servidor y conectar nuestros grafos a LangGraph Studio. </li> </ul>"},{"location":"curso2/tema8_langgraph_studio/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos LangSmith, una herramienta avanzada para analizar y optimizar el comportamiento de nuestros grafos con m\u00e9tricas detalladas y mayor control sobre la ejecuci\u00f3n.  </p>"},{"location":"curso2/tema9_langsmith/","title":"\ud83d\udee0\ufe0f Tema 9: LangSmith \u2013 Analizando y Optimizando Grafos","text":""},{"location":"curso2/tema9_langsmith/#que-es-langsmith","title":"\ud83c\udf1f \u00bfQu\u00e9 es LangSmith?","text":"<p>LangSmith es una plataforma avanzada de depuraci\u00f3n y monitoreo dise\u00f1ada para mejorar la observabilidad, evaluaci\u00f3n y optimizaci\u00f3n de modelos de lenguaje y flujos de trabajo en LangGraph.  </p> <p>\ud83d\udd39 \u00bfQu\u00e9 nos permite hacer LangSmith? \u2705 Registrar y analizar cada ejecuci\u00f3n de nuestro grafo. \u2705 Evaluar m\u00e9tricas clave como tiempos de respuesta y eficiencia del modelo. \u2705 Depurar errores y entender mejor las decisiones tomadas por el grafo. \u2705 Optimizar la l\u00f3gica de nuestro flujo mediante un an\u00e1lisis detallado.  </p> <p>LangSmith act\u00faa como un registro interactivo que nos permite inspeccionar la ejecuci\u00f3n de nuestros grafos con mayor granularidad, lo que facilita la optimizaci\u00f3n y el mantenimiento de flujos complejos.  </p>"},{"location":"curso2/tema9_langsmith/#por-que-usar-langsmith","title":"\ud83d\udee0\ufe0f \u00bfPor Qu\u00e9 Usar LangSmith?","text":"<p>Cuando trabajamos con grafos complejos que involucran m\u00faltiples modelos de lenguaje, herramientas y flujos condicionales, es crucial contar con una herramienta que nos ayude a rastrear y analizar cada paso del proceso.  </p> <p>LangSmith nos permite: \u2705 Auditar y depurar fallos en la ejecuci\u00f3n de modelos y flujos de datos. \u2705 Identificar cuellos de botella y optimizar la eficiencia del sistema. \u2705 Comparar diferentes versiones de un mismo grafo para elegir la m\u00e1s efectiva.  </p> <p>En resumen, LangSmith convierte los flujos de trabajo en datos estructurados que podemos analizar para mejorar continuamente nuestras aplicaciones.  </p>"},{"location":"curso2/tema9_langsmith/#como-se-ve-langsmith","title":"\ud83d\udcca \u00bfC\u00f3mo Se Ve LangSmith?","text":"<p>LangSmith ofrece una interfaz visual que permite explorar ejecuciones previas de nuestros grafos, visualizar m\u00e9tricas clave y analizar cada paso del flujo en detalle.  </p> <p>\ud83d\udcf7 Captura de pantalla de LangSmith en acci\u00f3n </p>"},{"location":"curso2/tema9_langsmith/#como-configurar-langsmith","title":"\ud83d\udee0\ufe0f \u00bfC\u00f3mo Configurar LangSmith?","text":"<p>Para integrar LangSmith en nuestro flujo de trabajo, primero debemos configurar las credenciales y luego habilitar el seguimiento de logs en nuestros grafos.  </p> <p>\ud83d\udccc Pasos para la configuraci\u00f3n: 1\ufe0f\u20e3 Crear una cuenta en LangSmith en smith.langchain.com. 2\ufe0f\u20e3 Obtener la API Key desde el panel de configuraci\u00f3n. 3\ufe0f\u20e3 Configurar la API Key en nuestro entorno. 4\ufe0f\u20e3 Integrar LangSmith en nuestro c\u00f3digo para empezar a registrar las ejecuciones.  </p> <p> </p> <pre><code>LANGSMITH_TRACING=true\nLANGSMITH_ENDPOINT=\"https://eu.api.smith.langchain.com\"\nLANGSMITH_API_KEY=\"&lt;your-api-key&gt;\"\nLANGSMITH_PROJECT=\"Mi-projecto\"\nOPENAI_API_KEY=\"&lt;your-openai-api-key&gt;\"\n</code></pre>"},{"location":"curso2/tema9_langsmith/#ejemplo-practico-monitoreo-de-un-grafo-con-langsmith","title":"\ud83c\udfaf Ejemplo Pr\u00e1ctico: Monitoreo de un Grafo con LangSmith","text":"<p>Veamos c\u00f3mo podemos conectar un grafo a LangSmith y analizar sus m\u00e9tricas.  </p> <p>\ud83d\udccc Pasos: 1\ufe0f\u20e3 Definir un grafo y habilitar la integraci\u00f3n con LangSmith. 2\ufe0f\u20e3 Ejecutar el grafo y analizar su comportamiento en LangSmith. 3\ufe0f\u20e3 Visualizar los logs y m\u00e9tricas obtenidas. </p> <p> </p> <p>Tras ejecutar nuestro grafo con LangSmith, podremos inspeccionar cada paso del proceso y evaluar c\u00f3mo se comporta nuestro flujo en t\u00e9rminos de eficiencia y precisi\u00f3n.  </p>"},{"location":"curso2/tema9_langsmith/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>LangSmith es una herramienta fundamental para cualquier desarrollador que desee mejorar la transparencia, trazabilidad y rendimiento de sus grafos en LangGraph.  </p> <p>\u2705 Nos permite auditar y optimizar flujos de trabajo con datos estructurados. \u2705 Facilita la identificaci\u00f3n de errores y cuellos de botella. \u2705 Mejora la eficiencia y la toma de decisiones en la construcci\u00f3n de modelos de lenguaje.  </p> <p>Si queremos llevar nuestros grafos al siguiente nivel, LangSmith nos proporciona la informaci\u00f3n necesaria para hacerlo de manera precisa y efectiva. \ud83d\ude80  </p>"},{"location":"curso2/tema9_langsmith/#has-completado-el-curso-2","title":"\ud83c\udf93 \u00a1Has Completado el Curso 2!","text":"<p>\ud83c\udf89 \u00a1Felicidades! Has finalizado el segundo curso de LangGraph, donde hemos aprendido sobre herramientas avanzadas como LangGraph Studio, paralelismo, subgrafos y optimizaci\u00f3n con LangSmith.  </p> <p>Hemos visto c\u00f3mo construir grafos m\u00e1s eficientes, depurarlos y analizar su rendimiento para mejorar nuestras aplicaciones basadas en modelos de lenguaje.  </p> <p>Pero esto no termina aqu\u00ed...  </p>"},{"location":"curso2/tema9_langsmith/#que-viene-en-el-curso-3","title":"\ud83d\ude80 \u00bfQu\u00e9 Viene en el Curso 3?","text":"<p>En el pr\u00f3ximo curso, llevaremos LangGraph a un nivel a\u00fan m\u00e1s profesional. Exploraremos temas como: \u2705 Memoria a Largo Plazo para mantener contexto entre sesiones. \u2705 Almacenamiento en LangGraph Store para persistencia de datos. \u2705 Despliegue de grafos en producci\u00f3n con LangGraph CLI y servidores locales.  </p> <p>Si est\u00e1s listo para llevar tus conocimientos al siguiente nivel, te esperamos en el Curso 3. \u00a1Nos vemos all\u00ed! \ud83d\ude80  </p>"},{"location":"curso3/","title":"\ud83d\ude80 Curso 3: Despliegue y Proyecto Final","text":""},{"location":"curso3/#bienvenida-al-curso-3","title":"\ud83d\udc4b Bienvenida al Curso 3","text":"<p>\u00a1Ya est\u00e1s en la recta final! \ud83c\udfd7\ufe0f En este curso, nos enfocaremos en llevar tus flujos de trabajo a producci\u00f3n. Aprender\u00e1s c\u00f3mo desplegar aplicaciones LangGraph, crear memorias extensas y desarrollar APIs REST para interactuar con tus flujos de manera externa.</p>"},{"location":"curso3/#que-aprenderemos","title":"\ud83c\udfaf \u00bfQu\u00e9 Aprenderemos?","text":"<ul> <li>Implementar flujos de memoria a largo plazo.</li> <li>Desplegar LangGraph en servidores locales y en la nube.</li> <li>Crear APIs REST para conectar tus flujos con otras aplicaciones.</li> <li>Usar la CLI de LangGraph para facilitar despliegues y pruebas.</li> <li>Utilizar la SDK de LangGraph para interactuar con grafos de manera program\u00e1tica.</li> <li>Explorar Langflow, una herramienta visual para dise\u00f1ar y gestionar flujos de IA sin c\u00f3digo.</li> </ul>"},{"location":"curso3/#objetivo-del-curso","title":"\ud83c\udfc6 Objetivo del Curso","text":"<p>El objetivo es que logres desarrollar y desplegar flujos completos listos para producci\u00f3n. Aprender\u00e1s a trabajar con herramientas que facilitan la puesta en marcha y el mantenimiento de tus aplicaciones de LangGraph.</p>"},{"location":"curso3/#temario","title":"\ud83d\udccb Temario","text":"<ol> <li>Tema 1: Long Memory</li> <li>Tema 2: Memory Schema</li> <li>Tema 3: LangGraph Store</li> <li>Tema 4: LangGraph CLI</li> <li>Tema 5: La SDK de LangGraph</li> <li>Tema 6: Doble-texting</li> <li>Tema 7: API REST/GraphQL</li> <li>Tema 8: LangFlow</li> </ol>"},{"location":"curso3/#resultado-final-del-curso","title":"\ud83c\udfc1 Resultado Final del Curso","text":"<ul> <li>Desplegar\u00e1s flujos completos de LangGraph.</li> <li>Crear\u00e1s APIs REST que interact\u00faan con tus flujos.</li> <li>Podr\u00e1s implementar sistemas completos y listos para producci\u00f3n.</li> </ul>"},{"location":"curso3/#tecnologias-utilizadas","title":"\u2699\ufe0f Tecnolog\u00edas Utilizadas","text":"<ul> <li>LangGraph 0.3.x</li> <li>Docker</li> <li>Python FastAPI</li> <li>GraphQL</li> <li>PostgreSQL</li> </ul>"},{"location":"curso3/tema1_long_term_memory/","title":"\ud83e\udde0 Tema 1: Long-Term Memory \u2013 Memoria a Largo Plazo en LangGraph","text":""},{"location":"curso3/tema1_long_term_memory/#que-es-la-memoria-a-largo-plazo-en-langgraph","title":"\ud83c\udf1f \u00bfQu\u00e9 es la Memoria a Largo Plazo en LangGraph?","text":"<p>En el Curso 1, exploramos c\u00f3mo LangGraph maneja la memoria de estado, que nos permite almacenar informaci\u00f3n dentro del estado del grafo mientras se ejecuta una sesi\u00f3n. Sin embargo, esta memoria es temporal y se pierde una vez que el flujo del grafo finaliza.  </p> <p>Tambi\u00e9n aprendimos sobre la memoria de corto alcance (Short-Term Memory), que nos permit\u00eda guardar el historial de un chat dentro de una misma sesi\u00f3n, proporcionando mayor contexto y fluidez a la conversaci\u00f3n sin necesidad de almacenamiento externo.  </p> <p>Ahora, en el Curso 3, introducimos un concepto a\u00fan m\u00e1s avanzado: Long-Term Memory, una memoria persistente que permite mantener datos entre m\u00faltiples ejecuciones, haciendo posible que el sistema recuerde informaci\u00f3n pasada y la reutilice en futuras interacciones.  </p> <p>\ud83d\udd39 \u00bfQu\u00e9 nos permite hacer la memoria a largo plazo? \u2705 Almacenar contexto de conversaciones previas para mantener continuidad en interacciones futuras. \u2705 Persistir informaci\u00f3n relevante en bases de datos o sistemas externos. \u2705 Optimizar respuestas en chatbots al recordar detalles importantes a lo largo del tiempo.  </p> <p>Con esta memoria, nuestros grafos pueden aprender de interacciones pasadas y mejorar su desempe\u00f1o con el tiempo. \ud83d\ude80  </p>"},{"location":"curso3/tema1_long_term_memory/#comparacion-memoria-basica-vs-memoria-a-corto-y-largo-plazo","title":"\ud83d\udd04 Comparaci\u00f3n: Memoria B\u00e1sica vs. Memoria a Corto y Largo Plazo","text":"Caracter\u00edstica Memoria B\u00e1sica \ud83d\udfe2 Memoria a Corto Plazo \ud83d\udfe1 Memoria a Largo Plazo \ud83d\udd35 Alcance Solo durante la ejecuci\u00f3n del grafo Se mantiene dentro de una sesi\u00f3n activa Se mantiene entre sesiones Persistencia Se borra al finalizar el flujo Se almacena temporalmente en la memoria del grafo Se guarda en almacenamiento externo Ejemplo de Uso Referencias dentro de una conversaci\u00f3n en curso Mantener el historial de mensajes en una sesi\u00f3n de chat Recordar interacciones a lo largo del tiempo Implementaci\u00f3n Se gestiona dentro del estado (<code>State</code>) Se almacena en memoria (<code>MemorySaver</code>) dentro de la sesi\u00f3n Usa almacenamiento externo como bases de datos (<code>InMemoryStore</code>) Casos de Uso Flujos de conversaci\u00f3n dentro de un solo request Chats que necesitan contexto en una misma sesi\u00f3n Sistemas con memoria continua (chatbots avanzados, asistentes virtuales, etc.) Nota <ul> <li>La memoria b\u00e1sica es \u00fatil para tareas donde no necesitamos persistencia entre sesiones.  </li> <li>La memoria a corto plazo nos permite recordar datos dentro de una misma sesi\u00f3n, pero se pierde al cerrarla.  </li> <li>la memoria a largo plazo es ideal cuando queremos que el sistema recuerde informaci\u00f3n incluso despu\u00e9s de varias sesiones, permitiendo experiencias m\u00e1s personalizadas y contextuales.  </li> </ul>"},{"location":"curso3/tema1_long_term_memory/#como-funciona-la-memoria-a-largo-plazo","title":"\ud83d\udee0\ufe0f \u00bfC\u00f3mo Funciona la Memoria a Largo Plazo?","text":"<p>LangGraph nos permite almacenar memoria de manera persistente utilizando diferentes opciones:  </p> <p>\u2705 Bases de datos externas como PostgreSQL o MongoDB. \u2705 Sistemas de almacenamiento en la nube como Redis o vectores sem\u00e1nticos. \u2705 Archivos locales donde guardamos registros de conversaci\u00f3n.  </p> <p>En esta implementaci\u00f3n, LangGraph guarda los datos en un sistema de almacenamiento que podemos recuperar en futuras ejecuciones del grafo.  </p> <p>\ud83d\udccc Ejemplo Pr\u00e1ctico: Implementaci\u00f3n de Memoria a Largo Plazo en LangGraph  </p> <pre><code>from IPython.display import Image, display\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.store.base import BaseStore\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.runnables.config import RunnableConfig\n\n# Instrucci\u00f3n del chatbot\nMODEL_SYSTEM_MESSAGE = \"\"\"Eres un asistente \u00fatil con memoria que proporciona informaci\u00f3n personalizada sobre el usuario. \nSi tienes memoria para este usuario, util\u00edzala para personalizar tus respuestas.\nAqu\u00ed est\u00e1 la memoria (puede estar vac\u00eda): {memory}\"\"\"\n\n# Crear nueva memoria a partir del historial de chat y cualquier memoria existente\nCREATE_MEMORY_INSTRUCTION = \"\"\"Est\u00e1s recopilando informaci\u00f3n sobre el usuario para personalizar tus respuestas.\n\nINFORMACI\u00d3N ACTUAL DEL USUARIO:\n{memory}\n\nINSTRUCCIONES:\n1. Revisa cuidadosamente el historial de chat a continuaci\u00f3n.\n2. Identifica nueva informaci\u00f3n sobre el usuario, como:\n   - Datos personales (nombre, ubicaci\u00f3n).\n   - Preferencias (gustos, aversiones).\n   - Intereses y aficiones.\n   - Experiencias pasadas.\n   - Objetivos o planes futuros.\n3. Fusiona cualquier nueva informaci\u00f3n con la memoria existente.\n4. Formatea la memoria como una lista clara con vi\u00f1etas.\n5. Si la nueva informaci\u00f3n contradice la memoria existente, conserva la versi\u00f3n m\u00e1s reciente.\n\nRecuerda: Solo incluye informaci\u00f3n real directamente mencionada por el usuario. No hagas suposiciones ni inferencias.\n\nBasado en el historial de chat a continuaci\u00f3n, actualiza la informaci\u00f3n del usuario:\"\"\"\n\ndef call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n\n    \"\"\"Carga la memoria desde el almacenamiento y la usa para personalizar la respuesta del chatbot.\"\"\"\n\n    # Obtiene el ID del usuario desde la configuraci\u00f3n\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Recupera la memoria desde el almacenamiento\n    namespace = (\"memory\", user_id)\n    key = \"user_memory\"\n    existing_memory = store.get(namespace, key)\n\n    # Extrae el contenido de la memoria si existe y agrega un prefijo\n    if existing_memory:\n        # El valor es un diccionario con la clave \"memory\"\n        existing_memory_content = existing_memory.value.get('memory')\n    else:\n        existing_memory_content = \"No se encontr\u00f3 memoria previa.\"\n\n    # Formatea la memoria en el mensaje del sistema\n    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n\n    # Responde utilizando la memoria junto con el historial de chat\n    response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n\n    return {\"messages\": response}\n\ndef write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n\n    \"\"\"Reflexiona sobre el historial de chat y guarda la memoria en el almacenamiento.\"\"\"\n\n    # Obtiene el ID del usuario desde la configuraci\u00f3n\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Recupera la memoria existente desde el almacenamiento\n    namespace = (\"memory\", user_id)\n    existing_memory = store.get(namespace, \"user_memory\")\n\n    # Extrae el contenido de la memoria\n    if existing_memory:\n        existing_memory_content = existing_memory.value.get('memory')\n    else:\n        existing_memory_content = \"No se encontr\u00f3 memoria previa.\"\n\n    # Formatea la memoria en el mensaje del sistema\n    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n    new_memory = model.invoke([SystemMessage(content=system_msg)] + state['messages'])\n\n    # Sobrescribe la memoria existente en el almacenamiento \n    key = \"user_memory\"\n\n    # Guarda el valor como un diccionario con la clave \"memory\"\n    store.put(namespace, key, {\"memory\": new_memory.content})\n\n# Definir el grafo\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"write_memory\", write_memory)\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_edge(\"call_model\", \"write_memory\")\nbuilder.add_edge(\"write_memory\", END)\n\n# Almacenamiento para la memoria a largo plazo (entre sesiones)\nacross_thread_memory = InMemoryStore()\n\n# Checkpointer para la memoria a corto plazo (dentro de una sesi\u00f3n)\nwithin_thread_memory = MemorySaver()\n\n# Compilar el grafo con memoria a corto y largo plazo\ngraph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n\n# Visualizar el grafo\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n</code></pre> <p></p> <p>Este c\u00f3digo implementa un sistema de memoria a largo plazo y memoria a corto plazo en un grafo de LangGraph.  </p> <ul> <li> <p>\ud83d\udfe1 Memoria a corto plazo (<code>MemorySaver</code>):   Permite recordar informaci\u00f3n dentro de una \u00fanica sesi\u00f3n o hilo de conversaci\u00f3n.  </p> </li> <li> <p>\ud83d\udd35 Memoria a largo plazo (<code>InMemoryStore</code>):   Guarda informaci\u00f3n del usuario entre m\u00faltiples sesiones para personalizar respuestas futuras.  </p> </li> <li> <p>\ud83d\udccc Estructura del Grafo: </p> <ul> <li><code>call_model</code>: Recupera la memoria almacenada y la usa para personalizar las respuestas del chatbot.  </li> <li><code>write_memory</code>: Extrae informaci\u00f3n del historial de mensajes y la almacena para su uso posterior.  </li> </ul> </li> <li> <p>\ud83d\udca1 Personalizaci\u00f3n del Chatbot:   El chatbot puede recordar detalles personales del usuario, como su nombre o intereses, y utilizarlos en futuras conversaciones.    </p> </li> </ul>"},{"location":"curso3/tema1_long_term_memory/#probando-la-memoria-a-largo-plazo","title":"\ud83d\ude80 Probando la Memoria a Largo Plazo","text":""},{"location":"curso3/tema1_long_term_memory/#iniciamos-un-chat","title":"\ud83d\udcdd Iniciamos un chat","text":"<p>Ejecutamos un chat donde el usuario proporciona su nombre y un gusto personal:  </p> <pre><code># Proporcionamos un ID de hilo para la memoria a corto plazo (dentro de la sesi\u00f3n)\n# Proporcionamos un ID de usuario para la memoria a largo plazo (entre sesiones)\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n\n# Entrada del usuario \ninput_messages = [HumanMessage(content=\"Hola, me llamo Ra\u00fal y me gusta salir en bici.\")]\n\n# Ejecutamos el grafo y mostramos la respuesta del chatbot\nfor chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nHola, me llamo Ra\u00fal y me gusta salir en bici.\n================================== Ai Message ==================================\n\n\u00a1Hola, Ra\u00fal! Es genial que te guste salir en bici. \u00bfSueles hacer rutas largas o prefieres paseos m\u00e1s cortos?\n</code></pre> <p>Podemos ver c\u00f3mo esta informaci\u00f3n se almacena en la memoria a corto plazo, permitiendo que el chatbot siga record\u00e1ndola en la conversaci\u00f3n actual.  </p> <p>Para verificarlo, podemos inspeccionar el estado del grafo: </p> <pre><code>thread = {\"configurable\": {\"thread_id\": \"1\"}}\nstate = graph.get_state(thread).values\nfor m in state[\"messages\"]: \n    m.pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nHola, me llamo Ra\u00fal y me gusta salir en bici.\n================================== Ai Message ==================================\n\n\u00a1Hola, Ra\u00fal! Es genial que te guste salir en bici. \u00bfSueles hacer rutas largas o prefieres paseos m\u00e1s cortos?\n</code></pre> <p>Adem\u00e1s, tambi\u00e9n podemos revisar la memoria a largo plazo para comprobar qu\u00e9 informaci\u00f3n se ha almacenado:  </p> <pre><code>user_id = \"1\"\nnamespace = (\"memory\", user_id)\nexisting_memory = across_thread_memory.get(namespace, \"user_memory\")\nexisting_memory.dict()\n</code></pre> Resultado<pre><code>{'namespace': ['memory', '1'],\n 'key': 'user_memory',\n 'value': {'memory': 'Actualizaci\u00f3n de la memoria del usuario:\\n\\n- Nombre: Ra\u00fal\\n- Gustos: Salir en bici.'},\n 'created_at': '2025-02-05T11:39:01.252749+00:00',\n 'updated_at': '2025-02-05T11:39:01.252753+00:00'}\n</code></pre> Nota <p>Como podemos vemos ver, nuestro grafo ha creado ya un peque\u00f1o perfil de nuestro usuario, almazenando tanto su nombre, como sus gustos.</p>"},{"location":"curso3/tema1_long_term_memory/#iniciamos-un-nuevo-chat","title":"\ud83d\udd04 Iniciamos un nuevo chat","text":"<p>Ahora, simularemos una nueva conversaci\u00f3n, donde el usuario no proporciona contexto previo, pero el chatbot deber\u00eda recordar informaci\u00f3n de la memoria a largo plazo:  </p> <pre><code># Proporcionamos un ID de un nuvo hilo para la memoria a corto plazo (dentro de la sesi\u00f3n), esto simulara un nuevo chat.\n# Proporcionamos un ID de usuario para la memoria a largo plazo (entre sesiones)\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n\n# Entrada del usuario \ninput_messages = [HumanMessage(content=\"\u00bfMe puedes recomendar algo que hacer hoy?\")]\n\n# Ejecutamos el grafo y mostramos la respuesta del chatbot\nfor chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\n\u00bfMe puedes recomendar algo que hacer hoy?\n================================== Ai Message ==================================\n\n\u00a1Claro, Ra\u00fal! Dado que te gusta salir en bici, podr\u00edas aprovechar el d\u00eda para hacer una ruta en bicicleta. Busca alg\u00fan sendero o parque cercano donde puedas disfrutar del paisaje. Si el clima es agradable, tambi\u00e9n podr\u00edas considerar llevar algo de comida y hacer un picnic en el camino. \u00bfTe gustar\u00eda que te ayudara a encontrar lugares espec\u00edficos para montar en bici?\n</code></pre> Nota <p>A pesar de que el usuario no ha mencionado su nombre ni sus intereses en este nuevo chat, el chatbot recuerda que se llama Ra\u00fal y que le gusta salir en bici, gracias a la memoria almacenada en sesiones anteriores.  </p>"},{"location":"curso3/tema1_long_term_memory/#cuando-usar-memoria-a-largo-plazo","title":"\ud83d\udd0e \u00bfCu\u00e1ndo Usar Memoria a Largo Plazo?","text":"<p>La memoria persistente es clave en aplicaciones como:  </p> <ul> <li>Chatbots que deben recordar a los usuarios y personalizar respuestas seg\u00fan interacciones previas.  </li> <li>Asistentes virtuales que requieren seguimiento de tareas o preferencias del usuario.  </li> <li>Sistemas de soporte t\u00e9cnico que deben recordar consultas anteriores para brindar un mejor servicio.  </li> </ul> <p>Si nuestro sistema necesita contexto entre m\u00faltiples sesiones, entonces la memoria a largo plazo es la mejor opci\u00f3n.  </p>"},{"location":"curso3/tema1_long_term_memory/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>La memoria a largo plazo en LangGraph nos permite alamacenar informacion relevante de nuestro usuario, podriamos incluso generar un peque\u00f1o perfil personalizado para usarlo en posteriores sesiones.</p> <p>Uno de los puntos que hace esta tecnica tan interesante, es que podemos jugar con el tipo de informacion que almacenamos de nuestro usuario, haciendo que el resultado de las respuesta sea mas espectacular.</p> <p>\u2705 Nos ayuda a recordar informaci\u00f3n m\u00e1s all\u00e1 de una sesi\u00f3n, mejorando la experiencia del usuario. \u2705 Permite almacenar datos en bases de datos externas, brindando mayor flexibilidad. \u2705 Es ideal para chatbots, asistentes y sistemas de soporte t\u00e9cnico que requieren memoria a largo plazo.  </p> <p>\ud83d\ude80 Con esta funcionalidad, nuestros grafos pueden evolucionar y mejorar con el tiempo.  </p>"},{"location":"curso3/tema1_long_term_memory/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Definici\u00f3n: Memory</li> <li> Definici\u00f3n: Memory-store</li> <li> Class: BaseStore</li> <li> Wikipedia: Semantic-memory</li> </ul>"},{"location":"curso3/tema1_long_term_memory/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Diferencias entre la memoria b\u00e1sica y la memoria a largo plazo. </li> <li>C\u00f3mo funciona la persistencia de datos en LangGraph. </li> <li>Cu\u00e1ndo es \u00fatil utilizar memoria persistente en nuestros grafos. </li> </ul>"},{"location":"curso3/tema1_long_term_memory/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos memory schema, almacenando memoria con una estructura personalizada.  </p>"},{"location":"curso3/tema2_memory_schema/","title":"\ud83e\udde0 Tema 2: Memory Schema \u2013 Estructurando la Memoria en LangGraph","text":""},{"location":"curso3/tema2_memory_schema/#introduccion","title":"\ud83c\udf1f Introducci\u00f3n","text":"<p>En el tema anterior, exploramos c\u00f3mo la Memoria a Largo Plazo (Long-Term Memory) nos permite persistir informaci\u00f3n entre sesiones de un grafo.  </p> <p>Sin embargo, almacenar datos sin una estructura definida puede llevar a problemas de organizaci\u00f3n y recuperaci\u00f3n de la informaci\u00f3n.  </p> <p>Para solucionar esto, LangGraph introduce los Memory Schemas, que nos permiten definir c\u00f3mo se almacenan y gestionan los datos dentro del sistema de memoria del grafo.  </p>"},{"location":"curso3/tema2_memory_schema/#que-es-un-memory-schema","title":"\ud83d\udd0d \u00bfQu\u00e9 es un Memory Schema?","text":"<p>Un Memory Schema es una forma estructurada de almacenar datos en la memoria del grafo. Nos permite:  </p> <p>\u2705 Definir c\u00f3mo se organiza la informaci\u00f3n dentro de la memoria. \u2705 Garantizar consistencia en los datos almacenados. \u2705 Facilitar la recuperaci\u00f3n de informaci\u00f3n en futuras interacciones.  </p> <p>LangGraph nos ofrece dos tipos principales de esquemas de memoria:  </p> <p>1\ufe0f\u20e3 Profile Schema \u2013 Ideal para almacenar informaci\u00f3n estructurada sobre un usuario. 2\ufe0f\u20e3 Collection Schema \u2013 Perfecto para gestionar m\u00faltiples registros de datos.  </p>"},{"location":"curso3/tema2_memory_schema/#profile-schema-perfil-del-usuario","title":"\ud83d\udccc Profile Schema \u2013 Perfil del Usuario","text":"<p>El Profile Schema nos permite estructurar la informaci\u00f3n de un usuario de manera clara y organizada.  </p> <p>Esto es \u00fatil para asistentes virtuales, chatbots o sistemas personalizados que necesitan recordar detalles clave de cada usuario.  </p> <p>\ud83d\udccc Ejemplo de uso: - Nombre del usuario - Ubicaci\u00f3n - Intereses - Preferencias  </p> <pre><code>from typing import TypedDict\nfrom langgraph.store.memory import InMemoryStore\n\n# Definimos un esquema de perfil para almacenar informaci\u00f3n estructurada del usuario\nclass UserProfile(TypedDict):\n    name: str\n    location: str\n    interests: list[str]\n\n# Creamos una instancia de almacenamiento en memoria\nstore = InMemoryStore()\n\n# Definimos un usuario con informaci\u00f3n b\u00e1sica\nuser_id = \"user_123\"\nprofile_data = UserProfile(name=\"Ra\u00fal\", location=\"Barcelona\", interests=[\"Ciclismo\", \"Tecnolog\u00eda\"])\n\n# Guardamos el perfil en la memoria persistente\nstore.put(namespace=(\"profile\", user_id), key=\"user_info\", value=profile_data)\n\n# Recuperamos la informaci\u00f3n del usuario\nretrieved_profile = store.get(namespace=(\"profile\", user_id), key=\"user_info\")\n\n# Mostramos los datos almacenados\nprint(retrieved_profile.dict())\n</code></pre> Resultado<pre><code>{'namespace': ['profile', 'user_123'], 'key': 'user_info', 'value': {'name': 'Ra\u00fal', 'location': 'Barcelona', 'interests': ['Ciclismo', 'Tecnolog\u00eda']}, 'created_at': '2025-02-06T09:52:45.865704+00:00', 'updated_at': '2025-02-06T09:52:45.865711+00:00'}\n</code></pre> <p>En este ejemplo, creamos un esquema de perfil donde el asistente almacena detalles relevantes sobre el usuario, lo que permite ofrecer respuestas m\u00e1s personalizadas en futuras interacciones. \ud83d\ude80</p> Nota <p>Utilizamos tres elementos clave para almacenar y recuperar datos en memoria:</p> <p>\ud83d\udd39 namespace: Act\u00faa como una \"categor\u00eda\" o espacio de almacenamiento para organizar mejor los datos. En este caso, usamos <code>\"profile\"</code> para identificar que estamos guardando informaci\u00f3n de usuario.</p> <p>\ud83d\udd39 key: Es el identificador \u00fanico dentro de un <code>namespace</code>. Aqu\u00ed usamos <code>\"user_info\"</code> para hacer referencia al perfil del usuario dentro de la categor\u00eda <code>\"profile\"</code>.</p> <p>\ud83d\udd39 value: Contiene la informaci\u00f3n real que queremos almacenar, en este caso, el perfil del usuario con su nombre, ubicaci\u00f3n e intereses.</p> <p>De esta forma, organizamos los datos de manera estructurada y podemos recuperarlos f\u00e1cilmente cuando sea necesario.</p>"},{"location":"curso3/tema2_memory_schema/#collection-schema-almacenamiento-de-multiples-registros","title":"\ud83d\udccc Collection Schema \u2013 Almacenamiento de M\u00faltiples Registros","text":"<p>El Collection Schema es \u00fatil cuando necesitamos almacenar m\u00faltiples entradas organizadas de manera eficiente.  </p> <p>\ud83d\udccc Ejemplo de uso: - Historial de interacciones - Registro de compras en un e-commerce - Logs de asistencia t\u00e9cnica  </p> <p>A diferencia del Profile Schema, aqu\u00ed almacenamos una colecci\u00f3n de datos, permitiendo manejar un historial de eventos de forma estructurada.  </p> <pre><code>from typing import TypedDict\nfrom langgraph.store.memory import InMemoryStore\nfrom datetime import datetime\nimport uuid\n\n# Definimos un esquema para registrar m\u00faltiples eventos\nclass InteractionLog(TypedDict):\n    timestamp: str\n    user_message: str\n    bot_response: str\n\n# Instanciamos el almacenamiento en memoria\nstore = InMemoryStore()\n\n# Simulamos interacciones y las almacenamos en una colecci\u00f3n de datos\nuser_id = \"user_123\"\nnamespace = (\"history\", user_id)\n\n# Agregamos entradas al historial de conversaci\u00f3n\nkey = str(uuid.uuid4())\nstore.put(namespace, key=key, value=InteractionLog(\n    timestamp=str(datetime.now()),\n    user_message=\"\u00bfCu\u00e1l es la capital de Francia?\",\n    bot_response=\"La capital de Francia es Par\u00eds.\"\n))\n\nkey = str(uuid.uuid4())\nstore.put(namespace, key=key, value=InteractionLog(\n    timestamp=str(datetime.now()),\n    user_message=\"\u00bfQu\u00e9 tiempo hace hoy?\",\n    bot_response=\"No tengo acceso a datos en tiempo real, pero puedes consultarlo en un servicio meteorol\u00f3gico.\"\n))\n\n# Recuperamos el historial de conversaci\u00f3n\nchat_history = store.search(namespace)\n\n# Mostramos los mensajes almacenados\nfor m in chat_history:\n    print(m.dict())\n</code></pre> Resultado<pre><code>{'namespace': ['history', 'user_123'], 'key': '8b7b7f79-45e4-4d63-a890-a2c5120ff991', 'value': {'timestamp': '2025-02-06 10:03:27.270865', 'user_message': '\u00bfCu\u00e1l es la capital de Francia?', 'bot_response': 'La capital de Francia es Par\u00eds.'}, 'created_at': '2025-02-06T10:03:27.270936+00:00', 'updated_at': '2025-02-06T10:03:27.270939+00:00', 'score': None}\n{'namespace': ['history', 'user_123'], 'key': '72403e77-2919-4eb5-9b1b-ffa4108a9c0f', 'value': {'timestamp': '2025-02-06 10:03:27.271097', 'user_message': '\u00bfQu\u00e9 tiempo hace hoy?', 'bot_response': 'No tengo acceso a datos en tiempo real, pero puedes consultarlo en un servicio meteorol\u00f3gico.'}, 'created_at': '2025-02-06T10:03:27.271114+00:00', 'updated_at': '2025-02-06T10:03:27.271115+00:00', 'score': None}\n</code></pre> <p>Este esquema nos permite almacenar y recuperar datos de manera eficiente, facilitando la toma de decisiones basadas en informaci\u00f3n hist\u00f3rica.  </p>"},{"location":"curso3/tema2_memory_schema/#implementacion-completa","title":"\ud83d\udee0\ufe0f Implementaci\u00f3n Completa","text":"<p>Ahora combinaremos ambos esquemas en un grafo funcional, donde el sistema podr\u00e1 gestionar perfiles de usuario y almacenar registros de eventos dentro del mismo flujo.  </p> Importante <ul> <li>El Profile Schema nos permite definir atributos fijos de un usuario.  </li> <li>El Collection Schema es ideal para registrar eventos din\u00e1micos.  </li> <li>Ambos esquemas pueden coexistir dentro de una misma implementaci\u00f3n de memoria.  </li> </ul> <pre><code>from langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.store.base import BaseStore\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langchain_openai import ChatOpenAI\nfrom datetime import datetime\nfrom typing import TypedDict\nfrom typing_extensions import Annotated, TypedDict\nfrom IPython.display import Image\nimport uuid\n\n# Inicializamos el modelo de lenguaje (LLM) usando GPT-4o-mini \nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nMODEL_SYSTEM_MESSAGE = \"\"\"Eres un asistente \u00fatil con memoria que proporciona informaci\u00f3n personalizada sobre el usuario. \n    Si tienes memoria para este usuario, util\u00edzala para personalizar tus respuestas.\n    En caso de no tener memoria o estar incompleta, trata sutilmente, de hacer las preguntas claves para obtener la informaci\u00f3n necesaria (nombre, lugar de residencia y intereses).\n    Aqu\u00ed est\u00e1 la memoria (puede estar vac\u00eda o incompleta): {memory}\n\"\"\"\n\nUPDATE_MEMORY_INSTRUCTION = \"\"\"Est\u00e1s recopilando informaci\u00f3n sobre el usuario para personalizar tus respuestas.\n\nINFORMACI\u00d3N ACTUAL DEL USUARIO:\n{memory}\n\nINSTRUCCIONES:\n1. Revisa cuidadosamente el historial de chat a continuaci\u00f3n.\n2. Identifica nueva informaci\u00f3n sobre el usuario, como:\n   - Datos personales (nombre, ubicaci\u00f3n).\n   - Intereses y aficiones.\n   - Experiencias pasadas.\n   - Objetivos o planes futuros.\n3. Fusiona cualquier nueva informaci\u00f3n con la memoria existente.\n4. Formatea la memoria con el formato JSON proporcionado.\n5. Si la nueva informaci\u00f3n contradice la memoria existente, conserva la versi\u00f3n m\u00e1s reciente.\n\nRecuerda: Solo incluye informaci\u00f3n real directamente mencionada por el usuario. No hagas suposiciones ni inferencias.\n\nBasado en el historial de chat a continuaci\u00f3n, actualiza la informaci\u00f3n del usuario:\"\"\"\n\n# Definimos los esquemas de memoria\nclass UserProfile(TypedDict):\n    \"\"\" Memoria para almacenar el perfil del usuario \"\"\"\n    name: Annotated[str, ..., \"Nombre del usuario\"] \n    location: Annotated[str, ..., \"Lugar de residencia del usuario\"]  \n    interests: Annotated[list[str],..., \"Intereses y aficiones del usuario\"]\n\nclass InteractionLog(TypedDict):\n    timestamp: str\n    user_message: str\n    bot_response: str\n\n# Funci\u00f3n para gestionar la memoria del usuario\ndef manage_user_profile(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    namespace = (\"profile\", config[\"configurable\"][\"user_id\"])\n\n    # Intentamos recuperar el perfil existente\n    existing_profile = store.get(namespace, key=\"user_info\")\n\n    # Si no hay perfil, creamos uno nuevo\n    if not existing_profile:\n        profile_data = UserProfile(name=\"Desconocido\", location=\"Desconocida\", interests=[])\n        store.put(namespace, key=\"user_info\", value=profile_data)\n    return {}\n\n# Funci\u00f3n del asistente\ndef asistant(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    namespace = (\"profile\", config[\"configurable\"][\"user_id\"])\n    user_profile = store.get(namespace, key=\"user_info\")\n\n    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=user_profile.dict())\n    response = llm.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n\n    return {\"messages\": [response]}\n\n# Funci\u00f3n para actualizar informaci\u00f3n del usuario\ndef update_user_info(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    namespace = (\"profile\", config[\"configurable\"][\"user_id\"])\n\n    # Intentamos recuperar el perfil existente\n    existing_profile = store.get(namespace, key=\"user_info\")\n\n    # Le pedimos al llm que examine si hay alg\u00fan dato que actualizar en nuestra memoria\n    system_msg = UPDATE_MEMORY_INSTRUCTION.format(memory=existing_profile.dict())\n    llm_structured = llm.with_structured_output(UserProfile)\n    response = llm_structured.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n\n    print(response)\n\n    # Actualizamos la informaci\u00f3n del usuario\n    store.put(namespace,\n              key=\"user_info\",\n              value=UserProfile(\n                  name=response.get('properties').get('name'),\n                  location=response.get('properties').get('location'),\n                  interests=response.get('properties').get('interests')\n                  )\n              )\n\n    return {}\n\n# Funci\u00f3n para almacenar el historial de interacciones\ndef log_interaction(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    namespace = (\"history\", config[\"configurable\"][\"user_id\"])\n\n    # Extraemos el \u00faltimo mensaje del usuario y la respuesta de la IA\n    user_message = state[\"messages\"][-2].content if len(state[\"messages\"]) &gt; 1 else \"No disponible\"\n    bot_response = state[\"messages\"][-1].content if len(state[\"messages\"]) &gt; 0 else \"No disponible\"\n\n    # Guardamos la interacci\u00f3n en la memoria persistente\n    store.put(namespace, key=\"chat_history\", value=InteractionLog(\n        timestamp=str(datetime.now()),\n        user_message=user_message,\n        bot_response=bot_response\n    ))\n    return {}\n\n# Creamos el grafo\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"manage_profile\", manage_user_profile)\nbuilder.add_node(\"log_interaction\", log_interaction)\nbuilder.add_node(\"asistant\", asistant)\nbuilder.add_node(\"update_user_info\", update_user_info)\n\n# Definimos la estructura del flujo\nbuilder.add_edge(START, \"manage_profile\")\nbuilder.add_edge(\"manage_profile\", \"asistant\")\nbuilder.add_edge(\"asistant\", \"update_user_info\")\nbuilder.add_edge(\"asistant\", \"log_interaction\")\nbuilder.add_edge([\"update_user_info\",\"log_interaction\"], END)\n\n# Almacenamiento para la memoria a largo plazo (entre sesiones)\nacross_thread_memory = InMemoryStore()\n\n# Checkpointer para la memoria a corto plazo (dentro de una sesi\u00f3n)\nwithin_thread_memory = MemorySaver()\n\n# Compilamos el grafo con el almacenamiento\ngraph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\nImage(graph.get_graph().draw_mermaid_png())\n</code></pre> <p></p> <p>Ejecutamos el grafo.</p> <pre><code># Proporcionamos un ID de hilo para la memoria a corto plazo (dentro de la sesi\u00f3n)\n# Proporcionamos un ID de usuario para la memoria a largo plazo (entre sesiones)\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"cd3f791a-0a0c-4ea4-bb28-ea5176bd4450\"}}\n# Entrada del usuario \ninput_messages = [HumanMessage(content=\"Hola, me llamo Ra\u00fal y me gusta salir en bici.\")]\n\n# Ejecutamos el grafo y mostramos la respuesta del chatbot\nfor chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n    chunk[\"messages\"][-1].pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nHola, me llamo Ra\u00fal y me gusta salir en bici.\n================================== Ai Message ==================================\n\n\u00a1Hola, Ra\u00fal! Es genial conocerte. Salir en bici es una actividad muy divertida y saludable. \u00bfTienes rutas favoritas para recorrer en bicicleta? \u00bfO quiz\u00e1s alguna otra actividad que te guste?\n</code></pre> <p>Resvisamos la informaci\u00f3n generada y almacenada en nuestra memoria.</p> <pre><code>retrieved_profile = across_thread_memory.get(namespace=(\"profile\", \"cd3f791a-0a0c-4ea4-bb28-ea5176bd4450\"), key=\"user_info\")\nprint(retrieved_profile.dict())\n</code></pre> Resultado<pre><code>{\n   \"namespace\":[\n      \"profile\",\n      \"cd3f791a-0a0c-4ea4-bb28-ea5176bd4450\"\n   ],\n   \"key\":\"user_info\",\n   \"value\":{\n      \"name\":\"Ra\u00fal\",\n      \"location\":\"Desconocida\",\n      \"interests\":[\n         \"salir en bici\"\n      ]\n   },\n   \"created_at\":\"2025-02-06T12:38:08.354512+00:00\",\n   \"updated_at\":\"2025-02-06T12:38:08.354519+00:00\"\n}\n</code></pre> <p>En este ejemplo, el grafo gestiona perfiles de usuario y almacena un historial de interacciones, permitiendo un manejo estructurado y eficiente de la memoria.  </p>"},{"location":"curso3/tema2_memory_schema/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>Los Memory Schemas nos permiten estructurar y gestionar la informaci\u00f3n dentro de la memoria de LangGraph de manera eficiente.  </p> <p>\u2705 El Profile Schema es ideal para atributos est\u00e1ticos del usuario. \u2705 El Collection Schema nos ayuda a almacenar m\u00faltiples eventos organizados. \u2705 Ambos pueden combinarse para crear memorias m\u00e1s completas y funcionales.  </p> <p>\ud83d\ude80 Con estas herramientas, podemos construir sistemas m\u00e1s inteligentes y personalizados, optimizando la experiencia del usuario.  </p>"},{"location":"curso3/tema2_memory_schema/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>C\u00f3mo estructurar la memoria en LangGraph usando Profile y Collection Schema. </li> <li>Diferencias y casos de uso de ambos esquemas. </li> <li>Implementaci\u00f3n de memoria estructurada en un grafo real. </li> </ul>"},{"location":"curso3/tema2_memory_schema/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Definici\u00f3n: Memory</li> <li> Definici\u00f3n: Memory-store</li> <li> Class: BaseStore</li> <li> Wikipedia: Semantic-memory</li> </ul>"},{"location":"curso3/tema2_memory_schema/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos LangGraph Store, donde veremos c\u00f3mo almacenar y recuperar informaci\u00f3n de memoria a nivel avanzado.  </p>"},{"location":"curso3/tema3_langgraph_store/","title":"\ud83d\uddc4\ufe0f Tema 3: Store en LangGraph \u2013 Almacenamiento Persistente","text":""},{"location":"curso3/tema3_langgraph_store/#introduccion","title":"\ud83c\udf1f Introducci\u00f3n","text":"<p>Hasta ahora, hemos explorado diferentes tipos de memoria en LangGraph:  </p> <ul> <li>Memoria a Corto Plazo (Short-Term Memory) con <code>MemorySaver</code>, \u00fatil para almacenar informaci\u00f3n dentro de una misma sesi\u00f3n.  </li> <li>Memoria a Largo Plazo (Long-Term Memory) con <code>BaseStore</code>, que nos permite recordar datos entre sesiones.  </li> <li>Memory Schema, que nos ayuda a estructurar mejor los datos almacenados en la memoria a largo plazo.  </li> </ul> <p>Ahora, profundizaremos en el Store, un sistema de almacenamiento flexible en LangGraph que permite guardar, recuperar y gestionar datos de manera persistente.  </p>"},{"location":"curso3/tema3_langgraph_store/#que-es-basestore-y-por-que-es-importante","title":"\ud83e\udde0 \u00bfQu\u00e9 es <code>BaseStore</code> y por qu\u00e9 es importante?","text":"<p><code>BaseStore</code> es una clase base en LangGraph que define una interfaz com\u00fan para almacenar y recuperar datos. Es clave para aplicaciones que necesitan persistencia de datos, ya que permite:  </p> <p>\u2705 Guardar informaci\u00f3n entre sesiones, facilitando la memoria a largo plazo. \u2705 Organizar datos de manera estructurada usando namespaces y claves (<code>key-value</code>). \u2705 Realizar operaciones avanzadas como b\u00fasquedas o manipulaci\u00f3n en lote.  </p> <p>Este sistema es \u00fatil en escenarios como:  </p> <ul> <li>Chatbots con memoria persistente (recordar usuarios y sus preferencias).  </li> <li>Asistentes virtuales que necesitan retener informaci\u00f3n a largo plazo.  </li> <li>Aplicaciones empresariales que requieren almacenamiento de datos estructurado.  </li> </ul> Comparaci\u00f3n con <code>MemorySaver</code> <ul> <li><code>MemorySaver</code> almacena datos dentro de una \u00fanica sesi\u00f3n y se pierde al finalizar.  </li> <li><code>BaseStore</code> permite persistencia entre sesiones, almacenando datos de forma permanente en bases de datos o archivos.  </li> </ul>"},{"location":"curso3/tema3_langgraph_store/#metodos-principales-de-basestore","title":"\ud83d\udee0\ufe0f M\u00e9todos Principales de <code>BaseStore</code>","text":"<p>LangGraph proporciona varios m\u00e9todos esenciales para gestionar el almacenamiento:  </p>"},{"location":"curso3/tema3_langgraph_store/#putnamespace-key-value-guardar-datos","title":"\ud83d\udccc <code>put(namespace, key, value)</code> \u2013 Guardar Datos","text":"<p>Guarda un valor en un espacio de nombres (<code>namespace</code>) con una clave \u00fanica (<code>key</code>).  </p> <pre><code># Guardamos un dato en el Store\nstore.put(namespace=(\"usuarios\", \"user_123\"), key=\"perfil\", value={\"nombre\": \"Ra\u00fal\", \"edad\": 30})\n</code></pre>"},{"location":"curso3/tema3_langgraph_store/#getnamespace-key-recuperar-datos","title":"\ud83d\udccc <code>get(namespace, key)</code> \u2013 Recuperar Datos","text":"<p>Obtiene un valor almacenado a partir de su <code>namespace</code> y <code>key</code>.  </p> <pre><code># Recuperamos un dato almacenado\nperfil = store.get(namespace=(\"usuarios\", \"user_123\"), key=\"perfil\")\nprint(perfil.dict())  # {'nombre': 'Ra\u00fal', 'edad': 30}\n</code></pre>"},{"location":"curso3/tema3_langgraph_store/#searchnamespace-query-busqueda-de-datos","title":"\ud83d\udccc <code>search(namespace, query)</code> \u2013 B\u00fasqueda de Datos","text":"<p>Permite buscar valores dentro de un <code>namespace</code> utilizando una consulta espec\u00edfica.  </p> <pre><code># Buscamos informaci\u00f3n en el Store\nresultados = store.search(namespace=(\"usuarios\", \"user_123\"), query=\"Ra\u00fal\")\nprint(resultados)\n</code></pre>"},{"location":"curso3/tema3_langgraph_store/#batchnamespace-operations-operaciones-en-lote","title":"\ud83d\udccc <code>batch(namespace, operations)</code> \u2013 Operaciones en Lote","text":"<p>Ejecuta m\u00faltiples operaciones (<code>put</code>, <code>get</code>, <code>delete</code>) en una sola transacci\u00f3n.  </p> <pre><code># Realizamos operaciones en lote\noperaciones = [\n    (\"put\", \"user_123\", {\"nombre\": \"Ra\u00fal\", \"edad\": 30}),\n    (\"put\", \"user_456\", {\"nombre\": \"Elena\", \"edad\": 25}),\n    (\"get\", \"user_123\"),\n    (\"delete\", \"user_456\"),\n]\nstore.batch(namespace=\"usuarios\", operations=operaciones)\n</code></pre>"},{"location":"curso3/tema3_langgraph_store/#deletenamespace-key-eliminar-datos","title":"\ud83d\udccc <code>delete(namespace, key)</code> \u2013 Eliminar Datos","text":"<p>Elimina un valor almacenado a partir de su <code>namespace</code> y <code>key</code>.  </p> <pre><code># Eliminamos un dato del Store\nstore.delete(namespace=(\"usuarios\", \"user_123\"), key=\"perfil\")\n</code></pre>"},{"location":"curso3/tema3_langgraph_store/#ejemplo-practico-implementando-basestore-en-un-grafo","title":"\ud83c\udfd7\ufe0f Ejemplo Pr\u00e1ctico: Implementando <code>BaseStore</code> en un Grafo","text":"<p>Veamos c\u00f3mo podemos utilizar <code>BaseStore</code> en un flujo real dentro de LangGraph.  </p> <p>\ud83d\udccc Objetivo: - Guardar informaci\u00f3n de usuario en el Store. - Recuperar y actualizar datos en futuras ejecuciones.  </p> <pre><code>from langgraph.store.memory import InMemoryStore\n\n# Creamos un almacenamiento en memoria\nstore = InMemoryStore()\n\n# Definimos un ID de usuario\nuser_id = \"user_123\"\n\n# Guardamos datos en el Store\nstore.put(namespace=(\"usuarios\", user_id), key=\"perfil\", value={\"nombre\": \"Ra\u00fal\", \"edad\": 30})\n\n# Recuperamos los datos\nperfil = store.get(namespace=(\"usuarios\", user_id), key=\"perfil\")\nprint(perfil.dict())  # {'nombre': 'Ra\u00fal', 'edad': 30}\n</code></pre> <p>Al ejecutar el flujo, podemos observar c\u00f3mo el Store mantiene los datos entre sesiones, permitiendo un comportamiento m\u00e1s din\u00e1mico y personalizado.  </p>"},{"location":"curso3/tema3_langgraph_store/#integracion-de-basestore-con-una-base-de-datos-externa","title":"\ud83d\udee2\ufe0f Integraci\u00f3n de <code>BaseStore</code> con una Base de Datos Externa","text":"<p>Hasta ahora, hemos visto c\u00f3mo utilizar <code>BaseStore</code> con almacenamiento en memoria (<code>InMemoryStore</code>). Sin embargo, en aplicaciones del mundo real, es com\u00fan persistir los datos en bases de datos externas para asegurar que la informaci\u00f3n se mantenga disponible incluso despu\u00e9s de reinicios del sistema.  </p> <p>LangGraph permite integrar <code>BaseStore</code> con diferentes tipos de almacenamiento externo, como:  </p> <p>\u2705 Bases de datos SQL (PostgreSQL, MySQL, SQLite). \u2705 Bases de datos NoSQL (MongoDB, Redis, Firebase). \u2705 Sistemas de almacenamiento en la nube (Amazon S3, Google Cloud Storage).  </p> <p>Esto es ideal para chatbots empresariales, asistentes virtuales y cualquier aplicaci\u00f3n que necesite recordar informaci\u00f3n a lo largo del tiempo.  </p> Ventajas de usar una base de datos externa <ul> <li>Mayor persistencia: Los datos no se pierden entre sesiones o reinicios.  </li> <li>Escalabilidad: Capacidad para manejar grandes vol\u00famenes de datos.  </li> <li>Acceso desde m\u00faltiples instancias: Ideal para sistemas distribuidos o aplicaciones web.  </li> </ul>"},{"location":"curso3/tema3_langgraph_store/#ejemplo-conectando-basestore-con-postgresql","title":"\ud83d\udd17 Ejemplo: Conectando <code>BaseStore</code> con PostgreSQL","text":"<p>A continuaci\u00f3n, veremos c\u00f3mo integrar <code>BaseStore</code> con PostgreSQL utilizando <code>psycopg2</code> como cliente de base de datos.  </p> <p>\ud83d\udccc Pasos a seguir: 1\ufe0f\u20e3 Configurar la conexi\u00f3n a la base de datos. 2\ufe0f\u20e3 Implementar una clase <code>PostgresStore</code> que extienda <code>BaseStore</code>. 3\ufe0f\u20e3 Utilizar <code>put()</code>, <code>get()</code>, <code>delete()</code> y otros m\u00e9todos con PostgreSQL. </p> Atenci\u00f3n <p>Asegurate de tener instalada la extensi\u00f3n <code>langgraph-checkpoint-postgres</code> de lo contrario no funcionar\u00e1. <pre><code>pip install langgraph-checkpoint-postgres\n</code></pre></p> Alternativa 1<pre><code>from langgraph.store.postgres import PostgresStore\nfrom psycopg import Connection\n\nconn_string = \"postgresql://user:pass@localhost:5432/dbname\"\n\n# Usando conexi\u00f3n directa\nwith Connection.connect(conn_string) as conn:\n    store = PostgresStore(conn)\n    store.setup() # Ejecutar migraciones. Se realiza una sola vez\n</code></pre> Alternativa 2<pre><code>from langgraph.store.postgres import PostgresStore\n\nconn_string = \"postgresql://user:pass@localhost:5432/dbname\"\n\nwith PostgresStore.from_conn_string(conn_string) as store:\n    store.setup()\n</code></pre> Precuaci\u00f3n <p>Asegurate de llamar a <code>setup()</code> antes del primer uso para crear las tablas y los \u00edndices necesarios. La extensi\u00f3n pgvector debe estar disponible para utilizar la b\u00fasqueda vectorial.</p> <p>Ahora, en lugar de guardar datos en memoria, estaremos almacenando y recuperando la informaci\u00f3n directamente desde PostgreSQL.  </p>"},{"location":"curso3/tema3_langgraph_store/#ejemplo-de-uso-guardar-y-recuperar-datos-en-postgresql","title":"\ud83c\udfaf Ejemplo de Uso: Guardar y Recuperar Datos en PostgreSQL","text":"<p>Ahora que tenemos nuestra implementaci\u00f3n de <code>PostgresStore</code>, podemos integrarla en un grafo y realizar operaciones b\u00e1sicas como:  </p> <p>\u2705 Guardar informaci\u00f3n del usuario en la base de datos. \u2705 Recuperar informaci\u00f3n previamente almacenada. \u2705 Eliminar registros de la base de datos. </p> <pre><code># Guardamos un dato en PostgreSQL\nstore.put((\"users\", \"123\"), \"prefs\", {\"theme\": \"dark\"})\n\n# Recuperamos el dato\nitem = store.get((\"users\", \"123\"), \"prefs\")\nprint(item)\n</code></pre> Resultado<pre><code>Item(namespace=['users', '123'], key='prefs', value={'theme': 'dark'}, created_at='2025-02-08T17:46:23.643568+00:00', updated_at='2025-02-08T17:46:23.643568+00:00')\n</code></pre> <p>Este enfoque nos permite combinar la flexibilidad de LangGraph con la robustez de una base de datos externa, asegurando que los datos sean accesibles en m\u00faltiples sesiones y dispositivos.  </p>"},{"location":"curso3/tema3_langgraph_store/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>El Store en LangGraph es una herramienta clave para gestionar y persistir datos en nuestros grafos.  </p> <p>\u2705 Permite almacenamiento a largo plazo, evitando la p\u00e9rdida de informaci\u00f3n entre sesiones. \u2705 Facilita la recuperaci\u00f3n y manipulaci\u00f3n de datos con operaciones avanzadas. \u2705 Se puede integrar con bases de datos externas, brindando escalabilidad y persistencia real.  </p> <p>Si queremos que nuestros sistemas recuerden informaci\u00f3n a largo plazo de manera eficiente, usar <code>BaseStore</code> con una base de datos externa es la mejor opci\u00f3n. \ud83d\ude80  </p>"},{"location":"curso3/tema3_langgraph_store/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>C\u00f3mo funciona <code>BaseStore</code> y sus m\u00e9todos clave (<code>put</code>, <code>get</code>, <code>search</code>, <code>batch</code>, <code>delete</code>). </li> <li>Diferencias entre <code>MemorySaver</code>, <code>InMemoryStore</code> y <code>BaseStore</code>. </li> <li>C\u00f3mo integrar <code>BaseStore</code> con una base de datos externa como PostgreSQL. </li> <li>Ventajas de usar almacenamiento persistente para chatbots, asistentes y sistemas empresariales. </li> </ul>"},{"location":"curso3/tema3_langgraph_store/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Definici\u00f3n: Store</li> <li> Definici\u00f3n: BaseStore</li> <li> Definici\u00f3n: PostgresStore</li> <li> Definici\u00f3n: Memory-store</li> <li> Definici\u00f3n: Semantic-search</li> <li> Definici\u00f3n: Semantic-memory</li> </ul>"},{"location":"curso3/tema3_langgraph_store/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos c\u00f3mo integrar bases de datos externas con Store, optimizando a\u00fan m\u00e1s el almacenamiento en nuestros grafos.  </p>"},{"location":"curso3/tema4_langgraph_cli/","title":"\u2699\ufe0f Tema 4: LangGraph CLI \u2013 Comandos y Uso","text":""},{"location":"curso3/tema4_langgraph_cli/#que-es-langgraph-cli","title":"\ud83d\ude80 \u00bfQu\u00e9 es LangGraph CLI?","text":"<p>LangGraph CLI es una herramienta de l\u00ednea de comandos que facilita la creaci\u00f3n, ejecuci\u00f3n y despliegue de servidores de LangGraph. Con esta interfaz, podemos gestionar nuestros proyectos sin necesidad de escribir c\u00f3digo manualmente.  </p>"},{"location":"curso3/tema4_langgraph_cli/#instalacion-de-langgraph-cli","title":"\ud83d\udee0\ufe0f Instalaci\u00f3n de LangGraph CLI","text":"<p>Para comenzar a usar LangGraph CLI, primero debemos instalarlo en nuestro entorno de desarrollo.  </p> <p>\ud83d\udccc Instalaci\u00f3n: </p> <pre><code>pip install -U langgraph-cli\n</code></pre> Atenci\u00f3n <p>Para utilizar <code>langgraph dev</code>, es necesario instalar dependencias adicionales. Usa el siguiente comando: <pre><code>pip install -U \"langgraph-cli[inmem]\"\n</code></pre></p>"},{"location":"curso3/tema4_langgraph_cli/#comandos-principales-de-langgraph-cli","title":"\ud83d\udcdd Comandos Principales de LangGraph CLI","text":"Comando Descripci\u00f3n <code>langgraph new</code> \ud83c\udf31 Crea un nuevo proyecto de LangGraph a partir de una plantilla. <code>langgraph build</code> \ud83d\udce6 Genera una imagen Docker lista para desplegar el servidor. <code>langgraph up</code> \ud83d\ude80 Inicia el servidor de LangGraph en producci\u00f3n. <code>langgraph dev</code> \ud83c\udfc3\u200d\u2640\ufe0f Ejecuta el servidor en modo desarrollo, con recarga autom\u00e1tica. <code>langgraph dockerfile</code> \ud83d\udc33 Genera un Dockerfile personalizado para el servidor. <p>Cada uno de estos comandos cumple una funci\u00f3n espec\u00edfica dentro del ciclo de vida de un proyecto de LangGraph.  </p>"},{"location":"curso3/tema4_langgraph_cli/#cuando-usar-cada-comando","title":"\ud83c\udfaf \u00bfCu\u00e1ndo Usar Cada Comando?","text":"<p>Dependiendo del entorno en el que estemos trabajando, podemos utilizar diferentes comandos para ejecutar LangGraph. Distinguimos dos modos principales: desarrollo y producci\u00f3n.  </p>"},{"location":"curso3/tema4_langgraph_cli/#modo-desarrollo-testing-y-pruebas-rapidas","title":"\ud83d\udd39 Modo Desarrollo (Testing y Pruebas R\u00e1pidas)","text":"<p>\ud83d\udccc Para ejecutar un servidor en modo desarrollo, sin necesidad de Docker y con recarga autom\u00e1tica:  </p> <pre><code>langgraph dev --port 2024 # El puerto no es necesario pero si os falla por defecto probad este.\n</code></pre> Resultado<pre><code>INFO:langgraph_api.cli:\n\n        Welcome to\n\n\u2566  \u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u2554\u2550\u2557\u252c\u2500\u2510\u250c\u2500\u2510\u250c\u2500\u2510\u252c \u252c\n\u2551  \u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u252c\u2551 \u2566\u251c\u252c\u2518\u251c\u2500\u2524\u251c\u2500\u2518\u251c\u2500\u2524\n\u2569\u2550\u255d\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u255a\u2550\u255d\u2534\u2514\u2500\u2534 \u2534\u2534  \u2534 \u2534\n\n- \ud83d\ude80 API: http://127.0.0.1:2024\n- \ud83c\udfa8 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n- \ud83d\udcda API Docs: http://127.0.0.1:2024/docs\n\nThis in-memory server is designed for development and testing.\nFor production use, please use LangGraph Cloud.\n</code></pre> Ejemplo <p>Si estamos creando un chatbot en LangGraph y queremos probar cambios sin reiniciar manualmente, usaremos <code>langgraph dev</code>.  </p> <p>Cuando ejecutamos este comando, se generan tres puntos clave de interacci\u00f3n:  </p> <ul> <li>API: URL local para realizar llamadas directas a nuestro grafo mediante peticiones HTTP.  </li> <li>Studio UI: Interfaz gr\u00e1fica accesible desde el navegador que permite visualizar y depurar nuestro grafo sin necesidad de instalar herramientas adicionales.  </li> </ul> <p></p> <ul> <li>API Docs: Documentaci\u00f3n interactiva de la API, donde podemos explorar los endpoints disponibles junto con ejemplos de uso.  </li> </ul> <p></p> <p>Esto facilita la experimentaci\u00f3n y depuraci\u00f3n en tiempo real, haciendo m\u00e1s \u00e1gil el desarrollo.  </p> Atenci\u00f3n <p>Es posible que, en la primera ejecuci\u00f3n, se presenten errores relacionados con la conexi\u00f3n a LangSmith o la configuraci\u00f3n de la clave de OpenAI.  </p> <p>Para solucionarlo, es recomendable instalar python-dotenv: <pre><code>pip -U install python-dotenv\n</code></pre> Luego, debemos crear un archivo <code>.env</code> en la carpeta del proyecto con las credenciales necesarias: <pre><code>OPENAI_API_KEY=\"&lt;CLAVE_OPENAI&gt;\"\nLANGSMITH_API_KEY=\"&lt;CLAVE_LANGSMITH&gt;\"\n#LANGCHAIN_ENDPOINT=\"https://eu.api.smith.langchain.com/\" # Descomenta esta linea si usas el servidor de EU\nLANGSMITH_PROJECT=\"&lt;NOMBRE_PROYECTO&gt;\"\n</code></pre> \u26a0 IMPORTANTE: Nunca subas el archivo <code>.env</code> al repositorio para evitar exponer informaci\u00f3n sensible.  </p> Nota <p>Un beneficio adicional de este comando es que se sincroniza autom\u00e1ticamente con tu cuenta de LangSmith, lo que permite rastrear y analizar el flujo de ejecuci\u00f3n de tu grafo en tiempo real.  </p>"},{"location":"curso3/tema4_langgraph_cli/#modo-produccion-despliegue-final","title":"\ud83d\udd39 Modo Producci\u00f3n (Despliegue Final)","text":"<p>\ud83d\udccc Para construir una imagen de Docker antes de desplegar:  </p> <pre><code>langgraph build \n</code></pre> <p>\ud83d\udccc Para generar un Dockerfile personalizado:  </p> <pre><code>langgraph dockerfile \n</code></pre> <p>\ud83d\udccc Para levantar el servidor en producci\u00f3n:  </p> <pre><code>langgraph up\n</code></pre> Recomendaci\u00f3n <ul> <li>Usa <code>langgraph up</code> si deseas ejecutar el servidor sin Docker.  </li> <li>Usa <code>langgraph build</code> y <code>dockerfile</code> si prefieres un despliegue m\u00e1s controlado con Docker.  </li> </ul>"},{"location":"curso3/tema4_langgraph_cli/#ejemplo-practico-creando-y-ejecutando-un-servidor","title":"\ud83d\udee0\ufe0f Ejemplo Pr\u00e1ctico: Creando y Ejecutando un Servidor","text":"<p>Veamos un flujo de trabajo com\u00fan para crear y ejecutar un servidor con LangGraph CLI.  </p>"},{"location":"curso3/tema4_langgraph_cli/#1-creamos-un-nuevo-proyecto","title":"1\ufe0f\u20e3 Creamos un nuevo proyecto:","text":"<pre><code>langgraph new mi_proyecto &amp;&amp; cd mi_proyecto\n</code></pre>"},{"location":"curso3/tema4_langgraph_cli/#2-ejecutamos-el-servidor-en-modo-desarrollo","title":"2\ufe0f\u20e3 Ejecutamos el servidor en modo desarrollo:","text":"<pre><code>langgraph dev --port 2024\n</code></pre>"},{"location":"curso3/tema4_langgraph_cli/#3-si-todo-funciona-bien-lo-preparamos-para-produccion","title":"3\ufe0f\u20e3 Si todo funciona bien, lo preparamos para producci\u00f3n:","text":"<pre><code>langgraph build -t calculator # -t: El nombre de la imagen que se generar\u00e1\n</code></pre> Resultado<pre><code>(langgraph) &gt; langgraph-server % docker images\nREPOSITORY                  TAG       IMAGE ID       CREATED              SIZE\ncalculator                  latest    f13bafae5b87   About a minute ago   897MB\n</code></pre> <pre><code>langgraph dockerfile --add-docker-compose -c langgraph.json ./dockerfile\n</code></pre> Resultado<pre><code>\ud83d\udd0d Validating configuration at path: /Users/tu-usuario/langgraph-server/langgraph.json\n\u2705 Configuration validated!\n\ud83d\udcdd Generating Dockerfile at /Users/tu-usuario/langgraph-server/dockerfile\n\u2705 Created: Dockerfile\n\u2705 Created: .dockerignore\n\u2705 Created: docker-compose.yml\n\u2796 Skipped: .env. It already exists!\n\ud83c\udf89 Files generated successfully at path /Users/tu-usuario/langgraph-server!\n</code></pre>"},{"location":"curso3/tema4_langgraph_cli/#4-levantamos-el-servidor-en-produccion","title":"4\ufe0f\u20e3 Levantamos el servidor en producci\u00f3n:","text":"<pre><code>langgraph up -p 8800 # Podeis ajustar el puerto al que mejor os funcione.\n</code></pre> Resultado<pre><code>Starting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\nReady!       \n- API: http://localhost:8800\n- Docs: http://localhost:8800/docs\n- LangGraph Studio: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8800\n</code></pre> <p>\ud83d\udccc \u00a1Y listo! Con estos pasos, hemos pasado de desarrollo a producci\u00f3n con LangGraph CLI.  </p>"},{"location":"curso3/tema4_langgraph_cli/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>LangGraph CLI nos permite gestionar f\u00e1cilmente nuestros servidores sin necesidad de configuraciones manuales.  </p> <p>\u2705 Nos ayuda a crear y estructurar proyectos de LangGraph r\u00e1pidamente. \u2705 Facilita la ejecuci\u00f3n en entornos de desarrollo y producci\u00f3n. \u2705 Permite desplegar servidores de forma eficiente con Docker. </p> <p>Si trabajamos con grafos complejos, esta herramienta se vuelve esencial para simplificar el proceso de desarrollo y despliegue. \ud83d\ude80  </p>"},{"location":"curso3/tema4_langgraph_cli/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>C\u00f3mo instalar y usar LangGraph CLI en distintos entornos. </li> <li>Qu\u00e9 comandos usar para desarrollo y producci\u00f3n. </li> <li>C\u00f3mo ejecutar y desplegar un servidor de LangGraph. </li> </ul>"},{"location":"curso3/tema4_langgraph_cli/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos LangGraph SDK, la herramienta que nos permitir\u00e1 conectar, ejecutar y administrar grafos de manera program\u00e1tica. Aprenderemos a interactuar con nuestros grafos desde c\u00f3digo, gestionar ejecuciones (runs) y aprovechar funcionalidades avanzadas como streaming e intervenci\u00f3n humana.  </p>"},{"location":"curso3/tema5_langgraph_sdk/","title":"\ud83d\udda5\ufe0f Tema 5: Uso de la SDK de LangGraph","text":""},{"location":"curso3/tema5_langgraph_sdk/#introduccion","title":"\ud83c\udf1f Introducci\u00f3n","text":"<p>En el tema anterior, aprendimos a usar LangGraph CLI para iniciar y administrar nuestros grafos de manera local o en contenedores Docker. Ahora, daremos el siguiente paso: usar la SDK de LangGraph para interactuar con nuestro grafo de manera program\u00e1tica.  </p> <p>\ud83d\udccc \u00bfPor qu\u00e9 usar la SDK de LangGraph? - Permite conectarse a grafos remotos y ejecutarlos desde c\u00f3digo. - Facilita la gesti\u00f3n de ejecuciones (runs) y sesiones (threads). - Proporciona herramientas para recuperar y almacenar datos dentro de LangGraph. - Nos da acceso a caracter\u00edsticas avanzadas como streaming, Human-in-the-loop y persistencia de datos.  </p> <p>\ud83d\udd17 Referencia oficial: Documentaci\u00f3n de la SDK </p> <pre><code>pip install langgraph-sdk\n</code></pre>"},{"location":"curso3/tema5_langgraph_sdk/#1-remote-graph-ejecutar-un-grafo-de-forma-remota","title":"\ud83d\udee0\ufe0f 1. Remote Graph \u2013 Ejecutar un Grafo de Forma Remota","text":"<p>LangGraph nos permite ejecutar nuestros grafos en un servidor remoto, en lugar de hacerlo localmente. Para lograrlo, podemos optar por dos m\u00e9todos: 1\ufe0f\u20e3 Usar la SDK con <code>get_client()</code> 2\ufe0f\u20e3 Utilizar la clase <code>RemoteGraph</code> para interactuar con el grafo remoto directamente.  </p> <p>\ud83d\udccc Ejemplo de conexi\u00f3n a un grafo remoto: </p> Opci\u00f3n 1: client<pre><code>from langgraph_sdk import get_client\n\n# Connect via SDK\nurl_for_cli_deployment = \"http://localhost:8123\"\nclient = get_client(url=url_for_cli_deployment)\n</code></pre> Opci\u00f3n 2: RemoteGraph<pre><code>from langgraph.client import RemoteGraph\n\n# Especificamos que grafo queremos usar\ngraph_name = &lt;NOMBRE_DEL_GRAFO&gt;\n\n# Conectamos con el grafo remoto en LangGraph Cloud o servidor propio\ngraph = RemoteGraph(graph_name, url=\"http://localhost:8080\")\n\n# Definimos la entrada para el grafo\ninput_data = {\"messages\": [\"Hola, \u00bfc\u00f3mo est\u00e1s?\"]}\n\n# Ejecutamos el grafo y obtenemos la respuesta\nresponse = graph.invoke(input_data)\n\n# Mostramos la respuesta generada\nprint(response)\n</code></pre> <p>\ud83d\udd39 En este ejemplo: 1. Nos conectamos a un servidor remoto que ejecuta nuestro grafo. 2. Enviamos una entrada y obtenemos una respuesta. 3. Podemos utilizarlo de la misma manera que un <code>StateGraph</code> local.  </p>"},{"location":"curso3/tema5_langgraph_sdk/#2-runs-gestion-de-ejecuciones-en-langgraph","title":"\ud83d\udd04 2. Runs \u2013 Gesti\u00f3n de Ejecuciones en LangGraph","text":"<p>Un run es una instancia de ejecuci\u00f3n de un grafo. Cada vez que ejecutamos un grafo, se crea un nuevo run, que podemos gestionar mediante la SDK.  </p> <p>Un background run se ejecuta en segundo plano, permitiendo consultar su estado m\u00e1s adelante. existen dos tipos: - Fire and forget - Ejecutamos un run background y no esperamos a que acabe. - Esperamos la respuesta (blocking or polling) - Ejecutamos un run y esperamos a que responda.  </p> <p>\ud83d\udccc Ejemplo: Crear y recuperar un background run en LangGraph </p> <pre><code>from langgraph_sdk import get_client\nfrom langchain_core.messages import HumanMessage\n\n# Connect via SDK\nurl_for_cli_deployment = \"http://localhost:8123\"\nclient = get_client(url=url_for_cli_deployment)\n\n# Creamos un nuevo thread\nthread = await client.threads.create()\n\n# Iniciamos un nuevo run dentro del thread\nuser_input = \"Realizame esta operaci\u00f3n 5*2.\"\nconfig = {\"configurable\": {\"user_id\": \"Test\"}}\ngraph_name = \"calculator\" \nrun = await client.runs.create(thread[\"thread_id\"], graph_name, input={\"messages\": [HumanMessage(content=user_input)]}, config=config)\n\n# Mostramos el estado de ejecuciones en el thread\nprint(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))\n</code></pre> Resultado<pre><code>{\n   \"run_id\":\"1efedf2f-724e-60a1-92b6-19db6d246e4a\",\n   \"thread_id\":\"8cb32afa-c3a9-4640-9e07-fedce27cc892\",\n   \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\",\n   \"created_at\":\"2025-02-18T12:22:05.633972+00:00\",\n   \"updated_at\":\"2025-02-18T12:22:05.633972+00:00\",\n   \"metadata\":{\n      \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\"\n   },\n   \"status\":\"running\",\n   \"kwargs\":{\n      \"input\":{\n         \"messages\":[\n            {\n               \"id\":\"None\",\n               \"name\":\"None\",\n               \"type\":\"human\",\n               \"content\":\"Realizame esta operaci\u00f3n 5*2.\",\n               \"example\":false,\n               \"additional_kwargs\":{\n\n               },\n               \"response_metadata\":{\n\n               }\n            }\n         ]\n      },\n      \"config\":{\n         \"metadata\":{\n            \"created_by\":\"system\",\n            \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\"\n         },\n         \"configurable\":{\n            \"run_id\":\"1efedf2f-724e-60a1-92b6-19db6d246e4a\",\n            \"user_id\":\"Test\",\n            \"graph_id\":\"calculator\",\n            \"thread_id\":\"8cb32afa-c3a9-4640-9e07-fedce27cc892\",\n            \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\",\n            \"langgraph_auth_user\":\"None\",\n            \"langgraph_auth_user_id\":\"\",\n            \"langgraph_auth_permissions\":[\n\n            ]\n         }\n      },\n      \"command\":\"None\",\n      \"webhook\":\"None\",\n      \"subgraphs\":false,\n      \"temporary\":false,\n      \"stream_mode\":[\n         \"values\"\n      ],\n      \"feedback_keys\":\"None\",\n      \"interrupt_after\":\"None\",\n      \"interrupt_before\":\"None\"\n   },\n   \"multitask_strategy\":\"reject\"\n}\n</code></pre> Nota <p>Si el estado del run aparece como <code>pending</code> o <code>running</code>, significa que el grafo a\u00fan est\u00e1 en ejecuci\u00f3n.  </p> <p>\ud83d\udccc Si queremos esperar el resultado del run antes de continuar: </p> <p>En caso de quisieramos esperar a que el run termine (blocking or polling) para mostrar el resultado podemos usar <code>client.runs.join</code>.</p> <pre><code># Esperamos a que termine y mostramos el output\nawait client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\nprint(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))\n</code></pre> <pre><code>{\n   \"run_id\":\"1efedf2f-724e-60a1-92b6-19db6d246e4a\",\n   \"thread_id\":\"8cb32afa-c3a9-4640-9e07-fedce27cc892\",\n   \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\",\n   \"created_at\":\"2025-02-18T12:22:05.633972+00:00\",\n   \"updated_at\":\"2025-02-18T12:22:05.633972+00:00\",\n   \"metadata\":{\n      \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\"\n   },\n   \"status\":\"success\",\n   \"kwargs\":{\n      \"input\":{\n         \"messages\":[\n            {\n               \"id\":\"None\",\n               \"name\":\"None\",\n               \"type\":\"human\",\n               \"content\":\"Realizame esta operaci\u00f3n 5*2.\",\n               \"example\":false,\n               \"additional_kwargs\":{\n\n               },\n               \"response_metadata\":{\n\n               }\n            }\n         ]\n      },\n      \"config\":{\n         \"metadata\":{\n            \"created_by\":\"system\",\n            \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\"\n         },\n         \"configurable\":{\n            \"run_id\":\"1efedf2f-724e-60a1-92b6-19db6d246e4a\",\n            \"user_id\":\"Test\",\n            \"graph_id\":\"calculator\",\n            \"thread_id\":\"8cb32afa-c3a9-4640-9e07-fedce27cc892\",\n            \"assistant_id\":\"0d1ff216-6054-5926-941a-33f750dff0a5\",\n            \"langgraph_auth_user\":\"None\",\n            \"langgraph_auth_user_id\":\"\",\n            \"langgraph_auth_permissions\":[\n\n            ]\n         }\n      },\n      \"command\":\"None\",\n      \"webhook\":\"None\",\n      \"subgraphs\":false,\n      \"temporary\":false,\n      \"stream_mode\":[\n         \"values\"\n      ],\n      \"feedback_keys\":\"None\",\n      \"interrupt_after\":\"None\",\n      \"interrupt_before\":\"None\"\n   },\n   \"multitask_strategy\":\"reject\"\n}\n</code></pre>"},{"location":"curso3/tema5_langgraph_sdk/#3-streaming-runs-recibir-resultados-en-tiempo-real","title":"\ud83d\udd00 3. Streaming Runs \u2013 Recibir Resultados en Tiempo Real","text":"<p>En algunos casos, queremos recibir respuestas parciales mientras el grafo se ejecuta. Para ello, LangGraph SDK nos permite hacer streaming de runs.  </p> <p>\ud83d\udccc Ejemplo de streaming de un run: </p> <pre><code># Creamos una ejecuci\u00f3n en streaming\nuser_input = \"Realizame esta operaci\u00f3n ((4+2)*2)/2.\"\nasync for chunk in client.runs.stream(thread[\"thread_id\"], \n                                      graph_name, \n                                      input={\"messages\": [HumanMessage(content=user_input)]},\n                                      config=config,\n                                      stream_mode=\"messages-tuple\"):\n\n    # Filtramos que el evento sea \"messages\" y que el tipo de dato sera \"AIMessageChunk\" que es la respuesta de la ia\n    if chunk.event == \"messages\":\n        print(\"\".join(\n            data_item['content'] \n            for data_item in chunk.data \n            if data_item.get(\"type\") == \"AIMessageChunk\" and 'content' in data_item\n            ), end=\"\", flush=True)\n</code></pre> Resultado<pre><code>La operaci\u00f3n \\(((4+2) \\times 2) / 2\\) es igual a \\(6\\).\n</code></pre> <p>\ud83d\udd39 Ventajas del streaming: \u2705 Nos permite visualizar resultados en tiempo real. \u2705 Es \u00fatil en asistentes conversacionales que generan respuesta progresivamente. \u2705 Reduce la latencia en respuestas largas.  </p> Note <p>Para obtener los token en stream debemos de usar <code>stream_mode=\"messages-tuple\"</code> </p>"},{"location":"curso3/tema5_langgraph_sdk/#4-threads-organizacion-de-conversaciones-y-runs","title":"\ud83e\uddf5 4. Threads \u2013 Organizaci\u00f3n de Conversaciones y Runs","text":"<p>Un Thread en LangGraph agrupa m\u00faltiples runs dentro de una misma conversaci\u00f3n o contexto.  </p> <p>\ud83d\udccc Ejemplo: Crear y administrar un Thread en LangGraph </p> <pre><code>from langchain_core.messages import convert_to_messages\n\nthread_state = await client.threads.get_state(thread['thread_id'])\nfor m in convert_to_messages(thread_state['values']['messages']):\n    m.pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nRealizame esta operaci\u00f3n ((4+2)*2)/2.\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_p75b81d9KwOL75B1MLBCqfDF)\n Call ID: call_p75b81d9KwOL75B1MLBCqfDF\n  Args:\n    a: 4\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n6\n================================== Ai Message ==================================\nTool Calls:\n  multiplicar (call_iiFyJDKknNcSp7RmRL38Elld)\n Call ID: call_iiFyJDKknNcSp7RmRL38Elld\n  Args:\n    a: 6\n    b: 2\n================================= Tool Message =================================\nName: multiplicar\n\n12\n================================== Ai Message ==================================\nTool Calls:\n  dividir (call_UzMKbkQoMkx3j7m5IjDvEewc)\n Call ID: call_UzMKbkQoMkx3j7m5IjDvEewc\n  Args:\n    a: 12\n    b: 2\n================================= Tool Message =================================\nName: dividir\n\n6.0\n================================== Ai Message ==================================\n\nLa operaci\u00f3n \\(((4+2) \\times 2) / 2\\) es igual a \\(6\\).\n</code></pre> <p>\ud83d\udccc Ejemplo: Listar Threads en LangGraph </p> <pre><code># Listamos todos los threads disponibles\nthreads = await client.threads.search()\n\nfor t in threads:\n    print(t)\n</code></pre> <p>\ud83d\udccc Ejemplo: ELiminar Threads en LangGraph </p> <pre><code>await client.threads.delete(\n    thread_id=\"&lt;THREAD_ID&gt;\"\n)\n</code></pre> <p>\ud83d\udd39 Diferencia clave entre threads y runs: - Un thread agrupa m\u00faltiples ejecuciones relacionadas. - Un run es una \u00fanica ejecuci\u00f3n dentro de un thread.  </p> <p>\ud83d\udd39 Casos de uso: \u2705 Chatbots que necesitan agrupar interacciones de un usuario en una misma sesi\u00f3n. \u2705 Flujos conversacionales complejos donde varios runs deben estar conectados.  </p>"},{"location":"curso3/tema5_langgraph_sdk/#5-human-in-the-loop-integracion-de-intervencion-humana","title":"\ud83d\udc69\u200d\ud83d\udcbb 5. Human in the Loop \u2013 Integraci\u00f3n de Intervenci\u00f3n Humana","text":"<p>Como vimos en el Curso 2, Tema 4: Human in the Loop, LangGraph permite pausar el flujo del grafo y solicitar intervenci\u00f3n humana.  </p> <p>\ud83d\udccc Ejemplo: Usar Human in the Loop con la SDK </p> <pre><code># Una vez detenido por la iterrupci\u00f3n del grafo, volveriamos a ejecutarlo de la siguiente manera \nasync for chunk in client.runs.stream(thread[\"thread_id\"], \n                                      graph_name, \n                                      input=Command(resume=\"SI\"),\n                                      config=config,                                      \n                                      stream_mode=\"messages-tuple\"):\n\n    if chunk.event == \"messages\":\n        print(\"\".join(data_item['content'] for data_item in chunk.data if 'content' in data_item), end=\"\", flush=True)\n</code></pre> <p>\ud83d\udd39 Casos de uso: \u2705 Validar respuestas antes de ejecutarlas. \u2705 Solicitar confirmaci\u00f3n de un usuario antes de tomar una decisi\u00f3n cr\u00edtica.  </p>"},{"location":"curso3/tema5_langgraph_sdk/#6-manejo-del-store-con-la-sdk","title":"\ud83d\uddc4\ufe0f 6. Manejo del Store con la SDK","text":"<p>Hemos explorado LangGraph Store en los Temas 1, 2 y 3 de este curso. Ahora veremos c\u00f3mo interactuar con estos datos mediante la SDK.  </p> <p>\ud83d\udccc Ejemplo: Almacenar datos en el Store </p> <pre><code>from uuid import uuid4\n\n# Almacenamos los datos\nawait client.store.put_item(\n    (\"testing\", \"Test\"),\n    key=str(uuid4()),\n    value={\"todo\": \"Probando a\u00f1adir desde la SDK\"},\n)\n</code></pre> <p>\ud83d\udccc Ejemplo: Buscar informaci\u00f3n en la base de datos de LangGraph </p> <pre><code># Buscamos todos los valores en un namespace espec\u00edfico\nitems = await client.store.search_items(\n    (\"testing\", \"Test\"),\n    limit=5,\n    offset=0\n)\n\n# Mostramos los resultados de la b\u00fasqueda\nfor item in items['items']:\n    print(item)\n</code></pre> Nota <p>Tal y como vimos en los temas sobre <code>Store</code>, puedes consultar m\u00e1s m\u00e9todos en la documentaci\u00f3n oficial: StoreClient </p>"},{"location":"curso3/tema5_langgraph_sdk/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>La SDK de LangGraph nos permite conectar, ejecutar y gestionar grafos remotos de manera eficiente.  </p> <p>\u2705 Podemos ejecutar grafos alojados en LangGraph Cloud o Docker. \u2705 Podemos administrar ejecuciones (runs) y sesiones (threads). \u2705 Podemos utilizar streaming para recibir resultados en tiempo real. \u2705 Podemos integrar Human in the Loop y manipular datos almacenados en LangGraph Store.  </p> <p>\ud83d\ude80 Con estos conocimientos, podemos integrar LangGraph en cualquier aplicaci\u00f3n de manera flexible y escalable. </p>"},{"location":"curso3/tema5_langgraph_sdk/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>C\u00f3mo conectarnos a grafos remotos usando RemoteGraph.  </li> <li>C\u00f3mo gestionar ejecuciones mediante background runs.  </li> <li>C\u00f3mo recibir datos en tiempo real con streaming runs.  </li> <li>C\u00f3mo organizar interacciones con threads.  </li> <li>C\u00f3mo usar Human in the Loop dentro de la SDK.  </li> <li>C\u00f3mo acceder a datos almacenados en LangGraph Store.  </li> </ul>"},{"location":"curso3/tema5_langgraph_sdk/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Concept: LangGraph SDK</li> <li> How-to-guide: Runs</li> <li> How-to-guide: RemoteGraph</li> <li> How-to-guide: stream_mode</li> <li> How-to-guide: Check status of your threads</li> <li> API Reference: Langgraph SDK</li> <li> Class: StoreClient</li> <li> Ejemplos: Background Runs</li> </ul>"},{"location":"curso3/tema5_langgraph_sdk/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos Doble-Texting, una t\u00e9cnica para manejar m\u00faltiples mensajes simult\u00e1neos en un grafo. Analizaremos diferentes estrategias como reject, enqueue, interrupt y rollback para gestionar estos eventos correctamente.  </p>"},{"location":"curso3/tema6_doble-texting/","title":"\ud83d\udd04 Tema 7: Doble-Texting \u2013 Manejo de Mensajes Simult\u00e1neos en LangGraph","text":""},{"location":"curso3/tema6_doble-texting/#introduccion","title":"\ud83c\udf1f Introducci\u00f3n","text":"<p>Cuando un usuario interact\u00faa con un chatbot o un sistema basado en LangGraph, puede ocurrir que env\u00ede m\u00faltiples mensajes antes de recibir una respuesta. Este fen\u00f3meno se conoce como doble-texting y puede generar inconsistencias en el flujo conversacional si no se maneja correctamente.  </p> <p>\ud83d\udccc Ejemplo de problema con doble-texting: 1\ufe0f\u20e3 Un usuario env\u00eda el mensaje: \"\u00bfCu\u00e1nto cuesta el producto?\" 2\ufe0f\u20e3 Antes de que el chatbot responda, env\u00eda otro mensaje: \"Olv\u00eddalo, mejor dime sobre el env\u00edo.\" 3\ufe0f\u20e3 Si el grafo no maneja el doble-texting, puede responder primero a la pregunta del precio, ignorando el segundo mensaje o proces\u00e1ndolo de forma desordenada.  </p> <p>\ud83d\udd39 Para evitar estos problemas, LangGraph ofrece cuatro estrategias para manejar el doble-texting de forma efectiva: \u2705 Reject (Rechazar) \u2013 Ignorar el segundo mensaje. \u2705 Enqueue (Encolar) \u2013 Poner el mensaje en espera hasta que termine la respuesta actual. \u2705 Interrupt (Interrumpir) \u2013 Detener la ejecuci\u00f3n y procesar el mensaje nuevo. \u2705 Rollback (Revertir) \u2013 Deshacer la ejecuci\u00f3n y comenzar desde el nuevo mensaje.  </p> <p>Cada una de estas estrategias se adapta a diferentes necesidades seg\u00fan el contexto del grafo.  </p> <p>\ud83d\udd17 Referencia oficial: Doble-Texting en LangGraph </p>"},{"location":"curso3/tema6_doble-texting/#1-reject-rechazar-mensajes-adicionales","title":"\u274c 1. Reject \u2013 Rechazar Mensajes Adicionales","text":"<p>Con esta estrategia, LangGraph ignora cualquier nuevo mensaje si ya hay una ejecuci\u00f3n en curso. Es \u00fatil cuando queremos procesar solo una solicitud a la vez, evitando interrupciones o cambios en el flujo.  </p> Atenci\u00f3n <p>Esta estrategia puede generar una mala experiencia de usuario si el segundo mensaje es m\u00e1s relevante que el primero.  </p> <pre><code>from langgraph_sdk import get_client\nurl_for_cli_deployment = \"http://localhost:8123\"\nclient = get_client(url=url_for_cli_deployment)\n\nimport httpx\nfrom langchain_core.messages import HumanMessage\n\n# Creamos un thread\nthread = await client.threads.create()\n\n# Creamos las operaciones\ninput_1 = \"Realizame esta suma 4+2.\"\ninput_2 = \"perdon, era 4+3.\"\nconfig = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\ngraph_name = \"calculator\" \n\n# Ejecutamos el primero Run\nrun = await client.runs.create(\n    thread[\"thread_id\"],\n    graph_name,\n    input={\"messages\": [HumanMessage(content=input_1)]}, \n    config=config,\n)\ntry:\n    # Simulamos que ejecutamos otro Run de forma seguida.\n    await client.runs.create(\n        thread[\"thread_id\"],\n        graph_name,\n        input={\"messages\": [HumanMessage(content=input_2)]}, \n        config=config,\n        multitask_strategy=\"reject\",\n    )\nexcept httpx.HTTPStatusError as e:\n    print(\"A\u00fan hay una ejecuci\u00f3n en marcha:\", e)\n</code></pre> Resultado<pre><code>A\u00fan hay una ejecuci\u00f3n en marcha: Client error '409 Conflict' for url 'http://localhost:8123/threads/xxxxxxxxxxxxxxxxxx/runs'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409\n</code></pre> <p>Como vemos en el c\u00f3digo, al ejecutar el segundo run, obtenemos un error que indica que ya hay un proceso en ejecuci\u00f3n. Esto significa que el segundo mensaje no se procesar\u00e1, evitando conflictos en el flujo del grafo.  </p> Recomendaci\u00f3n <p>Si se usa esta estrategia, es importante informar al usuario que debe esperar antes de enviar otro mensaje.  </p>"},{"location":"curso3/tema6_doble-texting/#2-enqueue-encolar-mensajes-en-espera","title":"\ud83d\udce5 2. Enqueue \u2013 Encolar Mensajes en Espera","text":"<p>En lugar de ignorar el nuevo mensaje, LangGraph lo almacena en una cola y lo procesa cuando termine la ejecuci\u00f3n actual. Esta estrategia permite mantener el orden de los mensajes sin perder informaci\u00f3n.  </p> Nota <p>Con esta estrategia, el chatbot responder\u00e1 cada mensaje en el orden en que se recibi\u00f3, garantizando coherencia en la conversaci\u00f3n.  </p> <pre><code>from langchain_core.messages import HumanMessage, convert_to_messages\n\n# Creamos un thread\nthread = await client.threads.create()\n\n# Creamos las operaciones\ninput_1 = \"Realizame esta suma 4+2.\"\ninput_2 = \"perdon, era 4+3.\"\nconfig = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\ngraph_name = \"calculator\" \n\nfirst_run = await client.runs.create(\n    thread[\"thread_id\"],\n    graph_name,\n    input={\"messages\": [HumanMessage(content=input_1)]}, \n    config=config,\n)\n\nsecond_run = await client.runs.create(\n    thread[\"thread_id\"],\n    graph_name,\n    input={\"messages\": [HumanMessage(content=input_2)]}, \n    config=config,\n    multitask_strategy=\"enqueue\",\n)\n\n# Esperamos hasta que el segundo termine\nawait client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n\n# Obtenemos el resultado del grafo\nstate = await client.threads.get_state(thread[\"thread_id\"])\nfor m in convert_to_messages(state[\"values\"][\"messages\"]):\n    m.pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nRealizame esta suma 4+2.\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_ptkHcyJZIEtBxedW0map6xxO)\n Call ID: call_ptkHcyJZIEtBxedW0map6xxO\n  Args:\n    a: 4\n    b: 2\n================================= Tool Message =================================\nName: sumar\n\n6\n================================== Ai Message ==================================\n\nLa suma de 4 + 2 es 6.\n================================ Human Message =================================\n\nperdon, era 4+3.\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_T4rPpzzVKsQPFz2Q7b4JRdtW)\n Call ID: call_T4rPpzzVKsQPFz2Q7b4JRdtW\n  Args:\n    a: 4\n    b: 3\n================================= Tool Message =================================\nName: sumar\n\n7\n================================== Ai Message ==================================\n\nLa suma de 4 + 3 es 7.\n</code></pre> <p>Aqu\u00ed vemos que, aunque se env\u00edan dos mensajes consecutivos, LangGraph almacena el segundo mensaje en la cola y lo procesa una vez que finaliza el primero.  </p> Ventajas <ul> <li>Garantiza respuestas en orden. </li> <li>Evita errores de ejecuci\u00f3n simult\u00e1nea. </li> <li>Mantiene el flujo de la conversaci\u00f3n sin bloqueos. </li> </ul>"},{"location":"curso3/tema6_doble-texting/#3-interrupt-interrumpir-la-ejecucion-en-curso","title":"\u23f9\ufe0f 3. Interrupt \u2013 Interrumpir la Ejecuci\u00f3n en Curso","text":"<p>Esta estrategia detiene la ejecuci\u00f3n actual y prioriza el nuevo mensaje, asegurando que siempre se responda la \u00faltima entrada del usuario.  </p> Ejemplo <p>Si un usuario pregunta por un producto y luego dice \"Olv\u00eddalo, mejor dime sobre el env\u00edo\", el chatbot interrumpir\u00e1 la respuesta y procesar\u00e1 la \u00faltima solicitud.  </p> <pre><code>from langchain_core.messages import HumanMessage, convert_to_messages\n\n# Creamos un thread\nthread = await client.threads.create()\n\n# Creamos las operaciones\ninput_1 = \"Realizame esta suma 4+2.\"\ninput_2 = \"perdon, era 4+3.\"\nconfig = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\ngraph_name = \"calculator\" \n\ninterrupted_run = await client.runs.create(\n    thread[\"thread_id\"],\n    graph_name,\n    input={\"messages\": [HumanMessage(content=input_1)]}, \n    config=config,\n)\n\nsecond_run = await client.runs.create(\n    thread[\"thread_id\"],\n    graph_name,\n    input={\"messages\": [HumanMessage(content=input_2)]}, \n    config=config,\n    multitask_strategy=\"interrupt\",\n)\n\n# Esperamos hasta que el segundo termine\nawait client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n\n# Obtenemos el resultado del grafo\nstate = await client.threads.get_state(thread[\"thread_id\"])\nfor m in convert_to_messages(state[\"values\"][\"messages\"]):\n    m.pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nRealizame esta suma 4+2.\n================================ Human Message =================================\n\nperdon, era 4+3.\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_y7Q6Og96OLpoVXZ4Xt92yE1B)\n Call ID: call_y7Q6Og96OLpoVXZ4Xt92yE1B\n  Args:\n    a: 4\n    b: 3\n================================= Tool Message =================================\nName: sumar\n\n7\n================================== Ai Message ==================================\n\nLa suma de 4 + 3 es igual a 7.\n</code></pre> <p>Como podemos ver, la ejecuci\u00f3n anterior se interrumpe y el segundo mensaje se procesa inmediatamente. Sin embargo, cualquier informaci\u00f3n generada hasta ese punto sigue almacenada.  </p> Cuidado <p>Esta estrategia no borra el estado del grafo antes de interrumpir la ejecuci\u00f3n. Si la ejecuci\u00f3n anterior hab\u00eda guardado datos parciales, estos seguir\u00e1n disponibles.  </p> <p>Podemos comprobar que el run ha sido interrumpido ejecutando lo siguiente: </p> <pre><code># Confirmamos que el primer run se ha interrumpido y que el segundo ha terminado.\nprint((await client.runs.get(thread[\"thread_id\"], interrupted_run[\"run_id\"]))[\"status\"])\nprint((await client.runs.get(thread[\"thread_id\"], second_run[\"run_id\"]))[\"status\"])\n</code></pre> Resultado<pre><code>interrupted\nsuccess\n</code></pre>"},{"location":"curso3/tema6_doble-texting/#4-rollback-revertir-la-ejecucion-y-procesar-el-nuevo-mensaje","title":"\ud83d\udd04 4. Rollback \u2013 Revertir la Ejecuci\u00f3n y Procesar el Nuevo Mensaje","text":"<p>Con esta estrategia, LangGraph deshace la ejecuci\u00f3n actual y vuelve al estado anterior antes de procesar el nuevo mensaje. Es \u00fatil cuando queremos garantizar que cada respuesta tenga en cuenta la \u00faltima entrada del usuario sin interrupciones abruptas.  </p> Ejemplo <p>Si un usuario dice \"Quiero pedir una pizza\", pero segundos despu\u00e9s dice \"Mejor una hamburguesa\", la solicitud anterior se revierte y se procesa solo la \u00faltima.  </p> <pre><code>from langchain_core.messages import HumanMessage, convert_to_messages\n\n# Creamos un thread\nthread = await client.threads.create()\n\n# Creamos las operaciones\ninput_1 = \"Realizame esta suma 4+2.\"\ninput_2 = \"perdon, era 4+3.\"\nconfig = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\ngraph_name = \"calculator\" \n\nrolled_back_run = await client.runs.create(\n    thread[\"thread_id\"],\n    graph_name,\n    input={\"messages\": [HumanMessage(content=input_1)]}, \n    config=config,\n)\n\nsecond_run = await client.runs.create(\n    thread[\"thread_id\"],\n    graph_name,\n    input={\"messages\": [HumanMessage(content=input_2)]}, \n    config=config,\n    multitask_strategy=\"rollback\",\n)\n\n# Esperamos hasta que el segundo termine\nawait client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n\n# Obtenemos el resultado del grafo\nstate = await client.threads.get_state(thread[\"thread_id\"])\nfor m in convert_to_messages(state[\"values\"][\"messages\"]):\n    m.pretty_print()\n</code></pre> Resultado<pre><code>================================ Human Message =================================\n\nperdon, era 4+3.\n================================== Ai Message ==================================\nTool Calls:\n  sumar (call_O8oEtXiQ34q6K4MjhFsAkuP7)\n Call ID: call_O8oEtXiQ34q6K4MjhFsAkuP7\n  Args:\n    a: 4\n    b: 3\n================================= Tool Message =================================\nName: sumar\n\n7\n================================== Ai Message ==================================\n\nLa soluci\u00f3n de la ecuaci\u00f3n \\( 4 + 3 \\) es \\( 7 \\).\n</code></pre> <p>Aqu\u00ed, observamos que el primer mensaje se borra completamente del estado del grafo y se procesa solo el \u00faltimo.</p> Consideraciones <p>\u2705 Esta estrategia garantiza precisi\u00f3n en la respuesta, pero... \u274c Puede eliminar informaci\u00f3n \u00fatil que se haya procesado en el primer mensaje.</p> <p>Se recomienda usar Rollback solo cuando se tiene un control claro de los datos almacenados en el grafo.</p>"},{"location":"curso3/tema6_doble-texting/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>El doble-texting es un desaf\u00edo com\u00fan en sistemas conversacionales, pero con LangGraph podemos gestionar m\u00faltiples mensajes de manera estructurada.  </p> <p>\u2705 Podemos ignorar mensajes adicionales (Reject). \u2705 Podemos encolarlos para responderlos en orden (Enqueue)(Recomendado). \u2705 Podemos interrumpir la ejecuci\u00f3n para priorizar el \u00faltimo mensaje (Interrupt). \u2705 Podemos revertir la ejecuci\u00f3n y comenzar desde el nuevo mensaje (Rollback). </p> <p>\ud83d\ude80 Con estas estrategias, podemos mejorar la fluidez y coherencia en las conversaciones con LangGraph. </p>"},{"location":"curso3/tema6_doble-texting/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>Qu\u00e9 es el doble-texting y por qu\u00e9 puede generar problemas en un grafo.  </li> <li>Cu\u00e1les son las cuatro estrategias para manejar m\u00faltiples mensajes.  </li> <li>Cu\u00e1ndo usar Reject, Enqueue, Interrupt y Rollback seg\u00fan el caso de uso.  </li> </ul>"},{"location":"curso3/tema6_doble-texting/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li> Ver notebook en Google Colab</li> <li> Concept: Double Texting</li> <li> How-to-guide: Reject</li> <li> How-to-guide: Enqueue</li> <li> How-to-guide: Interrupt</li> <li> How-to-guide: Rollback</li> </ul>"},{"location":"curso3/tema6_doble-texting/#que-es-lo-siguiente","title":"\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente?","text":"<p>En el pr\u00f3ximo tema, exploraremos c\u00f3mo exponer nuestro grafo como una API REST o GraphQL, permitiendo integrarlo en cualquier aplicaci\u00f3n externa.  </p>"},{"location":"curso3/tema7_api_rest/","title":"Tema 7: API REST/GraphQL","text":"En proceso <p>Este documento aun no esta terminado.</p>"},{"location":"curso3/tema7_api_rest/#tema-8-creando-una-api-restgraphql-para-langgraph","title":"\ud83c\udf10 Tema 8: Creando una API REST/GraphQL para LangGraph","text":""},{"location":"curso3/tema7_api_rest/#introduccion","title":"\ud83c\udf1f Introducci\u00f3n","text":"<p>A lo largo del curso, hemos aprendido a ejecutar grafos con LangGraph CLI, gestionar su ejecuci\u00f3n con la SDK de LangGraph y conectarnos a ellos mediante RemoteGraph. Sin embargo, en algunos casos puede ser \u00fatil exponer una API personalizada para interactuar con nuestro grafo de manera m\u00e1s flexible.  </p> <p>\ud83d\udccc \u00bfPor qu\u00e9 crear una API REST o GraphQL para LangGraph? \u2705 Permite control total sobre los endpoints y la l\u00f3gica de negocio. \u2705 Facilita la integraci\u00f3n con otras aplicaciones que requieran consumir los datos del grafo. \u2705 Permite aplicar autenticaci\u00f3n, validaciones y permisos personalizados. \u2705 Mejora la escalabilidad, separando la l\u00f3gica del grafo de la API de acceso.  </p> <p>En este tema, veremos c\u00f3mo construir una API personalizada usando FastAPI y GraphQL, exponiendo nuestro grafo de manera estructurada.  </p> <p>\ud83d\udd17 Referencia oficial: LangGraph API </p>"},{"location":"curso3/tema7_api_rest/#estructura-del-proyecto","title":"\ud83c\udfd7\ufe0f Estructura del Proyecto","text":"<p>Para organizar nuestra API de forma modular y escalable, usaremos la siguiente estructura de archivos:  </p> <pre><code>mi_proyecto/\n\u251c\u2500\u2500 app/                    # Carpeta principal de la API\n\u2502   \u251c\u2500\u2500 main.py             # Archivo principal de FastAPI\n\u2502   \u251c\u2500\u2500 config.py           # Configuraci\u00f3n (API Keys, URL del grafo, etc.)\n\u2502   \u251c\u2500\u2500 routes/             # Carpeta de rutas\n\u2502   \u2502   \u251c\u2500\u2500 rest.py         # Endpoints REST\n\u2502   \u2502   \u251c\u2500\u2500 graphql.py      # Endpoints GraphQL\n\u2502   \u251c\u2500\u2500 services/           # L\u00f3gica de conexi\u00f3n con LangGraph\n\u2502   \u2502   \u251c\u2500\u2500 graph_client.py # Cliente para ejecutar el grafo\n\u2502   \u251c\u2500\u2500 security/           # Autenticaci\u00f3n y validaci\u00f3n\n\u2502   \u2502   \u251c\u2500\u2500 auth.py         # Validaci\u00f3n de API Keys\n\u251c\u2500\u2500 .env                    # Variables de entorno\n\u251c\u2500\u2500 requirements.txt        # Dependencias del proyecto\n\u2514\u2500\u2500 Dockerfile              # Dockerizaci\u00f3n de la API\n</code></pre>"},{"location":"curso3/tema7_api_rest/#1-creando-una-api-rest-con-fastapi","title":"\ud83d\ude80 1. Creando una API REST con FastAPI","text":"<p>Para exponer nuestro grafo como API, utilizaremos FastAPI, un framework r\u00e1pido y eficiente para construir APIs en Python.  </p> <p>\ud83d\udccc Pasos para crear la API: 1\ufe0f\u20e3 Instalar dependencias necesarias. 2\ufe0f\u20e3 Configurar FastAPI con un endpoint para ejecutar el grafo. 3\ufe0f\u20e3 Exponer endpoints REST para enviar entradas y obtener respuestas del grafo. </p> <p>\ud83d\udccc Ejemplo: Creando una API REST con FastAPI </p> <p>[api_rest_fastapi_placeholder]  </p> <p>\ud83d\udd39 Explicaci\u00f3n del c\u00f3digo: - Definimos un endpoint <code>/execute</code> que recibe una solicitud con datos de entrada. - Llamamos al grafo de LangGraph y enviamos los datos al servidor. - Retornamos la respuesta generada por el grafo en formato JSON.  </p> Atenci\u00f3n <p>FastAPI puede ejecutarse en modo as\u00edncrono, por lo que debemos asegurarnos de que LangGraph soporte async en nuestras llamadas.  </p>"},{"location":"curso3/tema7_api_rest/#2-creando-una-api-graphql-para-langgraph","title":"\ud83d\udcca 2. Creando una API GraphQL para LangGraph","text":"<p>En algunos casos, una API REST puede ser demasiado r\u00edgida, especialmente si necesitamos consultas m\u00e1s din\u00e1micas. Para ello, podemos usar GraphQL, que permite a los clientes solicitar solo los datos que necesitan.  </p> <p>\ud83d\udccc Ventajas de GraphQL sobre REST: \u2705 Permite consultas m\u00e1s flexibles, solicitando solo los datos necesarios. \u2705 Reduce la sobrecarga de m\u00faltiples requests, optimizando el rendimiento. \u2705 Es ideal para frontends din\u00e1micos que requieren diferentes datos en cada consulta.  </p> <p>\ud83d\udccc Ejemplo: Creando una API GraphQL para LangGraph </p> <p>[api_graphql_placeholder]  </p> <p>\ud83d\udd39 Explicaci\u00f3n del c\u00f3digo: - Creamos un esquema GraphQL que expone operaciones sobre el grafo. - Definimos mutaciones para enviar entradas y ejecutar procesos en LangGraph. - Implementamos resolvers que conectan GraphQL con nuestro grafo remoto.  </p> Nota <p>GraphQL es ideal para aplicaciones que requieren consultas din\u00e1micas y estructuras de datos flexibles.  </p>"},{"location":"curso3/tema7_api_rest/#3-comparacion-api-rest-vs-graphql","title":"\ud83d\udee0\ufe0f 3. Comparaci\u00f3n: API REST vs. GraphQL","text":"Caracter\u00edstica REST \ud83c\udf10 GraphQL \ud83d\udee0\ufe0f Estructura Endpoints fijos Consultas din\u00e1micas Flexibilidad Datos predefinidos Permite seleccionar qu\u00e9 datos obtener Uso recomendado Integraciones sencillas Aplicaciones con m\u00faltiples consultas diferentes Eficiencia Puede requerir m\u00faltiples requests Reduce llamadas innecesarias Ejemplo pr\u00e1ctico <ul> <li>Si queremos una API sencilla para enviar mensajes al grafo, REST es suficiente.  </li> <li>Si necesitamos hacer consultas personalizadas sobre el estado del grafo, GraphQL es mejor opci\u00f3n.  </li> </ul>"},{"location":"curso3/tema7_api_rest/#4-seguridad-y-autenticacion","title":"\ud83d\udd10 4. Seguridad y Autenticaci\u00f3n","text":"<p>Cuando exponemos una API a internet, debemos proteger el acceso con autenticaci\u00f3n y permisos. Algunas estrategias incluyen:  </p> <p>\u2705 API Keys: Requieren que cada petici\u00f3n incluya una clave \u00fanica de acceso. \u2705 OAuth2/JWT: Implementan autenticaci\u00f3n con tokens seguros. \u2705 Rate Limiting: Limita el n\u00famero de peticiones por usuario para evitar abusos.  </p> <p>\ud83d\udccc Ejemplo: Implementando autenticaci\u00f3n con API Key </p> <p>[api_auth_placeholder]  </p> <p>\ud83d\udd39 Explicaci\u00f3n del c\u00f3digo: - Requerimos que cada petici\u00f3n incluya una API Key v\u00e1lida en los headers. - Si la clave no es v\u00e1lida, devolvemos un error 401 (Unauthorized). - Podemos extender esto a sistemas m\u00e1s avanzados como OAuth2 o JWT.  </p> Atenci\u00f3n <p>Nunca expongas tu API sin autenticaci\u00f3n si maneja informaci\u00f3n sensible o datos privados.  </p>"},{"location":"curso3/tema7_api_rest/#conclusion","title":"\u2728 Conclusi\u00f3n","text":"<p>Crear una API personalizada para LangGraph nos permite flexibilizar la integraci\u00f3n con otros sistemas.  </p> <p>\u2705 Podemos exponer nuestro grafo mediante endpoints REST o GraphQL. \u2705 Podemos optimizar las consultas usando GraphQL para reducir el consumo de datos. \u2705 Podemos aplicar seguridad y autenticaci\u00f3n para controlar el acceso a nuestra API.  </p> <p>\ud83d\ude80 Con esta implementaci\u00f3n, podemos integrar LangGraph en cualquier ecosistema sin depender solo de la CLI o la SDK. </p>"},{"location":"curso3/tema7_api_rest/#que-hemos-aprendido","title":"\ud83e\uddd1\u200d\ud83c\udfeb \u00bfQu\u00e9 Hemos Aprendido?","text":"<ul> <li>C\u00f3mo crear una API REST para LangGraph con FastAPI.  </li> <li>C\u00f3mo exponer consultas GraphQL para mayor flexibilidad.  </li> <li>Cu\u00e1les son las diferencias entre REST y GraphQL.  </li> <li>C\u00f3mo proteger nuestra API con autenticaci\u00f3n y permisos.  </li> </ul> <p>\ud83c\udf10 \u00bfQu\u00e9 es lo Siguiente? En el pr\u00f3ximo tema, exploraremos Langflow, una herramienta de desarrollo visual que permite dise\u00f1ar flujos de trabajo de IA sin necesidad de escribir c\u00f3digo. Aprenderemos c\u00f3mo utilizar su interfaz gr\u00e1fica para construir agentes conversacionales y c\u00f3mo integrarlo con LangGraph para aprovechar su potencia en entornos de producci\u00f3n.</p>"},{"location":"curso3/tema8_langflow/","title":"\ud83d\udda5\ufe0f Tema 8: Langflow","text":""},{"location":"curso3/tema8_langflow/#introduccion","title":"\ud83c\udf1f Introducci\u00f3n","text":"<p>En este tema, exploraremos Langflow, una herramienta de bajo c\u00f3digo dise\u00f1ada para facilitar la creaci\u00f3n de agentes de inteligencia artificial y flujos de trabajo complejos.  </p> <p>Langflow permite a los desarrolladores integrar modelos de lenguaje, APIs y bases de datos de manera intuitiva, acelerando el desarrollo de aplicaciones basadas en IA.  </p> <p></p>"},{"location":"curso3/tema8_langflow/#que-es-langflow","title":"\ud83d\udd0e \u00bfQu\u00e9 es Langflow?","text":"<p>Langflow es una plataforma que ofrece una interfaz gr\u00e1fica para construir aplicaciones de IA mediante componentes reutilizables.  </p> <p>Cada componente representa una unidad funcional, como: \u2705 Un modelo de lenguaje \u2705 Una fuente de datos \u2705 Una herramienta externa  </p> <p>Estos componentes se pueden conectar entre s\u00ed para formar flujos de trabajo completos.  </p> Ejemplo de uso <p>Un usuario puede arrastrar un modelo GPT, conectarlo con una base de datos y definir reglas para responder preguntas con informaci\u00f3n actualizada.  </p>"},{"location":"curso3/tema8_langflow/#como-se-utiliza-langflow","title":"\ud83d\udee0\ufe0f \u00bfC\u00f3mo se utiliza Langflow?","text":"<p>Para utilizar Langflow, sigue estos pasos:  </p> <p>1\ufe0f\u20e3 Creaci\u00f3n de Flujos \ud83c\udfd7\ufe0f    - Arrastra y suelta componentes desde la barra lateral al espacio de trabajo.    - Construye el flujo visualmente.  </p> <p></p> <p>2\ufe0f\u20e3 Configuraci\u00f3n de Componentes \u2699\ufe0f    - Haz clic en un componente para modificar su configuraci\u00f3n.    - Puedes ajustar los par\u00e1metros y ver el c\u00f3digo Python subyacente.  </p> <p></p> <p>3\ufe0f\u20e3 Conexi\u00f3n de Componentes \ud83d\udd17    - Une los componentes para definir el flujo de datos.    - Determina el orden y la l\u00f3gica del procesamiento.  </p> <p></p> <p>4\ufe0f\u20e3 Ejecuci\u00f3n del Flujo \u25b6\ufe0f    - Ejecuta el flujo desde la interfaz y observa los resultados en tiempo real.  </p> <p></p> Nota <p>Langflow permite exportar el c\u00f3digo generado para su uso en otros entornos.  </p>"},{"location":"curso3/tema8_langflow/#para-que-sirve-langflow","title":"\ud83c\udfaf \u00bfPara qu\u00e9 sirve Langflow?","text":"<p>Langflow es \u00fatil para:  </p> <ul> <li>\ud83d\ude80 Prototipado r\u00e1pido \u2013 Crear y probar flujos de trabajo de IA sin escribir c\u00f3digo desde cero.  </li> <li>\ud83d\udc40 Visualizaci\u00f3n de flujos \u2013 Representar gr\u00e1ficamente la l\u00f3gica de la aplicaci\u00f3n para facilitar su comprensi\u00f3n.  </li> <li>\ud83d\udd0c Integraci\u00f3n sencilla \u2013 Incorporar modelos de lenguaje, APIs y bases de datos en una plataforma \u00fanica.  </li> </ul> Consejo <p>Langflow es ideal para desarrolladores y no programadores que desean experimentar con IA de forma visual.  </p>"},{"location":"curso3/tema8_langflow/#integracion-de-langflow-con-langgraph","title":"\ud83d\udd17 Integraci\u00f3n de Langflow con LangGraph","text":"<p>Langflow se integra perfectamente con LangGraph, una herramienta que permite construir flujos de trabajo complejos mediante programaci\u00f3n.  </p> <p>\ud83d\udca1 \u00bfC\u00f3mo trabajan juntos? </p> Funci\u00f3n Langflow \ud83d\udda5\ufe0f LangGraph \ud83e\udde9 Dise\u00f1o Visual \ud83c\udfa8 \u2705 S\u00ed \u274c No Automatizaci\u00f3n \u2699\ufe0f \u2705 Limitada \u2705 Avanzada Ejecuci\u00f3n en Producci\u00f3n \ud83d\ude80 \ud83d\udd36 No recomendado \u2705 S\u00ed Control de Flujo \ud83d\udd04 \ud83d\udfe1 B\u00e1sico \u2705 Completo <p>\ud83d\udd39 Casos de uso combinados: \u2705 Dise\u00f1ar agentes en Langflow y luego exportarlos para ejecutarlos con LangGraph. \u2705 Probar flujos visualmente antes de escribir c\u00f3digo en LangGraph. \u2705 Monitorear el comportamiento de un grafo mediante una interfaz gr\u00e1fica.  </p> Nota <p>Si quieres desplegar una aplicaci\u00f3n con flujos de IA robustos, combinar Langflow y LangGraph es una excelente opci\u00f3n.  </p>"},{"location":"curso3/tema8_langflow/#recursos-adicionales","title":"\ud83d\udcda Recursos Adicionales","text":"<p>\ud83d\udcd6 Documentaci\u00f3n Oficial de Langflow \ud83d\udd17 Langflow Docs </p> <p>\ud83d\udcd6 Comparativa entre LangGraph, LangChain, LangFlow y LangSmith \ud83d\udd17 Medium: LangGraph vs LangChain </p>"},{"location":"curso3/tema8_langflow/#has-completado-el-curso-3","title":"\ud83c\udf93 \u00a1Has Completado el Curso 3!","text":"<p>\ud83c\udf89 \u00a1Felicidades! Has finalizado el tercer curso de LangGraph, donde exploramos la memoria a largo plazo, el almacenamiento de datos con Store, el uso de LangGraph CLI, la conexi\u00f3n con nuestro grafo mediante la SDK de Python y el poderoso LangFlow.  </p> <p>\ud83d\ude80 Ahora dominas herramientas clave para construir y desplegar grafos inteligentes de alto nivel. </p>"},{"location":"curso3/tema8_langflow/#has-completado-todo-el-curso-de-langgraph","title":"\ud83c\udfc6 \u00bfHas Completado Todo el Curso de LangGraph?","text":"<p>\ud83e\udd73 \u00a1Enhorabuena! Con esto, concluyes el curso completo de LangGraph, donde pasaste de principiante a experto en la creaci\u00f3n de grafos inteligentes.  </p> <p>Ahora tienes las habilidades necesarias para desarrollar un asistente virtual avanzado que:  </p> <p>\u2705 \u2060Gestiona m\u00faltiples consultas con LLMs. \u2705 \u2060Mantiene memoria y contexto de conversaciones previas. \u2705 \u2060Ejecuta tareas en paralelo y maneja flujos complejos. \u2705 \u2060Se despliega y escala en entornos de producci\u00f3n. \u2705 \u2060Aprovecha herramientas clave del ecosistema LangChain, como LangStudio, LangSmith y LangGraph CLI.  </p> \ud83d\udca1 \u00a1Misi\u00f3n Cumplida! <p>Has recorrido un camino incre\u00edble, aprendiendo desde los fundamentos hasta t\u00e9cnicas avanzadas. Ahora es el momento de poner en pr\u00e1ctica todo lo aprendido y construir proyectos reales. </p>"},{"location":"curso3/tema8_langflow/#y-ahora-que","title":"\ud83e\udd14 \u00bfY Ahora Qu\u00e9?","text":"<p>Aunque este curso ha llegado a su fin, esto no es el final del camino.  </p> <p>\ud83d\udccc Pr\u00f3ximamente, en el Curso 4, exploraremos t\u00e9cnicas a\u00fan m\u00e1s avanzadas y profundizaremos en aspectos que no pudimos cubrir completamente en este curso. \ud83d\udd1c \u00a1Vuelve pronto para descubrir las novedades! </p> \ud83d\udce3 \u00a1Comparte tu experiencia! <p>Si este curso te ha sido \u00fatil, no dudes en compartirlo con otros y ayudar a que m\u00e1s personas descubran LangGraph.  </p> \ud83d\udc96 \u00bfQuieres apoyar el curso? <p>Si quieres contribuir al desarrollo de m\u00e1s contenido gratuito, pronto habilitar\u00e9 opciones para donaciones. \u00a1Tu apoyo me ayuda a seguir creando m\u00e1s cursos y recursos! \ud83d\ude80  </p> <p>\ud83d\udca1 \u00a1Gracias por formar parte de este curso y mucho \u00e9xito en tus proyectos con LangGraph! \ud83c\udfaf\u2728  </p>"},{"location":"curso4/","title":"\ud83d\ude80 Bienvenida al Curso 4: Explorando en Profundidad","text":"<p>\u00a1Bienvenido al cuarto m\u00f3dulo del curso de LangGraph! \ud83c\udf89</p> <p>Hasta ahora, ya hemos recorrido los fundamentos esenciales y comprendido c\u00f3mo construir grafos, trabajar con nodos, agentes, herramientas y m\u00e1s.</p> <p>Este nuevo curso tiene un enfoque distinto: aqu\u00ed vamos a detenernos para mirar m\u00e1s de cerca aspectos que ya conocemos, pero que merecen una explicaci\u00f3n m\u00e1s detallada. Tambi\u00e9n ser\u00e1 el lugar donde iremos incorporando nuevas funcionalidades, conceptos y mejoras que vayan surgiendo con el tiempo.</p> <p>Una secci\u00f3n viva</p> <p>Este m\u00f3dulo est\u00e1 pensado para crecer con el tiempo. Cada vez que aparezca un tema nuevo o haya algo importante que profundizar, lo a\u00f1adiremos aqu\u00ed. \u00a1No olvides volver de vez en cuando!</p>"},{"location":"curso4/#objetivos-del-curso-4","title":"\ud83c\udfaf Objetivos del Curso 4","text":"<p>El objetivo de este apartado es brindarte un espacio donde:</p> <ul> <li>Profundizaremos en conceptos vistos anteriormente desde un enfoque m\u00e1s t\u00e9cnico o pr\u00e1ctico.</li> <li>Introduciremos nuevas funcionalidades o clases que aparezcan en futuras versiones de LangChain o LangGraph.</li> <li>Crearemos secciones tem\u00e1ticas que puedas consultar de forma independiente.</li> </ul> <p>Este curso no sigue una l\u00ednea cronol\u00f3gica como los anteriores, sino que est\u00e1 pensado como una colecci\u00f3n de temas avanzados que puedes revisar seg\u00fan tus intereses o necesidades.</p>"},{"location":"curso4/#temario","title":"\ud83d\udcda Temario","text":"<p>A continuaci\u00f3n, te dejo la lista de temas abordados en esta secci\u00f3n. \u00a1Se ir\u00e1 ampliando conforme avance el ecosistema!</p> <ul> <li>\ud83d\udcac C\u00e1psula 1: Messages</li> <li>\ud83d\udcac C\u00e1psula 2: State In - State Out</li> <li>\ud83d\udcac C\u00e1psula 3: Agent Chat UI</li> <li>\ud83d\udcac C\u00e1psula 4: LangMem SDK</li> <li>\ud83d\udcac C\u00e1psula 5: Computer Use Agents</li> </ul> <p>M\u00e1s temas pr\u00f3ximamente...</p>"},{"location":"curso4/capsula1_messages/","title":"\ud83d\udcac C\u00e1psula 1: Messages","text":""},{"location":"curso4/capsula1_messages/#que-son-los-messages-y-para-que-sirven","title":"\ud83d\udcac Qu\u00e9 son los Messages y para qu\u00e9 sirven","text":"<p>En LangGraph (y en LangChain en general), los <code>Messages</code> son objetos que representan piezas de conversaci\u00f3n o interacci\u00f3n entre el humano, el modelo de lenguaje (IA), el sistema o herramientas externas.</p> <p>Estos mensajes encapsulan el contenido del di\u00e1logo y ayudan a estructurar correctamente las interacciones para que los modelos entiendan el contexto de forma clara y ordenada.</p> <p>Piensa en los <code>Messages</code> como los actores de una conversaci\u00f3n</p> <p>Cada uno tiene su rol definido: algunos hablan, otros dan contexto, y otros entregan resultados de herramientas.</p>"},{"location":"curso4/capsula1_messages/#cuando-se-recomienda-su-uso-y-por-que","title":"\ud83e\udded \u00bfCu\u00e1ndo se recomienda su uso y por qu\u00e9?","text":"<p>Usar <code>Messages</code> te permite construir flujos conversacionales m\u00e1s claros, escalables y reutilizables. Son especialmente \u00fatiles cuando trabajamos con:</p> <ul> <li>\ud83d\udd01 Conversaciones multi-turno (m\u00e1s de una interacci\u00f3n)</li> <li>\ud83e\udde0 Modelos que necesitan mantener el contexto</li> <li>\ud83d\udd27 Herramientas (como <code>ToolMessage</code>) en agentes o workflows</li> </ul> <p>Ventajas de los Messages</p> <ul> <li>Claridad: cada parte del mensaje tiene un tipo espec\u00edfico.</li> <li>Compatibilidad: son totalmente compatibles con los modelos de LangChain.</li> <li>Escalabilidad: f\u00e1ciles de componer, almacenar y modificar.</li> </ul>"},{"location":"curso4/capsula1_messages/#tipos-de-messages","title":"\ud83e\uddf1 Tipos de Messages","text":"<p>En LangChain, disponemos de varios tipos de mensajes. Cada uno tiene una funci\u00f3n distinta, y todos heredan de la clase base <code>BaseMessage</code>.</p>"},{"location":"curso4/capsula1_messages/#humanmessage","title":"\ud83e\uddd1\u200d\ud83d\udcbb <code>HumanMessage</code>","text":"<p>Representa lo que dice el usuario humano.</p> <p>Atributos: - <code>content</code>: el texto que env\u00eda el humano - <code>name</code>: (opcional) \u00fatil si hay m\u00faltiples usuarios</p> <p>Ejemplo:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nmsg = HumanMessage(content=\"Hola, \u00bfme puedes ayudar con un resumen?\")\nprint(msg)\n</code></pre>"},{"location":"curso4/capsula1_messages/#aimessage","title":"\ud83e\udd16 <code>AIMessage</code>","text":"<p>Representa la respuesta generada por el modelo de IA.</p> <p>Atributos: - <code>content</code>: la respuesta de la IA - <code>name</code>: (opcional) nombre de la IA si se desea especificar - <code>tool_calls</code>: (opcional) lista de herramientas llamadas por la IA - <code>function_call</code>: (opcional) si se us\u00f3 una funci\u00f3n  </p> <p>Ejemplo:</p> <pre><code>from langchain_core.messages import AIMessage\n\nmsg = AIMessage(content=\"Claro, \u00bfsobre qu\u00e9 tema necesitas el resumen?\")\nprint(msg)\n</code></pre>"},{"location":"curso4/capsula1_messages/#toolmessage","title":"\ud83e\uddf0 <code>ToolMessage</code>","text":"<p>Representa la respuesta de una herramienta que ha sido llamada por el modelo.</p> <p>Atributos: - <code>tool_call_id</code>: el ID que relaciona esta respuesta con su llamada - <code>content</code>: lo que responde la herramienta</p> <p>Ejemplo:</p> <pre><code>from langchain_core.messages import ToolMessage\n\nmsg = ToolMessage(tool_call_id=\"tool-1\", content=\"La temperatura actual es 21\u00b0C.\")\nprint(msg)\n</code></pre>"},{"location":"curso4/capsula1_messages/#functionmessage-deprecated-en-algunos-casos-revisar-compatibilidad","title":"\ud83d\udee0\ufe0f <code>FunctionMessage</code> (deprecated en algunos casos, revisar compatibilidad)","text":"<p>Usado para representar mensajes derivados de funciones espec\u00edficas.</p> <p>Atributos: - <code>name</code>: nombre de la funci\u00f3n - <code>content</code>: resultado textual  </p> <p>Ejemplo:</p> <pre><code>from langchain_core.messages import FunctionMessage\n\nmsg = FunctionMessage(name=\"get_weather\", content=\"Hoy hace sol con 21\u00b0C.\")\nprint(msg)\n</code></pre>"},{"location":"curso4/capsula1_messages/#systemmessage","title":"\u2699\ufe0f <code>SystemMessage</code>","text":"<p>Proporciona instrucciones al modelo. No lo ve el usuario, solo el modelo.</p> <p>Atributos: - <code>content</code>: las instrucciones del sistema - <code>name</code>: (opcional)  </p> <p>Ejemplo:</p> <pre><code>from langchain_core.messages import SystemMessage\n\nmsg = SystemMessage(content=\"Eres un asistente \u00fatil y conciso.\")\nprint(msg)\n</code></pre>"},{"location":"curso4/capsula1_messages/#recursos","title":"\ud83d\udd0e Recursos:","text":"<ul> <li>\ud83d\udcda Definici\u00f3n: Messages</li> </ul>"},{"location":"curso4/capsula1_messages/#resumen","title":"\ud83e\udde0 Resumen","text":"<p>Hemos aprendido que:</p> <ul> <li>Los <code>Messages</code> son piezas clave para estructurar interacciones.</li> <li>Usarlos mejora la claridad y mantenimiento de nuestro flujo conversacional.</li> <li>Hay distintos tipos con roles bien definidos: usuario, IA, sistema y herramientas.</li> <li>Cada mensaje tiene atributos espec\u00edficos y se puede crear f\u00e1cilmente con clases de LangChain.</li> </ul>"},{"location":"curso4/capsula1_messages/#consideraciones-finales-y-ayudas","title":"\ud83d\udd0d Consideraciones finales y ayudas","text":"<p>\u00a1Cuidado con el orden!</p> <p>El orden en que se agregan los <code>Messages</code> importa mucho. El modelo interpreta la conversaci\u00f3n como un historial cronol\u00f3gico, \u00a1no lo confundas!</p> <p>Si est\u00e1s trabajando con agentes o LangGraph, esta forma de modelar la conversaci\u00f3n se vuelve esencial. \u00a1Dominar los <code>Messages</code> te dar\u00e1 control total sobre c\u00f3mo interact\u00faan los nodos de tu grafo!</p>"},{"location":"curso4/capsula2_statein_stateout/","title":"C\u00e1psula 4: State In &amp; State Out","text":""},{"location":"curso4/capsula2_statein_stateout/#entendiendo-el-state-en-langgraph","title":"\ud83e\udde0 Entendiendo el <code>State</code> en LangGraph","text":"<p>Uno de los conceptos m\u00e1s importantes (y menos explorados a fondo hasta ahora) es el de los <code>states</code>. En LangGraph, cada ejecuci\u00f3n del grafo trabaja sobre un <code>state</code>, que es simplemente un diccionario (o estructura similar) que contiene toda la informaci\u00f3n que fluye de un nodo a otro.</p> <p>Este <code>state</code> puede ser: - Un estado inicial (<code>input</code>), que alimenta al primer nodo. - Un estado interno, que se va actualizando entre nodos (ej: flags, metadatos, historial). - Un estado de salida (<code>output</code>), que representa el resultado final del grafo.  </p>"},{"location":"curso4/capsula2_statein_stateout/#como-definimos-un-state","title":"\ud83e\udde9 \u00bfC\u00f3mo definimos un <code>State</code>?","text":"<p>En este curso usamos <code>TypedDict</code> para definir nuestros <code>states</code>, lo cual nos permite describir su estructura con claridad sin necesidad de validaciones complejas.</p> <p>Ejemplo simple de TypedDict para un state</p> <p>Un estado que guarda la entrada del usuario, el historial del chat y la respuesta final.</p> <pre><code>from typing import TypedDict, List\n\nclass ChatState(TypedDict):\n    input: str\n    history: List[str]\n    response: str\n</code></pre> <p>Esto nos da una base clara para saber qu\u00e9 informaci\u00f3n fluye dentro del grafo.</p>"},{"location":"curso4/capsula2_statein_stateout/#separando-nuestros-states-entrada-internos-y-salida","title":"\ud83d\udee0\ufe0f Separando nuestros <code>states</code>: entrada, internos y salida","text":"<p>Veamos c\u00f3mo podemos separar la l\u00f3gica del <code>state</code> en distintas fases para organizar mejor nuestros flujos:</p>"},{"location":"curso4/capsula2_statein_stateout/#1-state-in-la-entrada","title":"1\ufe0f\u20e3 <code>State In</code>: la entrada","text":"<p>Aqu\u00ed solo necesitamos lo m\u00ednimo necesario para iniciar el flujo, por ejemplo, el mensaje del usuario.</p> <pre><code># State In (solo entrada)\nfrom typing import TypedDict\n\nclass InputState(TypedDict):\n    input: str\n\ninitial_state = InputState(input=\"\u00bfCu\u00e1l es la capital de Espa\u00f1a?\")\nprint(initial_state)\n</code></pre>"},{"location":"curso4/capsula2_statein_stateout/#2-state-interno-gestionando-datos-temporales","title":"2\ufe0f\u20e3 <code>State Interno</code>: gestionando datos temporales","text":"<p>Podemos usar campos como <code>metadata</code>, <code>flags</code>, o <code>intermediate_results</code> para compartir informaci\u00f3n entre nodos sin que necesariamente formen parte del resultado final.</p> <pre><code># State Interno (con campos extra para el flujo)\nfrom typing import TypedDict, List\n\nclass InternalState(TypedDict):\n    input: str\n    metadata: str\n    flag: str\n    history: List[str]\n\ninternal_state = InternalState(\n    input=\"\u00bfCu\u00e1l es la capital de Espa\u00f1a?\",\n    metadata=\"interacci\u00f3n 1\",\n    flag=\"OK\",\n    history=[\"Usuario: \u00bfCu\u00e1l es la capital de Espa\u00f1a?\"]\n)\nprint(internal_state)\n</code></pre>"},{"location":"curso4/capsula2_statein_stateout/#3-state-out-resultado-final-del-grafo","title":"3\ufe0f\u20e3 <code>State Out</code>: resultado final del grafo","text":"<p>Una vez que el grafo termina, podemos definir exactamente qu\u00e9 campos nos interesa exponer a la app como resultado final.</p> <pre><code># State Out (estado final con la respuesta y el historial)\nfrom typing import TypedDict, List\n\nclass OutputState(TypedDict):\n    response: str\n    history: List[str]\n\noutput_state = OutputState(\n    response=\"La capital de Espa\u00f1a es Madrid.\",\n    history=[\n        \"Usuario: \u00bfCu\u00e1l es la capital de Espa\u00f1a?\",\n        \"IA: La capital de Espa\u00f1a es Madrid.\"\n    ]\n)\nprint(output_state)\n</code></pre>"},{"location":"curso4/capsula2_statein_stateout/#ejemplo-completo","title":"Ejemplo completo","text":"<pre><code>from typing import TypedDict, List\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.message import add_messages\n\n# 1. Definimos los estados\nclass InputState(TypedDict):\n    input: str\n\nclass InternalState(TypedDict):\n    input: str\n    metadata: str\n    flag: str\n    history: List[str]\n\nclass OutputState(TypedDict):\n    response: str\n    history: List[str]\n# 2. Nodos del grafo\n\n# Nodo de entrada: inicializa el historial\ndef nodo_entrada(state: InputState) -&gt; InternalState:\n    history = [f\"Usuario: {state['input']}\"]\n    return {**state, \"history\": history}\n\n# Nodo intermedio: agrega metadata y un flag\ndef nodo_interno(state: InternalState) -&gt; InternalState:\n    return {**state, \"metadata\": \"proceso_intermedio\", \"flag\": \"OK\"}\n\n# Nodo de salida: genera respuesta y actualiza historial\ndef nodo_salida(state: InternalState) -&gt; OutputState:\n    respuesta = \"La capital de Espa\u00f1a es Madrid.\"  # Supongamos una respuesta dummy\n    new_history = state[\"history\"] + [f\"IA: {respuesta}\"]\n    return {\n        \"response\": respuesta,\n        \"history\": new_history\n    }\n\n# 3. Construimos el grafo\nbuilder = StateGraph(InputState)\n\nbuilder.add_node(\"entrada\", nodo_entrada)\nbuilder.add_node(\"interno\", nodo_interno)\nbuilder.add_node(\"salida\", nodo_salida)\n\nbuilder.set_entry_point(\"entrada\")\nbuilder.add_edge(\"entrada\", \"interno\")\nbuilder.add_edge(\"interno\", \"salida\")\nbuilder.add_edge(\"salida\", END)\n\ngraph = builder.compile()\n\n# 4. Ejecutamos el grafo\ninitial_state = {\n    \"input\": \"\u00bfCu\u00e1l es la capital de Espa\u00f1a?\"\n}\n\nfinal_state = graph.invoke(initial_state)\nprint(final_state)\n</code></pre> Resultado<pre><code>{\n  'response': 'La capital de Espa\u00f1a es Madrid.',\n  'history': [\n    'Usuario: \u00bfCu\u00e1l es la capital de Espa\u00f1a?',\n    'IA: La capital de Espa\u00f1a es Madrid.'\n  ]\n}\n</code></pre>"},{"location":"curso4/capsula2_statein_stateout/#ventajas-de-trabajar-con-estados-divididos","title":"\ud83e\udde9 Ventajas de trabajar con estados divididos","text":"<p>Dise\u00f1ar tu <code>state</code> dividi\u00e9ndolo en entrada, internos y salida trae muchas ventajas:</p> <ul> <li>Claridad: Cada campo tiene un prop\u00f3sito espec\u00edfico. Sabes qu\u00e9 datos llegan, cu\u00e1les son temporales y cu\u00e1les se exponen.</li> <li>Encapsulamiento: Puedes mantener campos internos como <code>metadata</code>, <code>flags</code>, <code>steps</code>, etc., sin preocuparte de filtrarlos al final.</li> <li>Simplicidad para la app: El output es limpio. Solo contiene lo que tu aplicaci\u00f3n necesita, sin basura t\u00e9cnica.</li> <li>Reutilizaci\u00f3n: Puedes adaptar el grafo f\u00e1cilmente a distintos contextos cambiando solo la parte final del <code>state</code>.</li> </ul> <p>Informaci\u00f3n privada y \u00fatil</p> <p>Usar un <code>state</code> interno te permite almacenar datos sensibles, auxiliares o de control sin tener que incluirlos en el resultado final. \u00a1Perfecto para trabajar con contextos complejos de forma segura!</p>"},{"location":"curso4/capsula2_statein_stateout/#buenas-practicas-al-disenar-tus-states","title":"\ud83e\udded Buenas pr\u00e1cticas al dise\u00f1ar tus <code>states</code>","text":"<p>Consejos \u00fatiles</p> <ul> <li>Empieza simple: define solo lo que necesitas.</li> <li>Usa nombres claros: <code>input</code>, <code>output</code>, <code>metadata</code>, <code>flag</code>, etc.</li> <li>No todo tiene que salir al final: los estados internos son solo para el flujo.</li> <li>Usa <code>TypedDict</code> o <code>BaseModel</code> si necesitas validaci\u00f3n, pero no compliques de m\u00e1s.</li> </ul>"},{"location":"curso4/capsula2_statein_stateout/#resumen","title":"\u2705 Resumen","text":"<ul> <li>El <code>state</code> es el veh\u00edculo de datos que recorre todo tu grafo.</li> <li>Puedes dividirlo mentalmente en tres partes: entrada, internos y salida.</li> <li>Esto te ayuda a estructurar flujos m\u00e1s claros y mantener tu l\u00f3gica desacoplada.</li> <li>En esta c\u00e1psula vimos c\u00f3mo definir estos estados con <code>TypedDict</code> y c\u00f3mo usarlos dentro de un grafo.</li> </ul>"},{"location":"curso4/capsula3_agent_chat_ui/","title":"\ud83d\udcac C\u00e1psula: Agent Chat UI","text":""},{"location":"curso4/capsula3_agent_chat_ui/#que-es-agent-chat-ui","title":"\ud83c\udfaf \u00bfQu\u00e9 es Agent Chat UI?","text":"<p>Agent Chat UI es una aplicaci\u00f3n web construida con Next.js que te permite chatear con cualquier agente de LangGraph (tanto en Python como en TypeScript) a trav\u00e9s de una interfaz de chat moderna y lista para usar.</p> <p>En pocas palabras, es el \u201cfrontend\u201d de chat para tus agentes: conectas tu servidor de LangGraph y empiezas a interactuar con tu grafo/assistant sin tener que construir una UI desde cero. Soporta streaming, renderizado de artefactos en un panel lateral y opciones de producci\u00f3n con autenticaci\u00f3n.</p> <p>--</p>"},{"location":"curso4/capsula3_agent_chat_ui/#para-que-podemos-usarlo-casos-de-uso","title":"\ud83e\udde0 \u00bfPara qu\u00e9 podemos usarlo? (Casos de uso)","text":"<ul> <li>\ud83e\uddea Prototipado r\u00e1pido de agentes: prueba tus grafos sin desarrollar interfaz propia.</li> <li>\ud83e\uddf0 Demos y presentaciones: muestra tus agentes a usuarios o stakeholders de forma simple.</li> <li>\ud83d\udedf Soporte interno: interfaz para equipos que necesitan hablar con agentes internos.</li> <li>\ud83d\udd2c Experimentos y evaluaci\u00f3n: valida prompts, flujos y memoria sobre una UI estable.</li> <li>\ud83d\ude80 Producci\u00f3n con autenticaci\u00f3n: publica una UI de chat conectada a tu despliegue de LangGraph.</li> </ul> <p>--</p>"},{"location":"curso4/capsula3_agent_chat_ui/#como-se-instala","title":"\ud83d\udce6 \u00bfC\u00f3mo se instala?","text":"<p>Requisitos previos</p> <ul> <li>Node.js 18+ (recomendado LTS)</li> <li>pnpm instalado globalmente (o usa npm/yarn)</li> </ul> <p>Tienes dos caminos principales:</p>"},{"location":"curso4/capsula3_agent_chat_ui/#opcion-1-crear-proyecto-nuevo-recomendado","title":"\"Opci\u00f3n 1: Crear proyecto nuevo (recomendado)\"","text":"<pre><code>npx create-agent-chat-app\ncd agent-chat-ui\npnpm install\npnpm dev\n</code></pre>"},{"location":"curso4/capsula3_agent_chat_ui/#opcion-2-clonar-el-repositorio-oficial","title":"\"Opci\u00f3n 2: Clonar el repositorio oficial\"","text":"<p><pre><code>git clone https://github.com/langchain-ai/agent-chat-ui.git\ncd agent-chat-ui\npnpm install\npnpm dev\n</code></pre> La app quedar\u00e1 disponible en http://localhost:3000</p> <p>Sin instalaci\u00f3n local</p> <p>Puedes usar la demo desplegada: https://agentchat.vercel.app/</p>"},{"location":"curso4/capsula3_agent_chat_ui/#como-se-usa","title":"\ud83d\udd79\ufe0f \u00bfC\u00f3mo se usa?","text":"<p>Al abrir la app por primera vez (local o demo), ver\u00e1s un formulario de conexi\u00f3n donde debes indicar:</p> <ol> <li>\ud83d\udd17 Deployment URL: URL de tu servidor LangGraph (local o producci\u00f3n).</li> <li>\ud83e\udded Assistant/Graph ID: nombre del grafo o ID del assistant al que quieres chatear.</li> <li>\ud83d\udd10 LangSmith API Key: solo si te conectas a un LangGraph desplegado que requiera autenticaci\u00f3n con LangSmith.</li> </ol> <p>Despu\u00e9s, pulsa \u00abContinue\u00bb y entrar\u00e1s al chat. A partir de ah\u00ed podr\u00e1s enviar mensajes, ver respuestas en streaming, visualizar artefactos y controlar la sesi\u00f3n.</p>"},{"location":"curso4/capsula3_agent_chat_ui/#saltar-el-formulario-variables-de-entorno","title":"\ud83d\udeab Saltar el formulario (variables de entorno)","text":"<p>Si lo prefieres, puedes precargar la configuraci\u00f3n creando un archivo <code>.env</code> (copia <code>.env.example</code>) y definiendo por ejemplo:</p> <pre><code>NEXT_PUBLIC_API_URL=http://localhost:2024\nNEXT_PUBLIC_ASSISTANT_ID=agent\n</code></pre> <p>Al iniciar, la UI usar\u00e1 estos valores y no mostrar\u00e1 el formulario.</p>"},{"location":"curso4/capsula3_agent_chat_ui/#funciones-clave-utiles","title":"\ud83e\udde9 Funciones clave \u00fatiles","text":""},{"location":"curso4/capsula3_agent_chat_ui/#1-ocultar-mensajes-en-el-chat","title":"1) \ud83d\udd0d Ocultar mensajes en el chat","text":"<ul> <li>Ocultar el streaming (render en vivo): a\u00f1ade la etiqueta <code>langsmith:nostream</code> a tu modelo de chat.</li> <li>Ocultar un mensaje por completo (ni en vivo ni al final): a\u00f1ade la etiqueta <code>langsmith:do-not-render</code> y guarda el mensaje con un <code>id</code> que empiece por <code>do-not-render-</code>.</li> </ul>"},{"location":"curso4/capsula3_agent_chat_ui/#python","title":"\"Python\"","text":"<pre><code># Evitar streaming en la UI\nmodel = ChatAnthropic().with_config(config={\"tags\": [\"langsmith:nostream\"]})\n\n# Ocultar por completo un mensaje\nresult = model.invoke([messages])\nresult.id = f\"do-not-render-{result.id}\"\nreturn {\"messages\": [result]}\n</code></pre>"},{"location":"curso4/capsula3_agent_chat_ui/#typescript","title":"\"TypeScript\"","text":"<pre><code>// Evitar streaming en la UI\nconst model = new ChatAnthropic().withConfig({ tags: [\"langsmith:nostream\"] });\n\n// Ocultar por completo un mensaje\nconst result = await model.invoke([messages]);\nresult.id = `do-not-render-${result.id}`;\nreturn { messages: [result] };\n</code></pre> <p>Importante</p> <p>Aunque ocultes el streaming, si el mensaje se guarda en el estado sin modificaciones, aparecer\u00e1 al final salvo que uses el patr\u00f3n <code>do-not-render-</code>.</p>"},{"location":"curso4/capsula3_agent_chat_ui/#2-renderizado-de-artefactos","title":"2) \ud83e\uddf1 Renderizado de artefactos","text":"<p>La UI permite mostrar \u201cartefactos\u201d (contenido adicional) en un panel lateral. Desde tu grafo puedes establecer contexto en <code>thread.meta.artifact</code> y, en la UI, recuperar ese contexto con el hook <code>useArtifact</code> para renderizar componentes propios.</p>"},{"location":"curso4/capsula3_agent_chat_ui/#como-llevarlo-a-produccion","title":"\ud83c\udfed \u00bfC\u00f3mo llevarlo a producci\u00f3n?","text":"<p>Tienes dos enfoques principales:</p>"},{"location":"curso4/capsula3_agent_chat_ui/#quickstart-api-passthrough-proxy","title":"\"Quickstart: API Passthrough (proxy)\"","text":"<p>Usa el paquete de passthrough para proxy inverso y autenticaci\u00f3n sencilla. Configura estas variables en <code>.env</code>:</p> <pre><code># URL de tu despliegue de LangGraph (producci\u00f3n)\nLANGGRAPH_API_URL=\"https://mi-agente.default.us.langgraph.app\"\n# URL p\u00fablica de tu web + \"/api\" (el proxy)\nNEXT_PUBLIC_API_URL=\"https://mi-sitio.com/api\"\n# ID del assistant/grafo a usar\nNEXT_PUBLIC_ASSISTANT_ID=\"agent\"\n# API Key de LangSmith (solo en servidor, sin NEXT_PUBLIC_)\nLANGSMITH_API_KEY=\"lsv2_...\"\n</code></pre> <p>Con esto, el cliente habla con tu <code>/api</code>, y el servidor inyecta la clave de LangSmith al llamar a LangGraph.</p>"},{"location":"curso4/capsula3_agent_chat_ui/#avanzado-autenticacion-personalizada","title":"\"Avanzado: Autenticaci\u00f3n personalizada\"","text":"<p>Implementa autenticaci\u00f3n en tu servidor de LangGraph y pasa un token propio desde el cliente. Pasos t\u00edpicos:</p> <ol> <li>Exp\u00f3n un endpoint en tu despliegue de LangGraph que devuelva un token de acceso para el usuario.</li> <li>En la UI, ajusta el hook de streaming para enviar el token en headers, por ejemplo:</li> </ol>"},{"location":"curso4/capsula3_agent_chat_ui/#const-streamvalue-usetypedstream-apiurl-processenvnext_public_api_url-assistantid-processenvnext_public_assistant_id-defaultheaders-authentication-bearer-yourauthtoken","title":"<pre><code>const streamValue = useTypedStream({\n  apiUrl: process.env.NEXT_PUBLIC_API_URL,\n  assistantId: process.env.NEXT_PUBLIC_ASSISTANT_ID,\n  defaultHeaders: {\n    Authentication: `Bearer ${yourAuthToken}`,\n  },\n});\n</code></pre>","text":""},{"location":"curso4/capsula3_agent_chat_ui/#ventajas","title":"\u2705 Ventajas","text":"<ul> <li>\u23f1\ufe0f Ahorro de tiempo: UI de chat lista para conectar a tus grafos.</li> <li>\ud83e\udde9 Integraci\u00f3n directa con LangGraph (Python y TS) v\u00eda clave <code>messages</code>.</li> <li>\ud83d\udce1 Streaming en vivo y control fino de visibilidad de mensajes.</li> <li>\ud83d\uddbc\ufe0f Panel de artefactos para contenido enriquecido.</li> <li>\ud83c\udfc1 Lista para producci\u00f3n con opciones de autenticaci\u00f3n y proxy.</li> <li>\ud83e\uddea Ideal para demos, pruebas, evaluaci\u00f3n y uso interno.</li> <li>\ud83c\udd93 Open Source (MIT), f\u00e1cil de extender y personalizar.</li> </ul>"},{"location":"curso4/capsula3_agent_chat_ui/#recursos","title":"\ud83d\udd0e Recursos","text":"<ul> <li>\ud83c\udfa5 Video gu\u00eda</li> <li>\ud83d\udcbb Repositorio oficial</li> <li>\ud83e\uddea Demo en vivo</li> <li>\ud83d\udcd8 Docs LangGraph (autenticaci\u00f3n):<ul> <li>Python</li> <li>TypeScript</li> </ul> </li> </ul>"},{"location":"curso4/capsula3_agent_chat_ui/#que-hemos-aprendido","title":"\ud83e\udde9 \u00bfQu\u00e9 hemos aprendido?","text":"<ul> <li>Qu\u00e9 es Agent Chat UI y cu\u00e1ndo usarlo.</li> <li>C\u00f3mo instalarlo localmente o usar la demo.</li> <li>C\u00f3mo conectarlo a tu servidor de LangGraph.</li> <li>C\u00f3mo ocultar mensajes, renderizar artefactos y preparar un despliegue de producci\u00f3n.</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/","title":"\ud83e\udde0 C\u00e1psula 4: LangMem SDK \u2014 Memoria a Largo Plazo para Agentes","text":""},{"location":"curso4/capsula4_langmem_sdk/#que-es-langmem","title":"\ud83c\udfaf \u00bfQu\u00e9 es LangMem?","text":"<p>LangMem es un SDK que ayuda a tus agentes a aprender y adaptarse con el tiempo mediante memoria a largo plazo. Proporciona herramientas para:</p> <ul> <li>\ud83e\udde9 Extraer informaci\u00f3n importante de las conversaciones.</li> <li>\u270d\ufe0f Actualizar el comportamiento del agente mediante mejoras del prompt (memoria procedimental).</li> <li>\ud83d\uddc2\ufe0f Mantener recuerdos persistentes (hechos, preferencias, eventos) entre sesiones.</li> </ul> <p>Funciona con cualquier sistema de almacenamiento y se integra de forma nativa con la capa de memoria de LangGraph (BaseStore). Esto permite que tus agentes sean m\u00e1s personales, consistentes y \u201caprendan\u201d de sus interacciones.</p> <p>Lecturas clave</p> <ul> <li>Blog de lanzamiento del SDK: https://blog.langchain.dev/langmem-sdk-launch/ </li> <li>Documentaci\u00f3n (Key features + Quickstarts): https://langchain-ai.github.io/langmem/</li> <li>API de herramientas de memoria: https://langchain-ai.github.io/langmem/reference/tools/</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/#relacion-con-los-tipos-de-memoria-del-curso","title":"\ud83e\udded Relaci\u00f3n con los tipos de memoria del curso","text":"<p>En el curso ya vimos la diferencia entre:</p> <ul> <li>\ud83d\udfe1 Memoria a corto plazo (checkpointing dentro del hilo)</li> <li>\ud83d\udd35 Memoria a largo plazo (persistente entre sesiones)</li> </ul> <p>Adem\u00e1s, distinguimos 3 \u201csabores\u201d de memoria a largo plazo:</p> <ul> <li>\ud83e\udde0 Sem\u00e1ntica (facts): datos y preferencias del usuario, conocimiento atemporal.</li> <li>\ud83e\uddea Epis\u00f3dica (experiencias): ejemplos o res\u00famenes de interacciones pasadas.</li> <li>\u2699\ufe0f Procedimental (comportamiento): reglas y pautas del agente que evolucionan.</li> </ul> <p>LangMem ofrece herramientas listas para implementar estas memorias sobre la capa de almacenamiento de LangGraph, y se complementa con la memoria de corto plazo del grafo.</p> <p></p> <p>Como vimos en \u201cLong-Term Memory\u201d, LangMem encaja como la capa que persiste recuerdos entre sesiones, mientras que <code>MemorySaver</code> mantiene el historial dentro de un hilo activo.</p>"},{"location":"curso4/capsula4_langmem_sdk/#para-que-podemos-usarlo-casos-de-uso","title":"\ud83e\udde0 \u00bfPara qu\u00e9 podemos usarlo? (Casos de uso)","text":"<ul> <li>\ud83d\udc64 Personalizaci\u00f3n: recordar preferencias del usuario (p. ej., \u201cprefiero modo oscuro\u201d).</li> <li>\ud83d\uddc3\ufe0f Perfiles y contexto: construir y mantener fichas de usuario o equipo.</li> <li>\ud83e\udded Recuperaci\u00f3n contextual: buscar recuerdos relevantes para responder mejor.</li> <li>\ud83d\udd01 Mejora continua: ajustar instrucciones del agente seg\u00fan su rendimiento.</li> <li>\ud83e\udd1d Colaboraci\u00f3n: compartir memorias entre agentes o por equipos (namespaces).</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/#instalacion","title":"\ud83d\udce6 Instalaci\u00f3n","text":"<pre><code>pip install -U langmem\n</code></pre> <p>Configura la clave del proveedor de LLM (ejemplo con Anthropic):</p> <pre><code>export ANTHROPIC_API_KEY=\"sk-...\"\n</code></pre> <p>Almacenamiento</p> <ul> <li>Desarrollo: <code>InMemoryStore</code> (en memoria, se pierde al reiniciar). </li> <li>Producci\u00f3n: almacenes persistentes como <code>AsyncPostgresStore</code> u otros compatibles con <code>BaseStore</code>.</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/#como-se-usa-paso-a-paso","title":"\ud83d\udd79\ufe0f \u00bfC\u00f3mo se usa? (Paso a paso)","text":"<p>A continuaci\u00f3n ver\u00e1s c\u00f3mo a\u00f1adir memoria persistente a un agente de LangGraph usando las herramientas de LangMem. La idea clave es que el agente pueda:</p> <ol> <li>crear/actualizar/borrar recuerdos cuando haga falta, y</li> <li>buscar recuerdos relevantes durante el chat.</li> </ol>"},{"location":"curso4/capsula4_langmem_sdk/#1-preparar-el-store-y-namespaces","title":"1) Preparar el Store y Namespaces","text":"<p>Los recuerdos se guardan en un <code>BaseStore</code> y se organizan por <code>namespace</code>. Es habitual incluir un identificador de usuario para evitar mezcla de memorias.</p> <pre><code>from langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore(\n    index={\n        \"dims\": 1536,\n        \"embed\": \"openai:text-embedding-3-small\",\n    }\n)\n\n# El namespace final puede formarse con valores configurables a runtime.\n# Ej.: (\"memories\", \"&lt;user-id&gt;\")\n</code></pre> <p>Namespaces din\u00e1micos</p> <p>Puedes usar placeholders como <code>{langgraph_user_id}</code> y rellenarlos con <code>config={\"configurable\": {\"langgraph_user_id\": \"user-123\"}}</code>.</p>"},{"location":"curso4/capsula4_langmem_sdk/#2-herramientas-de-memoria-langmem","title":"2) Herramientas de memoria (LangMem)","text":"<p>LangMem provee dos herramientas principales:</p> <ul> <li>\ud83d\udee0\ufe0f <code>create_manage_memory_tool(...)</code>: crea/actualiza/borra recuerdos.</li> <li>\ud83d\udd0e <code>create_search_memory_tool(...)</code>: busca recuerdos por similitud o filtros.</li> </ul> <pre><code>from langmem import create_manage_memory_tool, create_search_memory_tool\n\nmanage_memory = create_manage_memory_tool(\n    namespace=(\"memories\", \"{langgraph_user_id}\"),\n)\n\nsearch_memory = create_search_memory_tool(\n    namespace=(\"memories\", \"{langgraph_user_id}\"),\n)\n</code></pre> <p>Contrato de <code>manage_memory</code></p> <ul> <li><code>content: str | None</code> \u2014 contenido del recuerdo (crear/actualizar) </li> <li><code>id: str | None</code> \u2014 id de un recuerdo existente (actualizar/borrar) </li> <li><code>action: Literal[\"create\",\"update\",\"delete\"]</code> \u2014 acci\u00f3n a realizar</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/#3-crear-un-agente-con-memoria","title":"3) Crear un agente con memoria","text":"<p>Puedes incorporar estas herramientas en un agente preconstruido de LangGraph (p. ej. ReAct) y pasarle el <code>store</code>:</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[manage_memory, search_memory],\n    store=store,\n)\n</code></pre>"},{"location":"curso4/capsula4_langmem_sdk/#4-guardar-y-recuperar-recuerdos-en-el-hot-path","title":"4) Guardar y recuperar recuerdos en el \u201chot path\u201d","text":"<p>El propio LLM decide cu\u00e1ndo llamar a las herramientas (no necesitas comandos especiales). Si el usuario dice algo relevante, el agente puede invocar <code>manage_memory</code> para guardarlo; y si necesita contexto pasado, <code>search_memory</code> para encontrarlo.</p> <pre><code># El agente decide guardar una preferencia\nagent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"Recu\u00e9rdame en el futuro que prefiero el modo oscuro.\"}]\n}, config={\"configurable\": {\"langgraph_user_id\": \"user-123\"}})\n\n# M\u00e1s tarde, el agente puede recuperar ese dato\nresp = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"\u00bfCu\u00e1les son mis preferencias de iluminaci\u00f3n?\"}]\n}, config={\"configurable\": {\"langgraph_user_id\": \"user-123\"}})\nprint(resp[\"messages\"][-1].content)\n# \u2192 \"Me comentaste que prefieres el modo oscuro.\"\n</code></pre> <p>Memoria procedimental</p> <p>LangMem tambi\u00e9n incluye utilidades para optimizar prompts (p. ej., <code>metaprompt</code>, <code>gradient</code>). Sirven para ajustar reglas del agente a partir de conversaciones exitosas o fallidas.</p>"},{"location":"curso4/capsula4_langmem_sdk/#5-background-memory-manager-opcional","title":"5) Background memory manager (opcional)","text":"<p>Adem\u00e1s del uso \u201cen caliente\u201d, puedes correr un gestor en background que procese conversaciones, consolide recuerdos y mantenga consistencia sin bloquear la interacci\u00f3n.</p> <ul> <li>Gu\u00eda \u201cBackground Quickstart\u201d: https://langchain-ai.github.io/langmem/background_quickstart/</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/#ventajas","title":"\u2705 Ventajas","text":"<ul> <li>\ud83e\udde9 API simple y portable; funciona con cualquier <code>BaseStore</code>.</li> <li>\ud83d\udd0c Integraci\u00f3n nativa con la memoria persistente de LangGraph.</li> <li>\ud83e\udd16 Agentes m\u00e1s personalizados y consistentes entre sesiones.</li> <li>\ud83d\udd0d B\u00fasqueda sem\u00e1ntica de recuerdos para mejores respuestas.</li> <li>\u267b\ufe0f Mejora continua del comportamiento mediante optimizaci\u00f3n del prompt.</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/#lo-aprendido","title":"\ud83e\uddea Lo aprendido","text":"<ul> <li>Qu\u00e9 es LangMem y c\u00f3mo se relaciona con los tipos de memoria del curso.</li> <li>C\u00f3mo instalarlo y configurarlo con un <code>BaseStore</code> y namespaces.</li> <li>C\u00f3mo usar sus herramientas (<code>manage_memory</code> y <code>search_memory</code>) en el \u201chot path\u201d.</li> <li>Qu\u00e9 opciones hay para mantenimiento en background y optimizaci\u00f3n del prompt.</li> </ul>"},{"location":"curso4/capsula4_langmem_sdk/#recursos","title":"\ud83d\udd0e Recursos","text":"<ul> <li>\ud83d\udcf0 Blog (lanzamiento): https://blog.langchain.dev/langmem-sdk-launch/</li> <li>\ud83d\udcd8 Docs (inicio): https://langchain-ai.github.io/langmem/</li> <li>\ud83e\uddf0 Memory Tools API: https://langchain-ai.github.io/langmem/reference/tools/</li> <li>\u26a1 Hot Path Quickstart: https://langchain-ai.github.io/langmem/hot_path_quickstart/</li> <li>\ud83e\udde0 Background Quickstart: https://langchain-ai.github.io/langmem/background_quickstart/</li> <li>\ud83c\udfd7\ufe0f BaseStore (LangGraph): https://langchain-ai.github.io/langgraph/reference/store/</li> <li>\ud83d\udcda Conceptos de Memoria (LangGraph): https://langchain-ai.github.io/langgraph/concepts/memory/</li> </ul>"},{"location":"curso4/capsula5_computer_use_agents/","title":"\ud83d\udda5\ufe0f C\u00e1psula 5: LangGraph Computer Use Agents (CUA)","text":""},{"location":"curso4/capsula5_computer_use_agents/#que-es-y-como-funciona","title":"\ud83c\udfaf \u00bfQu\u00e9 es y c\u00f3mo funciona?","text":"<p>Los Computer Use Agents (CUA) son agentes capaces de interactuar con un ordenador como lo har\u00eda una persona: abrir el navegador, escribir en la barra de direcciones, hacer clic en botones y enlaces, completar formularios, desplazarse, instalar apps en una VM, etc. Este proyecto empaqueta un agente de este tipo sobre LangGraph, con soporte nativo para:</p> <ul> <li>\ud83d\udce1 Streaming del estado para visualizar la sesi\u00f3n en vivo.</li> <li>\ud83e\udde0 Memoria a corto y largo plazo (integraci\u00f3n con la capa de memoria de LangGraph).</li> <li>\ud83d\ude4b Human-in-the-loop (intervenir cuando el agente lo necesite).</li> <li>\ud83d\udd10 Autenticaci\u00f3n persistente (Auth States) en sesiones de navegador.</li> </ul> <p>Para ejecutar acciones \u201cen el ordenador\u201d, el CUA se apoya en un entorno controlado (p. ej. una VM en Scrapybara). El grafo orquesta pasos del agente (percibir \u2192 razonar \u2192 actuar) y llama a herramientas de \u201ccomputer use\u201d que realizan acciones at\u00f3micas (click, type, open, screenshot, bash, etc.). El LLM decide qu\u00e9 herramienta usar en cada iteraci\u00f3n hasta cumplir el objetivo o alcanzar un l\u00edmite de recursi\u00f3n.</p> <p>Fuentes que estudiamos</p> <ul> <li>Repo oficial (Python): https://github.com/langchain-ai/langgraph-cua-py </li> <li>Video demo: https://www.youtube.com/watch?v=ndCFqT6xFQ4 </li> <li>LangGraph (framework): https://github.com/langchain-ai/langgraph </li> <li>LangGraph CUA (framework): https://github.com/langchain-ai/langgraph-cua-py</li> </ul>"},{"location":"curso4/capsula5_computer_use_agents/#para-que-podemos-usarlo-casos-de-uso","title":"\ud83e\udded \u00bfPara qu\u00e9 podemos usarlo? (Casos de uso)","text":"<ul> <li>\ud83d\udd0e Navegaci\u00f3n web aut\u00f3noma: buscar informaci\u00f3n, comparar precios, recopilar evidencia.</li> <li>\ud83d\uded2 Automatizar compras o reservas: rellenar formularios, iniciar sesi\u00f3n, completar pasos.</li> <li>\ud83d\udd27 Operaciones en escritorio/VM: instalar paquetes, ejecutar scripts, manipular archivos.</li> <li>\ud83d\udcc4 RPA ligero con razonamiento: flujos con UI no estructurada, m\u00e1s all\u00e1 de APIs.</li> <li>\ud83e\uddea QA end-to-end de interfaces web: explorar, verificar flujos y capturar resultados.</li> <li>\ud83d\udd12 Tareas autenticadas con reuso de sesi\u00f3n (Auth States) sin re-login constante.</li> </ul>"},{"location":"curso4/capsula5_computer_use_agents/#instalacion","title":"\ud83d\udce6 Instalaci\u00f3n","text":"<pre><code>pip install -U langgraph-cua\n</code></pre> <p>Configura tus claves necesarias (ejemplo en bash):</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport SCRAPYBARA_API_KEY=\"sb-...\"\n</code></pre> <p>Opcional: usa un fichero <code>.env</code> y carga variables con <code>python-dotenv</code>.</p> <p>Entornos soportados</p> <ul> <li><code>environment: web</code> (por defecto, navegador en VM). </li> <li><code>environment: ubuntu</code> o <code>windows</code> para tareas de SO.</li> </ul>"},{"location":"curso4/capsula5_computer_use_agents/#alternativas-opensource-a-scrapybara","title":"\ufffd Alternativas Open\u2011Source a Scrapybara","text":"<p>Si prefieres una soluci\u00f3n open\u2011source para controlar navegadores/VMs, aqu\u00ed tienes opciones compactas con sus pros y contras:</p> <ul> <li> <p>Playwright (Python) \u2014 https://playwright.dev/python {target=\"_blank\"}</p> <ul> <li>Pros: API moderna, multi\u2011navegador, f\u00e1cil guardar/recuperar <code>storage_state</code>, buena documentaci\u00f3n.</li> <li>Contras: no ofrece por s\u00ed mismo streaming ni VMs gestionadas; debes desplegarlo en contenedores/VMs.</li> </ul> </li> <li> <p>Browserless (self\u2011hosted) \u2014 https://www.browserless.io/ {target=\"_blank\"}</p> <ul> <li>Pros: servicio HTTP/WebSocket para controlar navegadores, imagen Docker lista, parecido a un servicio gestionado.</li> <li>Contras: necesitas a\u00f1adir persistencia de auth (storage) y/o una capa para streaming si la necesitas.</li> </ul> </li> <li> <p>Playwright en contenedores/VM (autogestionado)</p> <ul> <li>Pros: control total (puedes ejecutar comandos de SO dentro del contenedor), ideal si necesitas una \"VM\" real.</li> <li>Contras: m\u00e1s complejidad operativa (orquestar contenedores, limpieza, streaming).</li> </ul> </li> <li> <p>Selenium / Selenium Grid \u2014 https://www.selenium.dev/ {target=\"_blank\"}</p> <ul> <li>Pros: muy maduro y soportado, f\u00e1cil de escalar con Grid.</li> <li>Contras: API menos moderna para SPAs comparado con Playwright.</li> </ul> </li> </ul> <p>Peque\u00f1a recomendaci\u00f3n</p> <p>Para un balance entre API moderna y facilidad de despliegue, usa Browserless (self\u2011hosted) + Playwright como cliente; si necesitas ejecutar comandos a nivel SO, opta por Playwright en contenedores/VM.</p>"},{"location":"curso4/capsula5_computer_use_agents/#como-se-usa-paso-a-paso","title":"\ud83d\udd79\ufe0f \u00bfC\u00f3mo se usa? (paso a paso)","text":"<p>A continuaci\u00f3n explicamos el flujo t\u00edpico y, sobre todo, c\u00f3mo se maneja la memoria con la SDK y la configuraci\u00f3n.</p>"},{"location":"curso4/capsula5_computer_use_agents/#1-crear-el-grafo-cua","title":"1) Crear el grafo CUA","text":"<pre><code>from langgraph_cua import create_cua\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ncua_graph = create_cua(\n    # Opcionalmente: par\u00e1metros de configuraci\u00f3n\n    # environment=\"web\",               # \"web\" | \"ubuntu\" | \"windows\"\n    # timeout_hours=2,                  # Vida de la VM\n    # recursion_limit=100,              # Iteraciones m\u00e1ximas del agente\n    # zdr_enabled=False,                # Zero Data Retention\n    # auth_state_id=None,               # Reutilizar sesi\u00f3n autenticada\n    # prompt=\"...\"                      # Prompt del sistema recomendado\n)\n</code></pre> <p>Memoria y LangGraph</p> <p>El grafo CUA est\u00e1 construido sobre LangGraph, que ya incluye memoria de corto plazo (historia del hilo) y puede integrarse con memoria de largo plazo mediante su Store. El agente pasa mensajes y tool-calls entre pasos; seg\u00fan la opci\u00f3n <code>zdr_enabled</code>, puede incluir todo el historial o usar <code>previous_response_id</code> para optimizar.</p>"},{"location":"curso4/capsula5_computer_use_agents/#2-invocar-con-mensajes-y-recibir-streaming","title":"2) Invocar con mensajes y recibir streaming","text":"<pre><code>messages = [\n    {\n        \"role\": \"system\",\n        \"content\": (\n            \"Eres un asistente avanzado de uso de ordenador. El navegador que est\u00e1s utilizando \"\n            \"ya est\u00e1 inicializado y visitando google.com.\"\n        ),\n    },\n    {\n        \"role\": \"user\",\n        \"content\": (\n            \"\u00bfPuedes encontrar el mejor precio para neum\u00e1ticos nuevos para todas las estaciones que sean compatibles con mi Subaru Forester 2019?\"\n        ),\n    },\n]\n\nasync def main():\n    async for update in cua_graph.astream({\"messages\": messages}, stream_mode=\"updates\"):\n        if \"create_vm_instance\" in update:\n            stream_url = update[\"create_vm_instance\"].get(\"stream_url\")\n            print(\"VM lista. Abre la URL para ver la sesi\u00f3n:\", stream_url)\n\n# Ejecutar con asyncio.run(main())\n</code></pre> <p>El stream emite eventos clave (p. ej., creaci\u00f3n de VM) y el <code>stream_url</code> para observar la ejecuci\u00f3n en vivo en el navegador.</p>"},{"location":"curso4/capsula5_computer_use_agents/#3-personalizar-el-comportamiento-config","title":"3) Personalizar el comportamiento (config)","text":"<p>Puedes pasar <code>config</code> al crear el grafo o en cada invocaci\u00f3n (v\u00eda <code>config={\"configurable\": {...}}</code>). Par\u00e1metros relevantes:</p> <ul> <li><code>environment</code>: <code>web</code> | <code>ubuntu</code> | <code>windows</code>.</li> <li><code>timeout_hours</code>: vida de la VM antes de apagar.</li> <li><code>recursion_limit</code>: iteraciones m\u00e1ximas (m\u00e1s alto que 25 por la naturaleza de CUA).</li> <li><code>zdr_enabled</code>: si tu cuenta OpenAI tiene Zero Data Retention.</li> <li><code>prompt</code>: prompt de sistema. Scrapybara recomienda uno extenso con buenas pr\u00e1cticas (zoom, scroll, manejo de formularios, etc.).</li> <li><code>auth_state_id</code>: id de un Auth State para reutilizar sesi\u00f3n autenticada del navegador.</li> </ul> <p>Prompt de sistema recomendado</p> <p>Consulta el README del repo para un prompt detallado con buenas pr\u00e1cticas: https://github.com/langchain-ai/langgraph-cua-py#system-prompts</p>"},{"location":"curso4/capsula5_computer_use_agents/#4-autenticacion-persistente-auth-states","title":"4) Autenticaci\u00f3n persistente (Auth States)","text":"<p>Los Auth States permiten guardar y reutilizar sesiones del navegador (por ejemplo, ya logueado en Amazon) para futuras ejecuciones.</p> <ul> <li>Pasa <code>auth_state_id</code> a <code>create_cua(...)</code>.</li> <li>El grafo almacenar\u00e1 ese id en el estado (<code>authenticated_id</code>).</li> <li>Si cambias <code>auth_state_id</code> m\u00e1s adelante, el grafo reautentica autom\u00e1ticamente.</li> </ul> <p>Gesti\u00f3n con la SDK de Scrapybara:</p> <pre><code>from scrapybara import Scrapybara\n\nclient = Scrapybara(api_key=\"sb-...\")\ninstance = client.get(\"&lt;instance_id&gt;\")\nauth_state_id = instance.save_auth(name=\"example_site\").auth_state_id\n\n# Modificar un Auth State existente\ninstance.modify_auth(auth_state_id=auth_state_id, name=\"renamed_auth_state\")\n\n# Nota: Para forzar re-autenticaci\u00f3n en un run activo, pon authenticated_id=None en el state.\n</code></pre>"},{"location":"curso4/capsula5_computer_use_agents/#5-memoria-corto-vs-largo-plazo-y-zdr","title":"5) Memoria: corto vs. largo plazo y ZDR","text":"<ul> <li>Corto plazo: la historia del hilo fluye dentro del grafo (mensajes + tool calls). Si <code>zdr_enabled=False</code>, el agente puede usar <code>previous_response_id</code> y mandar solo el mensaje m\u00e1s reciente; si <code>True</code>, enviar\u00e1 todo el historial cada vez.</li> <li>Largo plazo: puedes combinar CUA con la capa de memoria persistente de LangGraph (BaseStore) y/o LangMem para recuerdos entre sesiones (preferencias, facts, etc.).</li> </ul> <p>Cu\u00e1ndo activar ZDR</p> <p>Activa <code>zdr_enabled=True</code> si tu pol\u00edtica de privacidad exige no depender de <code>previous_response_id</code>. El coste puede ser mayor al enviar todo el historial, pero aumenta el aislamiento.</p>"},{"location":"curso4/capsula5_computer_use_agents/#6-entornos-alternativos","title":"6) Entornos alternativos","text":"<p>Adem\u00e1s de <code>web</code>, puedes usar entornos <code>ubuntu</code> o <code>windows</code> para tareas de SO. El agente usar\u00e1 herramientas apropiadas (p. ej., <code>bash</code>) para instalar y ejecutar comandos.</p>"},{"location":"curso4/capsula5_computer_use_agents/#ventajas","title":"\u2705 Ventajas","text":"<ul> <li>\ud83d\uddb1\ufe0f Acceso a UI real: el agente puede interactuar con p\u00e1ginas y apps como un humano.</li> <li>\ud83e\udde9 Orquestaci\u00f3n s\u00f3lida con LangGraph: streaming, memoria, control de flujo, HITL.</li> <li>\ud83d\udd01 Reuso de sesiones (Auth States) para tareas autenticadas.</li> <li>\ud83e\uddf1 Aislamiento en VM: m\u00e1s seguro y reproducible que tocar tu m\u00e1quina local.</li> <li>\ud83d\udd27 Multi-entorno: web, Ubuntu y Windows.</li> </ul>"},{"location":"curso4/capsula5_computer_use_agents/#lo-aprendido","title":"\ud83e\uddea Lo aprendido","text":"<ul> <li>Qu\u00e9 es un Computer Use Agent y c\u00f3mo se integra con LangGraph.</li> <li>C\u00f3mo instalar, configurar claves y levantar un grafo CUA.</li> <li>C\u00f3mo invocar con streaming y personalizar par\u00e1metros.</li> <li>C\u00f3mo funciona la memoria (corto/largo plazo) y el impacto de ZDR.</li> <li>C\u00f3mo mantener sesiones autenticadas con Auth States.</li> </ul>"},{"location":"curso4/capsula5_computer_use_agents/#recursos-externos","title":"\ud83d\udd0e Recursos externos","text":"<ul> <li>\ud83d\udce6 Repo (Python): https://github.com/langchain-ai/langgraph-cua-py</li> <li>\ud83c\udfac Demo en YouTube: https://www.youtube.com/watch?v=ndCFqT6xFQ4</li> <li>\ud83d\udcd8 Docs de memoria en LangGraph: https://langchain-ai.github.io/langgraph/concepts/memory/</li> <li>\ud83e\uddd1\u200d\ud83d\udcbb Streaming en LangGraph: https://langchain-ai.github.io/langgraph/how-tos/#streaming</li> <li>\ud83d\udd10 Auth States (Scrapybara): https://docs.scrapybara.com/auth-states</li> <li>\ud83e\uddf0 TS version: https://github.com/langchain-ai/langgraphjs/tree/main/libs/langgraph-cua</li> </ul>"}]}